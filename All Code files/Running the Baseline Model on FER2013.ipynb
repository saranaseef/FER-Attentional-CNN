{"cells":[{"cell_type":"code","execution_count":null,"id":"3dc833bf-8110-4db8-88d6-05f8e30f53a9","metadata":{"id":"3dc833bf-8110-4db8-88d6-05f8e30f53a9"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":null,"id":"ce51e520-4031-4887-a0fd-c820ebde34a8","metadata":{"id":"ce51e520-4031-4887-a0fd-c820ebde34a8"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"id":"c7279646","metadata":{"id":"c7279646"},"outputs":[],"source":["from __future__ import print_function\n","import argparse\n","import numpy  as np\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torchvision import transforms"]},{"cell_type":"code","execution_count":null,"id":"5fce1e3b","metadata":{"id":"5fce1e3b"},"outputs":[],"source":["import data_loaders"]},{"cell_type":"code","execution_count":null,"id":"5b7540c4","metadata":{"id":"5b7540c4"},"outputs":[],"source":["from data_loaders import Plain_Dataset, eval_data_dataloader"]},{"cell_type":"code","execution_count":null,"id":"4d7ecda3","metadata":{"id":"4d7ecda3"},"outputs":[],"source":["from deep_emotion import Deep_Emotion"]},{"cell_type":"code","execution_count":null,"id":"8d5df464","metadata":{"id":"8d5df464"},"outputs":[],"source":["from generate_data import Generate_data"]},{"cell_type":"code","execution_count":null,"id":"87900505","metadata":{"id":"87900505"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"id":"463cc574","metadata":{"id":"463cc574","outputId":"263f4b86-b797-41b8-8e43-a709419ee512"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["print(\"cuda:0\") if torch.cuda.is_available() else print(\"cpu\")"]},{"cell_type":"code","execution_count":null,"id":"14a1fdbb","metadata":{"id":"14a1fdbb"},"outputs":[],"source":["def Train(epochs,train_loader,val_loader,criterion,optmizer,device):\n","    '''\n","    Training Loop\n","    '''\n","    print(\"===================================Start Training===================================\")\n","    for e in range(epochs):\n","        train_loss = 0\n","        validation_loss = 0\n","        train_correct = 0\n","        val_correct = 0\n","        # Train the model  #\n","        net.train()\n","        for data, labels in train_loader:\n","            data, labels = data.to(device), labels.to(device)\n","            optmizer.zero_grad()\n","            outputs = net(data)\n","            #print(outputs[0])\n","            #print(outputs[0])\n","\n","            loss = criterion(outputs,labels)\n","            #print(labels[0])\n","\n","            loss.backward()\n","            optmizer.step()\n","            train_loss += loss.item()\n","            _, preds = torch.max(outputs,1)\n","            train_correct += torch.sum(preds == labels.data)\n","\n","        #validate the model#\n","        net.eval()\n","        for data,labels in val_loader:\n","            data, labels = data.to(device), labels.to(device)\n","            val_outputs = net(data)\n","            #print(val_outputs[0][3])\n","            #print(labels[0])\n","            val_loss = criterion(val_outputs, labels)\n","            validation_loss += val_loss.item()\n","            _, val_preds = torch.max(val_outputs,1)\n","            val_correct += torch.sum(val_preds == labels.data)\n","\n","        train_loss = train_loss/len(train_dataset)\n","        train_acc = train_correct.double() / len(train_dataset)\n","        validation_loss =  validation_loss / len(validation_dataset)\n","        val_acc = val_correct.double() / len(validation_dataset)\n","        print('Epoch: {} \\tTraining Loss: {:.8f} \\tValidation Loss {:.8f} \\tTraining Acuuarcy {:.3f}% \\tValidation Acuuarcy {:.3f}%'\n","                                                           .format(e+1, train_loss,validation_loss,train_acc * 100, val_acc*100))\n","\n","    torch.save(net.state_dict(),'deep_emotion-{}-{}-{}.pt'.format(epochs,batchsize,lr))\n","    print(\"===================================Training Finished===================================\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"3fb395e9","metadata":{"id":"3fb395e9","outputId":"63100e90-7a11-4a83-cdc1-6bee76546a86"},"outputs":[{"data":{"text/plain":["_StoreAction(option_strings=['-s', '--setup'], dest='setup', nargs=None, const=None, default=None, type=<class 'bool'>, choices=None, help='setup the dataset for the first time', metavar=None)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["parser = argparse.ArgumentParser(description=\"Configuration of setup and training process\")\n","parser.add_argument('-s', '--setup', type=bool, help='setup the dataset for the first time')\n",""]},{"cell_type":"code","execution_count":null,"id":"6110c98c","metadata":{"id":"6110c98c","outputId":"34ef8690-6864-4a3f-ad3a-31bc603d9b45"},"outputs":[{"data":{"text/plain":["_StoreAction(option_strings=['-d', '--data'], dest='data', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, help='data folder that contains data files that downloaded from kaggle (train.csv and test.csv)', metavar=None)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["parser.add_argument('-d', '--data', type=str,required= True,\n","                               help='data folder that contains data files that downloaded from kaggle (train.csv and test.csv)')\n",""]},{"cell_type":"code","execution_count":null,"id":"2a970635","metadata":{"id":"2a970635","outputId":"778601d4-47d0-422d-9182-8bc2d02d516a"},"outputs":[{"data":{"text/plain":["_StoreAction(option_strings=['-hparams', '--hyperparams'], dest='hyperparams', nargs=None, const=None, default=None, type=<class 'bool'>, choices=None, help='True when changing the hyperparameters e.g (batch size, LR, num. of epochs)', metavar=None)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["parser.add_argument('-hparams', '--hyperparams', type=bool,\n","                               help='True when changing the hyperparameters e.g (batch size, LR, num. of epochs)')\n",""]},{"cell_type":"code","execution_count":null,"id":"3c499a1e","metadata":{"id":"3c499a1e","outputId":"bb336bf5-62e9-422c-845c-1c778eacd7c7"},"outputs":[{"data":{"text/plain":["_StoreAction(option_strings=['-t', '--train'], dest='train', nargs=None, const=None, default=None, type=<class 'bool'>, choices=None, help='True when training', metavar=None)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["parser.add_argument('-e', '--epochs', type= int, help= 'number of epochs')\n","parser.add_argument('-lr', '--learning_rate', type= float, help= 'value of learning rate')\n","parser.add_argument('-bs', '--batch_size', type= int, help= 'training/validation batch size')\n","parser.add_argument('-t', '--train', type=bool, help='True when training')\n",""]},{"cell_type":"code","execution_count":null,"id":"9388e776","metadata":{"id":"9388e776"},"outputs":[],"source":["epochs = 1500\n","lr = 0.001\n","batchsize = 128"]},{"cell_type":"code","execution_count":null,"id":"b7be4207","metadata":{"id":"b7be4207"},"outputs":[],"source":["net = Deep_Emotion()"]},{"cell_type":"code","execution_count":null,"id":"3c4695c6","metadata":{"id":"3c4695c6","outputId":"fd19e219-7b93-449c-f6c6-34a21d7f1ff6"},"outputs":[{"data":{"text/plain":["Deep_Emotion(\n","  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n","  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n","  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n","  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv5): Conv2d(50, 50, kernel_size=(2, 2), stride=(1, 1))\n","  (conv6): Conv2d(50, 10, kernel_size=(2, 2), stride=(1, 1))\n","  (pool6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc1): Linear(in_features=810, out_features=100, bias=True)\n","  (fc2): Linear(in_features=100, out_features=9, bias=True)\n","  (softmax): Softmax(dim=1)\n","  (localization): Sequential(\n","    (0): Conv2d(1, 15, kernel_size=(7, 7), stride=(1, 1))\n","    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (2): ReLU(inplace=True)\n","    (3): Conv2d(15, 10, kernel_size=(5, 5), stride=(1, 1))\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): ReLU(inplace=True)\n","  )\n","  (fc_loc): Sequential(\n","    (0): Linear(in_features=640, out_features=32, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Linear(in_features=32, out_features=6, bias=True)\n","  )\n",")"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["net.to(device)"]},{"cell_type":"code","execution_count":null,"id":"bbd860b1","metadata":{"id":"bbd860b1","outputId":"2595f274-2c8a-4f0f-8f9f-ef2fd11cc204"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model archticture:  Deep_Emotion(\n","  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n","  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n","  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n","  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv5): Conv2d(50, 50, kernel_size=(2, 2), stride=(1, 1))\n","  (conv6): Conv2d(50, 10, kernel_size=(2, 2), stride=(1, 1))\n","  (pool6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (norm): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc1): Linear(in_features=810, out_features=100, bias=True)\n","  (fc2): Linear(in_features=100, out_features=9, bias=True)\n","  (softmax): Softmax(dim=1)\n","  (localization): Sequential(\n","    (0): Conv2d(1, 15, kernel_size=(7, 7), stride=(1, 1))\n","    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (2): ReLU(inplace=True)\n","    (3): Conv2d(15, 10, kernel_size=(5, 5), stride=(1, 1))\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): ReLU(inplace=True)\n","  )\n","  (fc_loc): Sequential(\n","    (0): Linear(in_features=640, out_features=32, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Linear(in_features=32, out_features=6, bias=True)\n","  )\n",")\n"]}],"source":["print(\"Model archticture: \", net)"]},{"cell_type":"code","execution_count":null,"id":"8e0d1f8a","metadata":{"id":"8e0d1f8a"},"outputs":[],"source":["transformation= transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])"]},{"cell_type":"code","execution_count":null,"id":"6b248c29","metadata":{"id":"6b248c29"},"outputs":[],"source":["traincsv_file = \"data\" + \"/\"+\"train.csv\"\n","validationcsv_file = \"data\" + \"/\"+\"val.csv\""]},{"cell_type":"code","execution_count":null,"id":"5d7d4d93","metadata":{"id":"5d7d4d93"},"outputs":[],"source":["train_img_dir = \"data\"+\"/\"+\"train/\""]},{"cell_type":"code","execution_count":null,"id":"5ad9c7fe","metadata":{"id":"5ad9c7fe"},"outputs":[],"source":["validation_img_dir = \"data\"+\"/\"+\"val/\""]},{"cell_type":"code","execution_count":null,"id":"d3160362","metadata":{"id":"d3160362"},"outputs":[],"source":["train_dataset= Plain_Dataset(csv_file=traincsv_file, img_dir = train_img_dir, datatype = 'train', transform = transformation)\n"]},{"cell_type":"code","execution_count":null,"id":"f3da698c","metadata":{"id":"f3da698c"},"outputs":[],"source":["validation_dataset= Plain_Dataset(csv_file=validationcsv_file, img_dir = validation_img_dir, datatype = 'val', transform = transformation)\n"]},{"cell_type":"code","execution_count":null,"id":"dc622e34","metadata":{"id":"dc622e34"},"outputs":[],"source":["train_loader= DataLoader(train_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n","val_loader=   DataLoader(validation_dataset,batch_size=batchsize,shuffle = True,num_workers=0)"]},{"cell_type":"code","execution_count":null,"id":"f8a03381","metadata":{"id":"f8a03381"},"outputs":[],"source":["criterion= nn.CrossEntropyLoss()\n","optmizer= optim.Adam(net.parameters(),lr= 0.0001)\n"]},{"cell_type":"code","execution_count":null,"id":"2fc73610","metadata":{"id":"2fc73610","outputId":"359488f1-1d96-49a6-9d19-d0c758abdf84"},"outputs":[{"name":"stdout","output_type":"stream","text":["===================================Start Training===================================\n","Epoch: 1 \tTraining Loss: 0.01352283 \tValidation Loss 0.01394671 \tTraining Acuuarcy 64.541% \tValidation Acuuarcy 65.366%\n","Epoch: 2 \tTraining Loss: 0.01350482 \tValidation Loss 0.01404846 \tTraining Acuuarcy 64.687% \tValidation Acuuarcy 64.503%\n","Epoch: 3 \tTraining Loss: 0.01351534 \tValidation Loss 0.01398192 \tTraining Acuuarcy 64.635% \tValidation Acuuarcy 64.809%\n","Epoch: 4 \tTraining Loss: 0.01349659 \tValidation Loss 0.01399706 \tTraining Acuuarcy 64.823% \tValidation Acuuarcy 64.614%\n","Epoch: 5 \tTraining Loss: 0.01350902 \tValidation Loss 0.01379753 \tTraining Acuuarcy 64.649% \tValidation Acuuarcy 65.840%\n","Epoch: 6 \tTraining Loss: 0.01350974 \tValidation Loss 0.01388077 \tTraining Acuuarcy 64.600% \tValidation Acuuarcy 65.283%\n","Epoch: 7 \tTraining Loss: 0.01347865 \tValidation Loss 0.01383290 \tTraining Acuuarcy 65.018% \tValidation Acuuarcy 65.506%\n","Epoch: 8 \tTraining Loss: 0.01349828 \tValidation Loss 0.01379822 \tTraining Acuuarcy 64.722% \tValidation Acuuarcy 65.088%\n","Epoch: 9 \tTraining Loss: 0.01351454 \tValidation Loss 0.01388252 \tTraining Acuuarcy 64.502% \tValidation Acuuarcy 64.642%\n","Epoch: 10 \tTraining Loss: 0.01349760 \tValidation Loss 0.01395076 \tTraining Acuuarcy 64.788% \tValidation Acuuarcy 64.503%\n","Epoch: 11 \tTraining Loss: 0.01349951 \tValidation Loss 0.01386942 \tTraining Acuuarcy 64.778% \tValidation Acuuarcy 65.784%\n","Epoch: 12 \tTraining Loss: 0.01348371 \tValidation Loss 0.01384208 \tTraining Acuuarcy 64.945% \tValidation Acuuarcy 64.586%\n","Epoch: 13 \tTraining Loss: 0.01349170 \tValidation Loss 0.01389087 \tTraining Acuuarcy 64.872% \tValidation Acuuarcy 65.060%\n","Epoch: 14 \tTraining Loss: 0.01351928 \tValidation Loss 0.01391229 \tTraining Acuuarcy 64.555% \tValidation Acuuarcy 64.140%\n","Epoch: 15 \tTraining Loss: 0.01347818 \tValidation Loss 0.01389855 \tTraining Acuuarcy 65.115% \tValidation Acuuarcy 64.586%\n","Epoch: 16 \tTraining Loss: 0.01351270 \tValidation Loss 0.01375348 \tTraining Acuuarcy 64.628% \tValidation Acuuarcy 65.868%\n","Epoch: 17 \tTraining Loss: 0.01351119 \tValidation Loss 0.01397851 \tTraining Acuuarcy 64.628% \tValidation Acuuarcy 64.280%\n","Epoch: 18 \tTraining Loss: 0.01349596 \tValidation Loss 0.01399515 \tTraining Acuuarcy 64.917% \tValidation Acuuarcy 64.503%\n","Epoch: 19 \tTraining Loss: 0.01350260 \tValidation Loss 0.01393050 \tTraining Acuuarcy 64.694% \tValidation Acuuarcy 64.753%\n","Epoch: 20 \tTraining Loss: 0.01351601 \tValidation Loss 0.01384991 \tTraining Acuuarcy 64.534% \tValidation Acuuarcy 64.976%\n","Epoch: 21 \tTraining Loss: 0.01353363 \tValidation Loss 0.01393626 \tTraining Acuuarcy 64.328% \tValidation Acuuarcy 64.614%\n","Epoch: 22 \tTraining Loss: 0.01350752 \tValidation Loss 0.01395292 \tTraining Acuuarcy 64.652% \tValidation Acuuarcy 64.503%\n","Epoch: 23 \tTraining Loss: 0.01349882 \tValidation Loss 0.01384071 \tTraining Acuuarcy 64.781% \tValidation Acuuarcy 65.366%\n","Epoch: 24 \tTraining Loss: 0.01350397 \tValidation Loss 0.01393740 \tTraining Acuuarcy 64.750% \tValidation Acuuarcy 65.394%\n","Epoch: 25 \tTraining Loss: 0.01349325 \tValidation Loss 0.01381813 \tTraining Acuuarcy 64.913% \tValidation Acuuarcy 65.534%\n","Epoch: 26 \tTraining Loss: 0.01349807 \tValidation Loss 0.01389970 \tTraining Acuuarcy 64.858% \tValidation Acuuarcy 64.531%\n","Epoch: 27 \tTraining Loss: 0.01350766 \tValidation Loss 0.01399384 \tTraining Acuuarcy 64.711% \tValidation Acuuarcy 64.865%\n","Epoch: 28 \tTraining Loss: 0.01349539 \tValidation Loss 0.01393990 \tTraining Acuuarcy 64.781% \tValidation Acuuarcy 65.255%\n","Epoch: 29 \tTraining Loss: 0.01350182 \tValidation Loss 0.01394865 \tTraining Acuuarcy 64.764% \tValidation Acuuarcy 65.283%\n","Epoch: 30 \tTraining Loss: 0.01351399 \tValidation Loss 0.01393252 \tTraining Acuuarcy 64.642% \tValidation Acuuarcy 64.837%\n","Epoch: 31 \tTraining Loss: 0.01351263 \tValidation Loss 0.01392419 \tTraining Acuuarcy 64.565% \tValidation Acuuarcy 64.837%\n","Epoch: 32 \tTraining Loss: 0.01347108 \tValidation Loss 0.01385224 \tTraining Acuuarcy 65.150% \tValidation Acuuarcy 65.394%\n","Epoch: 33 \tTraining Loss: 0.01350703 \tValidation Loss 0.01397887 \tTraining Acuuarcy 64.725% \tValidation Acuuarcy 64.586%\n","Epoch: 34 \tTraining Loss: 0.01352848 \tValidation Loss 0.01382493 \tTraining Acuuarcy 64.415% \tValidation Acuuarcy 65.534%\n","Epoch: 35 \tTraining Loss: 0.01352355 \tValidation Loss 0.01392973 \tTraining Acuuarcy 64.492% \tValidation Acuuarcy 64.921%\n","Epoch: 36 \tTraining Loss: 0.01351552 \tValidation Loss 0.01389521 \tTraining Acuuarcy 64.586% \tValidation Acuuarcy 64.642%\n","Epoch: 37 \tTraining Loss: 0.01350847 \tValidation Loss 0.01383160 \tTraining Acuuarcy 64.666% \tValidation Acuuarcy 64.893%\n","Epoch: 38 \tTraining Loss: 0.01349323 \tValidation Loss 0.01388421 \tTraining Acuuarcy 64.872% \tValidation Acuuarcy 64.586%\n","Epoch: 39 \tTraining Loss: 0.01350215 \tValidation Loss 0.01382190 \tTraining Acuuarcy 64.795% \tValidation Acuuarcy 65.534%\n","Epoch: 40 \tTraining Loss: 0.01352529 \tValidation Loss 0.01395453 \tTraining Acuuarcy 64.443% \tValidation Acuuarcy 65.199%\n","Epoch: 41 \tTraining Loss: 0.01350170 \tValidation Loss 0.01388503 \tTraining Acuuarcy 64.715% \tValidation Acuuarcy 65.255%\n","Epoch: 42 \tTraining Loss: 0.01352106 \tValidation Loss 0.01401581 \tTraining Acuuarcy 64.593% \tValidation Acuuarcy 64.280%\n","Epoch: 43 \tTraining Loss: 0.01351506 \tValidation Loss 0.01388688 \tTraining Acuuarcy 64.607% \tValidation Acuuarcy 64.753%\n","Epoch: 44 \tTraining Loss: 0.01350477 \tValidation Loss 0.01378055 \tTraining Acuuarcy 64.684% \tValidation Acuuarcy 65.952%\n","Epoch: 45 \tTraining Loss: 0.01349310 \tValidation Loss 0.01393422 \tTraining Acuuarcy 64.840% \tValidation Acuuarcy 65.645%\n","Epoch: 46 \tTraining Loss: 0.01349556 \tValidation Loss 0.01378208 \tTraining Acuuarcy 64.861% \tValidation Acuuarcy 66.035%\n","Epoch: 47 \tTraining Loss: 0.01346491 \tValidation Loss 0.01391173 \tTraining Acuuarcy 65.251% \tValidation Acuuarcy 64.308%\n","Epoch: 48 \tTraining Loss: 0.01350175 \tValidation Loss 0.01390973 \tTraining Acuuarcy 64.750% \tValidation Acuuarcy 65.143%\n","Epoch: 49 \tTraining Loss: 0.01348993 \tValidation Loss 0.01387113 \tTraining Acuuarcy 64.879% \tValidation Acuuarcy 65.561%\n","Epoch: 50 \tTraining Loss: 0.01350887 \tValidation Loss 0.01400339 \tTraining Acuuarcy 64.718% \tValidation Acuuarcy 65.171%\n","Epoch: 51 \tTraining Loss: 0.01351691 \tValidation Loss 0.01382048 \tTraining Acuuarcy 64.509% \tValidation Acuuarcy 65.339%\n","Epoch: 52 \tTraining Loss: 0.01349451 \tValidation Loss 0.01392499 \tTraining Acuuarcy 64.851% \tValidation Acuuarcy 64.280%\n","Epoch: 53 \tTraining Loss: 0.01346534 \tValidation Loss 0.01387970 \tTraining Acuuarcy 65.265% \tValidation Acuuarcy 64.781%\n","Epoch: 54 \tTraining Loss: 0.01350467 \tValidation Loss 0.01395173 \tTraining Acuuarcy 64.778% \tValidation Acuuarcy 65.199%\n","Epoch: 55 \tTraining Loss: 0.01349527 \tValidation Loss 0.01391542 \tTraining Acuuarcy 64.823% \tValidation Acuuarcy 65.060%\n","Epoch: 56 \tTraining Loss: 0.01351778 \tValidation Loss 0.01398586 \tTraining Acuuarcy 64.607% \tValidation Acuuarcy 64.753%\n","Epoch: 57 \tTraining Loss: 0.01349335 \tValidation Loss 0.01395119 \tTraining Acuuarcy 64.865% \tValidation Acuuarcy 64.586%\n","Epoch: 58 \tTraining Loss: 0.01348296 \tValidation Loss 0.01382096 \tTraining Acuuarcy 65.021% \tValidation Acuuarcy 65.506%\n","Epoch: 59 \tTraining Loss: 0.01350745 \tValidation Loss 0.01391078 \tTraining Acuuarcy 64.684% \tValidation Acuuarcy 65.088%\n","Epoch: 60 \tTraining Loss: 0.01350201 \tValidation Loss 0.01393723 \tTraining Acuuarcy 64.711% \tValidation Acuuarcy 65.339%\n","Epoch: 61 \tTraining Loss: 0.01348231 \tValidation Loss 0.01401764 \tTraining Acuuarcy 64.990% \tValidation Acuuarcy 63.722%\n","Epoch: 62 \tTraining Loss: 0.01349419 \tValidation Loss 0.01384919 \tTraining Acuuarcy 64.875% \tValidation Acuuarcy 65.171%\n","Epoch: 63 \tTraining Loss: 0.01349176 \tValidation Loss 0.01392351 \tTraining Acuuarcy 64.962% \tValidation Acuuarcy 65.534%\n","Epoch: 64 \tTraining Loss: 0.01349160 \tValidation Loss 0.01391634 \tTraining Acuuarcy 65.032% \tValidation Acuuarcy 64.893%\n","Epoch: 65 \tTraining Loss: 0.01348916 \tValidation Loss 0.01386641 \tTraining Acuuarcy 64.924% \tValidation Acuuarcy 65.534%\n","Epoch: 66 \tTraining Loss: 0.01347435 \tValidation Loss 0.01394789 \tTraining Acuuarcy 65.053% \tValidation Acuuarcy 64.419%\n","Epoch: 67 \tTraining Loss: 0.01347043 \tValidation Loss 0.01384976 \tTraining Acuuarcy 65.178% \tValidation Acuuarcy 65.088%\n","Epoch: 68 \tTraining Loss: 0.01349358 \tValidation Loss 0.01386987 \tTraining Acuuarcy 64.893% \tValidation Acuuarcy 64.921%\n","Epoch: 69 \tTraining Loss: 0.01350275 \tValidation Loss 0.01386208 \tTraining Acuuarcy 64.802% \tValidation Acuuarcy 65.645%\n","Epoch: 70 \tTraining Loss: 0.01351454 \tValidation Loss 0.01398376 \tTraining Acuuarcy 64.603% \tValidation Acuuarcy 64.726%\n","Epoch: 71 \tTraining Loss: 0.01348765 \tValidation Loss 0.01383262 \tTraining Acuuarcy 64.872% \tValidation Acuuarcy 66.063%\n","Epoch: 72 \tTraining Loss: 0.01349222 \tValidation Loss 0.01383945 \tTraining Acuuarcy 64.865% \tValidation Acuuarcy 65.199%\n","Epoch: 73 \tTraining Loss: 0.01348237 \tValidation Loss 0.01375126 \tTraining Acuuarcy 65.007% \tValidation Acuuarcy 65.701%\n","Epoch: 74 \tTraining Loss: 0.01347003 \tValidation Loss 0.01401148 \tTraining Acuuarcy 65.112% \tValidation Acuuarcy 64.531%\n","Epoch: 75 \tTraining Loss: 0.01349702 \tValidation Loss 0.01401091 \tTraining Acuuarcy 64.816% \tValidation Acuuarcy 64.475%\n","Epoch: 76 \tTraining Loss: 0.01350157 \tValidation Loss 0.01396135 \tTraining Acuuarcy 64.795% \tValidation Acuuarcy 64.391%\n","Epoch: 77 \tTraining Loss: 0.01351460 \tValidation Loss 0.01376188 \tTraining Acuuarcy 64.593% \tValidation Acuuarcy 65.701%\n","Epoch: 78 \tTraining Loss: 0.01347858 \tValidation Loss 0.01386283 \tTraining Acuuarcy 64.997% \tValidation Acuuarcy 65.032%\n","Epoch: 79 \tTraining Loss: 0.01349458 \tValidation Loss 0.01389254 \tTraining Acuuarcy 64.795% \tValidation Acuuarcy 65.478%\n","Epoch: 80 \tTraining Loss: 0.01350818 \tValidation Loss 0.01390607 \tTraining Acuuarcy 64.600% \tValidation Acuuarcy 65.088%\n","Epoch: 81 \tTraining Loss: 0.01347386 \tValidation Loss 0.01382816 \tTraining Acuuarcy 65.126% \tValidation Acuuarcy 65.506%\n","Epoch: 82 \tTraining Loss: 0.01350916 \tValidation Loss 0.01384873 \tTraining Acuuarcy 64.638% \tValidation Acuuarcy 64.893%\n","Epoch: 83 \tTraining Loss: 0.01349216 \tValidation Loss 0.01384144 \tTraining Acuuarcy 64.844% \tValidation Acuuarcy 65.255%\n","Epoch: 84 \tTraining Loss: 0.01348357 \tValidation Loss 0.01382146 \tTraining Acuuarcy 65.042% \tValidation Acuuarcy 65.756%\n","Epoch: 85 \tTraining Loss: 0.01350304 \tValidation Loss 0.01394405 \tTraining Acuuarcy 64.805% \tValidation Acuuarcy 65.534%\n","Epoch: 86 \tTraining Loss: 0.01351463 \tValidation Loss 0.01393884 \tTraining Acuuarcy 64.551% \tValidation Acuuarcy 64.113%\n","Epoch: 87 \tTraining Loss: 0.01350002 \tValidation Loss 0.01381898 \tTraining Acuuarcy 64.739% \tValidation Acuuarcy 65.589%\n","Epoch: 88 \tTraining Loss: 0.01348504 \tValidation Loss 0.01392888 \tTraining Acuuarcy 64.938% \tValidation Acuuarcy 64.809%\n","Epoch: 89 \tTraining Loss: 0.01348863 \tValidation Loss 0.01379549 \tTraining Acuuarcy 64.973% \tValidation Acuuarcy 65.227%\n","Epoch: 90 \tTraining Loss: 0.01348930 \tValidation Loss 0.01392335 \tTraining Acuuarcy 64.959% \tValidation Acuuarcy 64.029%\n","Epoch: 91 \tTraining Loss: 0.01350495 \tValidation Loss 0.01393569 \tTraining Acuuarcy 64.680% \tValidation Acuuarcy 64.642%\n","Epoch: 92 \tTraining Loss: 0.01351847 \tValidation Loss 0.01398823 \tTraining Acuuarcy 64.617% \tValidation Acuuarcy 65.311%\n","Epoch: 93 \tTraining Loss: 0.01349783 \tValidation Loss 0.01377123 \tTraining Acuuarcy 64.812% \tValidation Acuuarcy 65.311%\n","Epoch: 94 \tTraining Loss: 0.01349812 \tValidation Loss 0.01384727 \tTraining Acuuarcy 64.851% \tValidation Acuuarcy 65.227%\n","Epoch: 95 \tTraining Loss: 0.01349988 \tValidation Loss 0.01399223 \tTraining Acuuarcy 64.774% \tValidation Acuuarcy 64.670%\n","Epoch: 96 \tTraining Loss: 0.01350788 \tValidation Loss 0.01402891 \tTraining Acuuarcy 64.732% \tValidation Acuuarcy 63.639%\n","Epoch: 97 \tTraining Loss: 0.01351500 \tValidation Loss 0.01378508 \tTraining Acuuarcy 64.600% \tValidation Acuuarcy 65.283%\n","Epoch: 98 \tTraining Loss: 0.01348540 \tValidation Loss 0.01384632 \tTraining Acuuarcy 65.007% \tValidation Acuuarcy 65.729%\n","Epoch: 99 \tTraining Loss: 0.01349273 \tValidation Loss 0.01404120 \tTraining Acuuarcy 64.896% \tValidation Acuuarcy 64.865%\n","Epoch: 100 \tTraining Loss: 0.01349342 \tValidation Loss 0.01390791 \tTraining Acuuarcy 64.886% \tValidation Acuuarcy 65.143%\n","Epoch: 101 \tTraining Loss: 0.01349695 \tValidation Loss 0.01389998 \tTraining Acuuarcy 64.722% \tValidation Acuuarcy 65.171%\n","Epoch: 102 \tTraining Loss: 0.01347756 \tValidation Loss 0.01381858 \tTraining Acuuarcy 65.084% \tValidation Acuuarcy 65.589%\n","Epoch: 103 \tTraining Loss: 0.01345756 \tValidation Loss 0.01388878 \tTraining Acuuarcy 65.290% \tValidation Acuuarcy 65.227%\n","Epoch: 104 \tTraining Loss: 0.01345670 \tValidation Loss 0.01381780 \tTraining Acuuarcy 65.422% \tValidation Acuuarcy 64.781%\n","Epoch: 105 \tTraining Loss: 0.01349172 \tValidation Loss 0.01387458 \tTraining Acuuarcy 64.879% \tValidation Acuuarcy 65.506%\n","Epoch: 106 \tTraining Loss: 0.01347794 \tValidation Loss 0.01397186 \tTraining Acuuarcy 65.035% \tValidation Acuuarcy 64.224%\n","Epoch: 107 \tTraining Loss: 0.01349597 \tValidation Loss 0.01405361 \tTraining Acuuarcy 64.805% \tValidation Acuuarcy 64.726%\n","Epoch: 108 \tTraining Loss: 0.01348287 \tValidation Loss 0.01387526 \tTraining Acuuarcy 65.011% \tValidation Acuuarcy 65.673%\n","Epoch: 109 \tTraining Loss: 0.01348718 \tValidation Loss 0.01392506 \tTraining Acuuarcy 64.980% \tValidation Acuuarcy 65.589%\n","Epoch: 110 \tTraining Loss: 0.01347845 \tValidation Loss 0.01390523 \tTraining Acuuarcy 65.046% \tValidation Acuuarcy 64.642%\n","Epoch: 111 \tTraining Loss: 0.01348994 \tValidation Loss 0.01401507 \tTraining Acuuarcy 64.959% \tValidation Acuuarcy 64.363%\n","Epoch: 112 \tTraining Loss: 0.01347206 \tValidation Loss 0.01404669 \tTraining Acuuarcy 65.119% \tValidation Acuuarcy 64.614%\n","Epoch: 113 \tTraining Loss: 0.01349730 \tValidation Loss 0.01395734 \tTraining Acuuarcy 64.805% \tValidation Acuuarcy 65.088%\n","Epoch: 114 \tTraining Loss: 0.01347801 \tValidation Loss 0.01398683 \tTraining Acuuarcy 65.028% \tValidation Acuuarcy 64.753%\n","Epoch: 115 \tTraining Loss: 0.01348854 \tValidation Loss 0.01396126 \tTraining Acuuarcy 64.896% \tValidation Acuuarcy 65.840%\n","Epoch: 116 \tTraining Loss: 0.01348093 \tValidation Loss 0.01397149 \tTraining Acuuarcy 65.014% \tValidation Acuuarcy 64.280%\n","Epoch: 117 \tTraining Loss: 0.01347652 \tValidation Loss 0.01384692 \tTraining Acuuarcy 65.164% \tValidation Acuuarcy 65.060%\n","Epoch: 118 \tTraining Loss: 0.01347680 \tValidation Loss 0.01389388 \tTraining Acuuarcy 65.133% \tValidation Acuuarcy 65.366%\n","Epoch: 119 \tTraining Loss: 0.01348294 \tValidation Loss 0.01395595 \tTraining Acuuarcy 65.021% \tValidation Acuuarcy 64.335%\n","Epoch: 120 \tTraining Loss: 0.01348496 \tValidation Loss 0.01392672 \tTraining Acuuarcy 64.983% \tValidation Acuuarcy 65.534%\n","Epoch: 121 \tTraining Loss: 0.01349046 \tValidation Loss 0.01385179 \tTraining Acuuarcy 64.896% \tValidation Acuuarcy 64.781%\n","Epoch: 122 \tTraining Loss: 0.01348525 \tValidation Loss 0.01381146 \tTraining Acuuarcy 64.906% \tValidation Acuuarcy 65.060%\n","Epoch: 123 \tTraining Loss: 0.01348792 \tValidation Loss 0.01383512 \tTraining Acuuarcy 64.854% \tValidation Acuuarcy 64.670%\n","Epoch: 124 \tTraining Loss: 0.01348454 \tValidation Loss 0.01393837 \tTraining Acuuarcy 64.906% \tValidation Acuuarcy 65.422%\n","Epoch: 125 \tTraining Loss: 0.01350479 \tValidation Loss 0.01392757 \tTraining Acuuarcy 64.642% \tValidation Acuuarcy 64.642%\n","Epoch: 126 \tTraining Loss: 0.01349302 \tValidation Loss 0.01378737 \tTraining Acuuarcy 64.823% \tValidation Acuuarcy 65.868%\n","Epoch: 127 \tTraining Loss: 0.01346306 \tValidation Loss 0.01390707 \tTraining Acuuarcy 65.380% \tValidation Acuuarcy 65.143%\n","Epoch: 128 \tTraining Loss: 0.01347193 \tValidation Loss 0.01380761 \tTraining Acuuarcy 65.112% \tValidation Acuuarcy 65.534%\n","Epoch: 129 \tTraining Loss: 0.01347873 \tValidation Loss 0.01375720 \tTraining Acuuarcy 65.091% \tValidation Acuuarcy 65.701%\n","Epoch: 130 \tTraining Loss: 0.01348678 \tValidation Loss 0.01392375 \tTraining Acuuarcy 64.973% \tValidation Acuuarcy 64.865%\n","Epoch: 131 \tTraining Loss: 0.01348702 \tValidation Loss 0.01390557 \tTraining Acuuarcy 64.948% \tValidation Acuuarcy 65.227%\n","Epoch: 132 \tTraining Loss: 0.01348990 \tValidation Loss 0.01398153 \tTraining Acuuarcy 65.004% \tValidation Acuuarcy 65.534%\n","Epoch: 133 \tTraining Loss: 0.01348953 \tValidation Loss 0.01395860 \tTraining Acuuarcy 64.826% \tValidation Acuuarcy 65.311%\n","Epoch: 134 \tTraining Loss: 0.01349537 \tValidation Loss 0.01382733 \tTraining Acuuarcy 64.816% \tValidation Acuuarcy 64.781%\n","Epoch: 135 \tTraining Loss: 0.01348471 \tValidation Loss 0.01382282 \tTraining Acuuarcy 64.962% \tValidation Acuuarcy 65.506%\n","Epoch: 136 \tTraining Loss: 0.01348667 \tValidation Loss 0.01385644 \tTraining Acuuarcy 64.952% \tValidation Acuuarcy 65.116%\n","Epoch: 137 \tTraining Loss: 0.01349634 \tValidation Loss 0.01389676 \tTraining Acuuarcy 64.861% \tValidation Acuuarcy 64.614%\n","Epoch: 138 \tTraining Loss: 0.01348603 \tValidation Loss 0.01396012 \tTraining Acuuarcy 64.924% \tValidation Acuuarcy 64.698%\n","Epoch: 139 \tTraining Loss: 0.01348249 \tValidation Loss 0.01379163 \tTraining Acuuarcy 65.007% \tValidation Acuuarcy 66.119%\n","Epoch: 140 \tTraining Loss: 0.01350267 \tValidation Loss 0.01397591 \tTraining Acuuarcy 64.823% \tValidation Acuuarcy 64.921%\n","Epoch: 141 \tTraining Loss: 0.01347572 \tValidation Loss 0.01387862 \tTraining Acuuarcy 65.042% \tValidation Acuuarcy 64.809%\n","Epoch: 142 \tTraining Loss: 0.01349808 \tValidation Loss 0.01394837 \tTraining Acuuarcy 64.809% \tValidation Acuuarcy 64.586%\n","Epoch: 143 \tTraining Loss: 0.01347066 \tValidation Loss 0.01386959 \tTraining Acuuarcy 65.178% \tValidation Acuuarcy 64.753%\n","Epoch: 144 \tTraining Loss: 0.01346254 \tValidation Loss 0.01376555 \tTraining Acuuarcy 65.255% \tValidation Acuuarcy 65.506%\n","Epoch: 145 \tTraining Loss: 0.01347841 \tValidation Loss 0.01392354 \tTraining Acuuarcy 65.039% \tValidation Acuuarcy 65.561%\n","Epoch: 146 \tTraining Loss: 0.01348085 \tValidation Loss 0.01389219 \tTraining Acuuarcy 64.966% \tValidation Acuuarcy 64.503%\n","Epoch: 147 \tTraining Loss: 0.01345707 \tValidation Loss 0.01382498 \tTraining Acuuarcy 65.328% \tValidation Acuuarcy 66.063%\n","Epoch: 148 \tTraining Loss: 0.01349567 \tValidation Loss 0.01385945 \tTraining Acuuarcy 64.868% \tValidation Acuuarcy 65.896%\n","Epoch: 149 \tTraining Loss: 0.01351387 \tValidation Loss 0.01396756 \tTraining Acuuarcy 64.656% \tValidation Acuuarcy 65.004%\n","Epoch: 150 \tTraining Loss: 0.01350003 \tValidation Loss 0.01391632 \tTraining Acuuarcy 64.788% \tValidation Acuuarcy 65.506%\n","Epoch: 151 \tTraining Loss: 0.01348336 \tValidation Loss 0.01388696 \tTraining Acuuarcy 65.046% \tValidation Acuuarcy 64.531%\n","Epoch: 152 \tTraining Loss: 0.01347521 \tValidation Loss 0.01378562 \tTraining Acuuarcy 65.109% \tValidation Acuuarcy 65.339%\n","Epoch: 153 \tTraining Loss: 0.01349544 \tValidation Loss 0.01394524 \tTraining Acuuarcy 64.816% \tValidation Acuuarcy 64.447%\n","Epoch: 154 \tTraining Loss: 0.01346094 \tValidation Loss 0.01378438 \tTraining Acuuarcy 65.265% \tValidation Acuuarcy 66.035%\n","Epoch: 155 \tTraining Loss: 0.01348032 \tValidation Loss 0.01386522 \tTraining Acuuarcy 64.990% \tValidation Acuuarcy 64.921%\n","Epoch: 156 \tTraining Loss: 0.01346498 \tValidation Loss 0.01398916 \tTraining Acuuarcy 65.269% \tValidation Acuuarcy 64.948%\n","Epoch: 157 \tTraining Loss: 0.01349428 \tValidation Loss 0.01392380 \tTraining Acuuarcy 64.847% \tValidation Acuuarcy 64.196%\n","Epoch: 158 \tTraining Loss: 0.01347205 \tValidation Loss 0.01393186 \tTraining Acuuarcy 65.171% \tValidation Acuuarcy 64.029%\n","Epoch: 159 \tTraining Loss: 0.01349830 \tValidation Loss 0.01376563 \tTraining Acuuarcy 64.816% \tValidation Acuuarcy 66.982%\n","Epoch: 160 \tTraining Loss: 0.01348581 \tValidation Loss 0.01386046 \tTraining Acuuarcy 64.969% \tValidation Acuuarcy 66.509%\n","Epoch: 161 \tTraining Loss: 0.01347566 \tValidation Loss 0.01382112 \tTraining Acuuarcy 65.098% \tValidation Acuuarcy 64.893%\n","Epoch: 162 \tTraining Loss: 0.01347636 \tValidation Loss 0.01388520 \tTraining Acuuarcy 65.077% \tValidation Acuuarcy 65.506%\n","Epoch: 163 \tTraining Loss: 0.01350112 \tValidation Loss 0.01373596 \tTraining Acuuarcy 64.750% \tValidation Acuuarcy 66.035%\n","Epoch: 164 \tTraining Loss: 0.01348254 \tValidation Loss 0.01381307 \tTraining Acuuarcy 65.021% \tValidation Acuuarcy 65.645%\n","Epoch: 165 \tTraining Loss: 0.01348419 \tValidation Loss 0.01394590 \tTraining Acuuarcy 64.973% \tValidation Acuuarcy 65.116%\n","Epoch: 166 \tTraining Loss: 0.01347969 \tValidation Loss 0.01389544 \tTraining Acuuarcy 64.997% \tValidation Acuuarcy 65.255%\n","Epoch: 167 \tTraining Loss: 0.01348525 \tValidation Loss 0.01393193 \tTraining Acuuarcy 64.976% \tValidation Acuuarcy 66.091%\n","Epoch: 168 \tTraining Loss: 0.01350690 \tValidation Loss 0.01380533 \tTraining Acuuarcy 64.691% \tValidation Acuuarcy 65.645%\n","Epoch: 169 \tTraining Loss: 0.01348782 \tValidation Loss 0.01377052 \tTraining Acuuarcy 64.983% \tValidation Acuuarcy 66.286%\n","Epoch: 170 \tTraining Loss: 0.01347632 \tValidation Loss 0.01385152 \tTraining Acuuarcy 65.109% \tValidation Acuuarcy 65.952%\n","Epoch: 171 \tTraining Loss: 0.01348951 \tValidation Loss 0.01375039 \tTraining Acuuarcy 64.872% \tValidation Acuuarcy 65.673%\n","Epoch: 172 \tTraining Loss: 0.01350774 \tValidation Loss 0.01395296 \tTraining Acuuarcy 64.704% \tValidation Acuuarcy 65.116%\n","Epoch: 173 \tTraining Loss: 0.01349220 \tValidation Loss 0.01378786 \tTraining Acuuarcy 64.805% \tValidation Acuuarcy 65.952%\n","Epoch: 174 \tTraining Loss: 0.01348447 \tValidation Loss 0.01391157 \tTraining Acuuarcy 64.962% \tValidation Acuuarcy 65.032%\n","Epoch: 175 \tTraining Loss: 0.01347469 \tValidation Loss 0.01389523 \tTraining Acuuarcy 65.122% \tValidation Acuuarcy 65.171%\n","Epoch: 176 \tTraining Loss: 0.01346150 \tValidation Loss 0.01395150 \tTraining Acuuarcy 65.248% \tValidation Acuuarcy 64.196%\n","Epoch: 177 \tTraining Loss: 0.01349987 \tValidation Loss 0.01385105 \tTraining Acuuarcy 64.729% \tValidation Acuuarcy 65.729%\n","Epoch: 178 \tTraining Loss: 0.01349638 \tValidation Loss 0.01395421 \tTraining Acuuarcy 64.847% \tValidation Acuuarcy 65.227%\n","Epoch: 179 \tTraining Loss: 0.01349567 \tValidation Loss 0.01399664 \tTraining Acuuarcy 64.767% \tValidation Acuuarcy 65.283%\n","Epoch: 180 \tTraining Loss: 0.01347975 \tValidation Loss 0.01401748 \tTraining Acuuarcy 65.060% \tValidation Acuuarcy 65.199%\n","Epoch: 181 \tTraining Loss: 0.01346776 \tValidation Loss 0.01391305 \tTraining Acuuarcy 65.216% \tValidation Acuuarcy 65.088%\n","Epoch: 182 \tTraining Loss: 0.01349513 \tValidation Loss 0.01389043 \tTraining Acuuarcy 64.798% \tValidation Acuuarcy 65.283%\n","Epoch: 183 \tTraining Loss: 0.01344993 \tValidation Loss 0.01395240 \tTraining Acuuarcy 65.338% \tValidation Acuuarcy 65.311%\n","Epoch: 184 \tTraining Loss: 0.01347173 \tValidation Loss 0.01389365 \tTraining Acuuarcy 65.216% \tValidation Acuuarcy 65.394%\n","Epoch: 185 \tTraining Loss: 0.01346259 \tValidation Loss 0.01391993 \tTraining Acuuarcy 65.269% \tValidation Acuuarcy 65.561%\n","Epoch: 186 \tTraining Loss: 0.01344511 \tValidation Loss 0.01378468 \tTraining Acuuarcy 65.453% \tValidation Acuuarcy 65.227%\n","Epoch: 187 \tTraining Loss: 0.01347063 \tValidation Loss 0.01397361 \tTraining Acuuarcy 65.164% \tValidation Acuuarcy 64.614%\n","Epoch: 188 \tTraining Loss: 0.01347228 \tValidation Loss 0.01387067 \tTraining Acuuarcy 65.185% \tValidation Acuuarcy 65.589%\n","Epoch: 189 \tTraining Loss: 0.01347499 \tValidation Loss 0.01375270 \tTraining Acuuarcy 65.119% \tValidation Acuuarcy 65.701%\n","Epoch: 190 \tTraining Loss: 0.01346604 \tValidation Loss 0.01392289 \tTraining Acuuarcy 65.227% \tValidation Acuuarcy 66.397%\n","Epoch: 191 \tTraining Loss: 0.01348557 \tValidation Loss 0.01377901 \tTraining Acuuarcy 64.941% \tValidation Acuuarcy 66.230%\n","Epoch: 192 \tTraining Loss: 0.01347199 \tValidation Loss 0.01390866 \tTraining Acuuarcy 65.084% \tValidation Acuuarcy 65.812%\n","Epoch: 193 \tTraining Loss: 0.01346234 \tValidation Loss 0.01392961 \tTraining Acuuarcy 65.276% \tValidation Acuuarcy 65.088%\n","Epoch: 194 \tTraining Loss: 0.01346972 \tValidation Loss 0.01384144 \tTraining Acuuarcy 65.199% \tValidation Acuuarcy 66.119%\n","Epoch: 195 \tTraining Loss: 0.01349170 \tValidation Loss 0.01375539 \tTraining Acuuarcy 64.917% \tValidation Acuuarcy 66.369%\n","Epoch: 196 \tTraining Loss: 0.01349122 \tValidation Loss 0.01386958 \tTraining Acuuarcy 64.976% \tValidation Acuuarcy 65.394%\n","Epoch: 197 \tTraining Loss: 0.01350214 \tValidation Loss 0.01384676 \tTraining Acuuarcy 64.764% \tValidation Acuuarcy 65.171%\n","Epoch: 198 \tTraining Loss: 0.01348287 \tValidation Loss 0.01395530 \tTraining Acuuarcy 64.969% \tValidation Acuuarcy 65.199%\n","Epoch: 199 \tTraining Loss: 0.01347169 \tValidation Loss 0.01380989 \tTraining Acuuarcy 65.122% \tValidation Acuuarcy 65.506%\n","Epoch: 200 \tTraining Loss: 0.01347841 \tValidation Loss 0.01395764 \tTraining Acuuarcy 65.046% \tValidation Acuuarcy 64.475%\n","Epoch: 201 \tTraining Loss: 0.01346369 \tValidation Loss 0.01385779 \tTraining Acuuarcy 65.216% \tValidation Acuuarcy 65.227%\n","Epoch: 202 \tTraining Loss: 0.01348077 \tValidation Loss 0.01383322 \tTraining Acuuarcy 65.018% \tValidation Acuuarcy 65.450%\n","Epoch: 203 \tTraining Loss: 0.01345106 \tValidation Loss 0.01392880 \tTraining Acuuarcy 65.408% \tValidation Acuuarcy 65.339%\n","Epoch: 204 \tTraining Loss: 0.01345985 \tValidation Loss 0.01395281 \tTraining Acuuarcy 65.307% \tValidation Acuuarcy 65.227%\n","Epoch: 205 \tTraining Loss: 0.01347855 \tValidation Loss 0.01384319 \tTraining Acuuarcy 65.095% \tValidation Acuuarcy 65.283%\n","Epoch: 206 \tTraining Loss: 0.01348159 \tValidation Loss 0.01383841 \tTraining Acuuarcy 65.105% \tValidation Acuuarcy 64.670%\n","Epoch: 207 \tTraining Loss: 0.01347174 \tValidation Loss 0.01379267 \tTraining Acuuarcy 65.081% \tValidation Acuuarcy 65.812%\n","Epoch: 208 \tTraining Loss: 0.01344593 \tValidation Loss 0.01380814 \tTraining Acuuarcy 65.544% \tValidation Acuuarcy 65.589%\n","Epoch: 209 \tTraining Loss: 0.01347511 \tValidation Loss 0.01388259 \tTraining Acuuarcy 65.067% \tValidation Acuuarcy 65.255%\n","Epoch: 210 \tTraining Loss: 0.01345420 \tValidation Loss 0.01399262 \tTraining Acuuarcy 65.513% \tValidation Acuuarcy 65.199%\n","Epoch: 211 \tTraining Loss: 0.01347359 \tValidation Loss 0.01386718 \tTraining Acuuarcy 65.147% \tValidation Acuuarcy 66.314%\n","Epoch: 212 \tTraining Loss: 0.01347307 \tValidation Loss 0.01394866 \tTraining Acuuarcy 65.088% \tValidation Acuuarcy 65.255%\n","Epoch: 213 \tTraining Loss: 0.01347300 \tValidation Loss 0.01397015 \tTraining Acuuarcy 65.129% \tValidation Acuuarcy 64.921%\n","Epoch: 214 \tTraining Loss: 0.01346066 \tValidation Loss 0.01376233 \tTraining Acuuarcy 65.307% \tValidation Acuuarcy 65.979%\n","Epoch: 215 \tTraining Loss: 0.01346957 \tValidation Loss 0.01400851 \tTraining Acuuarcy 65.178% \tValidation Acuuarcy 65.116%\n","Epoch: 216 \tTraining Loss: 0.01347641 \tValidation Loss 0.01381780 \tTraining Acuuarcy 65.088% \tValidation Acuuarcy 65.478%\n","Epoch: 217 \tTraining Loss: 0.01347279 \tValidation Loss 0.01392900 \tTraining Acuuarcy 65.140% \tValidation Acuuarcy 64.837%\n","Epoch: 218 \tTraining Loss: 0.01347013 \tValidation Loss 0.01391781 \tTraining Acuuarcy 65.109% \tValidation Acuuarcy 64.893%\n","Epoch: 219 \tTraining Loss: 0.01348172 \tValidation Loss 0.01382666 \tTraining Acuuarcy 65.028% \tValidation Acuuarcy 65.311%\n","Epoch: 220 \tTraining Loss: 0.01344511 \tValidation Loss 0.01378132 \tTraining Acuuarcy 65.513% \tValidation Acuuarcy 65.450%\n","Epoch: 221 \tTraining Loss: 0.01345715 \tValidation Loss 0.01389152 \tTraining Acuuarcy 65.384% \tValidation Acuuarcy 65.450%\n","Epoch: 222 \tTraining Loss: 0.01347649 \tValidation Loss 0.01400084 \tTraining Acuuarcy 65.084% \tValidation Acuuarcy 65.227%\n","Epoch: 223 \tTraining Loss: 0.01345993 \tValidation Loss 0.01393050 \tTraining Acuuarcy 65.370% \tValidation Acuuarcy 64.614%\n","Epoch: 224 \tTraining Loss: 0.01346077 \tValidation Loss 0.01376373 \tTraining Acuuarcy 65.356% \tValidation Acuuarcy 66.314%\n","Epoch: 225 \tTraining Loss: 0.01346020 \tValidation Loss 0.01397487 \tTraining Acuuarcy 65.265% \tValidation Acuuarcy 64.781%\n","Epoch: 226 \tTraining Loss: 0.01346124 \tValidation Loss 0.01379208 \tTraining Acuuarcy 65.269% \tValidation Acuuarcy 65.812%\n","Epoch: 227 \tTraining Loss: 0.01347222 \tValidation Loss 0.01388593 \tTraining Acuuarcy 65.084% \tValidation Acuuarcy 65.366%\n","Epoch: 228 \tTraining Loss: 0.01346704 \tValidation Loss 0.01383103 \tTraining Acuuarcy 65.189% \tValidation Acuuarcy 65.394%\n","Epoch: 229 \tTraining Loss: 0.01346074 \tValidation Loss 0.01384781 \tTraining Acuuarcy 65.304% \tValidation Acuuarcy 65.924%\n","Epoch: 230 \tTraining Loss: 0.01347937 \tValidation Loss 0.01382610 \tTraining Acuuarcy 65.053% \tValidation Acuuarcy 66.119%\n","Epoch: 231 \tTraining Loss: 0.01344171 \tValidation Loss 0.01398210 \tTraining Acuuarcy 65.502% \tValidation Acuuarcy 64.726%\n","Epoch: 232 \tTraining Loss: 0.01345732 \tValidation Loss 0.01391085 \tTraining Acuuarcy 65.380% \tValidation Acuuarcy 65.701%\n","Epoch: 233 \tTraining Loss: 0.01346959 \tValidation Loss 0.01388087 \tTraining Acuuarcy 65.161% \tValidation Acuuarcy 64.586%\n","Epoch: 234 \tTraining Loss: 0.01347622 \tValidation Loss 0.01388109 \tTraining Acuuarcy 65.081% \tValidation Acuuarcy 66.091%\n","Epoch: 235 \tTraining Loss: 0.01345354 \tValidation Loss 0.01375278 \tTraining Acuuarcy 65.432% \tValidation Acuuarcy 65.645%\n","Epoch: 236 \tTraining Loss: 0.01345138 \tValidation Loss 0.01388728 \tTraining Acuuarcy 65.432% \tValidation Acuuarcy 65.311%\n","Epoch: 237 \tTraining Loss: 0.01346129 \tValidation Loss 0.01387771 \tTraining Acuuarcy 65.258% \tValidation Acuuarcy 65.617%\n","Epoch: 238 \tTraining Loss: 0.01349285 \tValidation Loss 0.01399470 \tTraining Acuuarcy 64.903% \tValidation Acuuarcy 65.311%\n","Epoch: 239 \tTraining Loss: 0.01346359 \tValidation Loss 0.01396389 \tTraining Acuuarcy 65.324% \tValidation Acuuarcy 64.921%\n","Epoch: 240 \tTraining Loss: 0.01347436 \tValidation Loss 0.01388490 \tTraining Acuuarcy 65.077% \tValidation Acuuarcy 65.394%\n","Epoch: 241 \tTraining Loss: 0.01345361 \tValidation Loss 0.01381733 \tTraining Acuuarcy 65.304% \tValidation Acuuarcy 65.255%\n","Epoch: 242 \tTraining Loss: 0.01344269 \tValidation Loss 0.01372660 \tTraining Acuuarcy 65.499% \tValidation Acuuarcy 66.843%\n","Epoch: 243 \tTraining Loss: 0.01345750 \tValidation Loss 0.01398393 \tTraining Acuuarcy 65.300% \tValidation Acuuarcy 64.001%\n","Epoch: 244 \tTraining Loss: 0.01346815 \tValidation Loss 0.01383942 \tTraining Acuuarcy 65.185% \tValidation Acuuarcy 65.366%\n","Epoch: 245 \tTraining Loss: 0.01346804 \tValidation Loss 0.01391026 \tTraining Acuuarcy 65.241% \tValidation Acuuarcy 65.701%\n","Epoch: 246 \tTraining Loss: 0.01347280 \tValidation Loss 0.01384360 \tTraining Acuuarcy 65.112% \tValidation Acuuarcy 65.032%\n","Epoch: 247 \tTraining Loss: 0.01348377 \tValidation Loss 0.01395771 \tTraining Acuuarcy 64.973% \tValidation Acuuarcy 65.701%\n","Epoch: 248 \tTraining Loss: 0.01347772 \tValidation Loss 0.01378339 \tTraining Acuuarcy 64.945% \tValidation Acuuarcy 65.311%\n","Epoch: 249 \tTraining Loss: 0.01349204 \tValidation Loss 0.01387814 \tTraining Acuuarcy 64.875% \tValidation Acuuarcy 65.534%\n","Epoch: 250 \tTraining Loss: 0.01346749 \tValidation Loss 0.01401398 \tTraining Acuuarcy 65.175% \tValidation Acuuarcy 65.088%\n","Epoch: 251 \tTraining Loss: 0.01347504 \tValidation Loss 0.01382528 \tTraining Acuuarcy 65.098% \tValidation Acuuarcy 65.450%\n","Epoch: 252 \tTraining Loss: 0.01345972 \tValidation Loss 0.01396655 \tTraining Acuuarcy 65.293% \tValidation Acuuarcy 64.893%\n","Epoch: 253 \tTraining Loss: 0.01344033 \tValidation Loss 0.01388286 \tTraining Acuuarcy 65.544% \tValidation Acuuarcy 65.116%\n","Epoch: 254 \tTraining Loss: 0.01346100 \tValidation Loss 0.01396700 \tTraining Acuuarcy 65.227% \tValidation Acuuarcy 64.976%\n","Epoch: 255 \tTraining Loss: 0.01346513 \tValidation Loss 0.01387872 \tTraining Acuuarcy 65.185% \tValidation Acuuarcy 65.143%\n","Epoch: 256 \tTraining Loss: 0.01348313 \tValidation Loss 0.01393305 \tTraining Acuuarcy 65.032% \tValidation Acuuarcy 65.506%\n","Epoch: 257 \tTraining Loss: 0.01345212 \tValidation Loss 0.01384397 \tTraining Acuuarcy 65.412% \tValidation Acuuarcy 65.032%\n","Epoch: 258 \tTraining Loss: 0.01347260 \tValidation Loss 0.01386689 \tTraining Acuuarcy 65.119% \tValidation Acuuarcy 65.394%\n","Epoch: 259 \tTraining Loss: 0.01345837 \tValidation Loss 0.01385805 \tTraining Acuuarcy 65.311% \tValidation Acuuarcy 65.366%\n","Epoch: 260 \tTraining Loss: 0.01345871 \tValidation Loss 0.01382910 \tTraining Acuuarcy 65.304% \tValidation Acuuarcy 65.896%\n","Epoch: 261 \tTraining Loss: 0.01346934 \tValidation Loss 0.01387640 \tTraining Acuuarcy 65.171% \tValidation Acuuarcy 65.422%\n","Epoch: 262 \tTraining Loss: 0.01346242 \tValidation Loss 0.01394966 \tTraining Acuuarcy 65.244% \tValidation Acuuarcy 65.227%\n","Epoch: 263 \tTraining Loss: 0.01347949 \tValidation Loss 0.01391541 \tTraining Acuuarcy 64.987% \tValidation Acuuarcy 64.976%\n","Epoch: 264 \tTraining Loss: 0.01347174 \tValidation Loss 0.01389185 \tTraining Acuuarcy 65.182% \tValidation Acuuarcy 65.366%\n","Epoch: 265 \tTraining Loss: 0.01347029 \tValidation Loss 0.01385901 \tTraining Acuuarcy 65.115% \tValidation Acuuarcy 65.088%\n","Epoch: 266 \tTraining Loss: 0.01346966 \tValidation Loss 0.01380437 \tTraining Acuuarcy 65.157% \tValidation Acuuarcy 65.701%\n","Epoch: 267 \tTraining Loss: 0.01347028 \tValidation Loss 0.01384455 \tTraining Acuuarcy 65.105% \tValidation Acuuarcy 65.812%\n","Epoch: 268 \tTraining Loss: 0.01347385 \tValidation Loss 0.01383683 \tTraining Acuuarcy 65.150% \tValidation Acuuarcy 65.227%\n","Epoch: 269 \tTraining Loss: 0.01346865 \tValidation Loss 0.01382566 \tTraining Acuuarcy 65.262% \tValidation Acuuarcy 65.478%\n","Epoch: 270 \tTraining Loss: 0.01347091 \tValidation Loss 0.01385808 \tTraining Acuuarcy 65.126% \tValidation Acuuarcy 65.729%\n","Epoch: 271 \tTraining Loss: 0.01349826 \tValidation Loss 0.01388148 \tTraining Acuuarcy 64.802% \tValidation Acuuarcy 64.976%\n","Epoch: 272 \tTraining Loss: 0.01348159 \tValidation Loss 0.01376711 \tTraining Acuuarcy 65.021% \tValidation Acuuarcy 65.561%\n","Epoch: 273 \tTraining Loss: 0.01347528 \tValidation Loss 0.01382148 \tTraining Acuuarcy 65.115% \tValidation Acuuarcy 65.617%\n","Epoch: 274 \tTraining Loss: 0.01345241 \tValidation Loss 0.01389614 \tTraining Acuuarcy 65.398% \tValidation Acuuarcy 65.311%\n","Epoch: 275 \tTraining Loss: 0.01343134 \tValidation Loss 0.01391305 \tTraining Acuuarcy 65.628% \tValidation Acuuarcy 65.088%\n","Epoch: 276 \tTraining Loss: 0.01347981 \tValidation Loss 0.01389117 \tTraining Acuuarcy 65.070% \tValidation Acuuarcy 65.952%\n","Epoch: 277 \tTraining Loss: 0.01347793 \tValidation Loss 0.01386204 \tTraining Acuuarcy 65.028% \tValidation Acuuarcy 65.478%\n","Epoch: 278 \tTraining Loss: 0.01345511 \tValidation Loss 0.01390884 \tTraining Acuuarcy 65.394% \tValidation Acuuarcy 65.701%\n","Epoch: 279 \tTraining Loss: 0.01349973 \tValidation Loss 0.01383454 \tTraining Acuuarcy 64.785% \tValidation Acuuarcy 65.478%\n","Epoch: 280 \tTraining Loss: 0.01347516 \tValidation Loss 0.01387232 \tTraining Acuuarcy 65.126% \tValidation Acuuarcy 65.589%\n","Epoch: 281 \tTraining Loss: 0.01347354 \tValidation Loss 0.01388230 \tTraining Acuuarcy 65.053% \tValidation Acuuarcy 64.642%\n","Epoch: 282 \tTraining Loss: 0.01347770 \tValidation Loss 0.01392277 \tTraining Acuuarcy 65.098% \tValidation Acuuarcy 65.645%\n","Epoch: 283 \tTraining Loss: 0.01346370 \tValidation Loss 0.01391420 \tTraining Acuuarcy 65.227% \tValidation Acuuarcy 65.812%\n","Epoch: 284 \tTraining Loss: 0.01345045 \tValidation Loss 0.01384479 \tTraining Acuuarcy 65.467% \tValidation Acuuarcy 65.116%\n","Epoch: 285 \tTraining Loss: 0.01347681 \tValidation Loss 0.01381036 \tTraining Acuuarcy 64.994% \tValidation Acuuarcy 65.617%\n","Epoch: 286 \tTraining Loss: 0.01347147 \tValidation Loss 0.01374211 \tTraining Acuuarcy 65.129% \tValidation Acuuarcy 65.812%\n","Epoch: 287 \tTraining Loss: 0.01347400 \tValidation Loss 0.01383076 \tTraining Acuuarcy 65.143% \tValidation Acuuarcy 65.422%\n","Epoch: 288 \tTraining Loss: 0.01346672 \tValidation Loss 0.01382246 \tTraining Acuuarcy 65.196% \tValidation Acuuarcy 65.589%\n","Epoch: 289 \tTraining Loss: 0.01348117 \tValidation Loss 0.01386508 \tTraining Acuuarcy 65.004% \tValidation Acuuarcy 65.589%\n","Epoch: 290 \tTraining Loss: 0.01346769 \tValidation Loss 0.01390830 \tTraining Acuuarcy 65.251% \tValidation Acuuarcy 65.896%\n","Epoch: 291 \tTraining Loss: 0.01344647 \tValidation Loss 0.01392759 \tTraining Acuuarcy 65.575% \tValidation Acuuarcy 64.001%\n","Epoch: 292 \tTraining Loss: 0.01346335 \tValidation Loss 0.01379001 \tTraining Acuuarcy 65.297% \tValidation Acuuarcy 65.896%\n","Epoch: 293 \tTraining Loss: 0.01347007 \tValidation Loss 0.01383272 \tTraining Acuuarcy 65.161% \tValidation Acuuarcy 65.366%\n","Epoch: 294 \tTraining Loss: 0.01347122 \tValidation Loss 0.01382440 \tTraining Acuuarcy 65.150% \tValidation Acuuarcy 65.729%\n","Epoch: 295 \tTraining Loss: 0.01344978 \tValidation Loss 0.01381498 \tTraining Acuuarcy 65.425% \tValidation Acuuarcy 65.645%\n","Epoch: 296 \tTraining Loss: 0.01345106 \tValidation Loss 0.01387761 \tTraining Acuuarcy 65.370% \tValidation Acuuarcy 65.534%\n","Epoch: 297 \tTraining Loss: 0.01346254 \tValidation Loss 0.01395309 \tTraining Acuuarcy 65.283% \tValidation Acuuarcy 65.060%\n","Epoch: 298 \tTraining Loss: 0.01346117 \tValidation Loss 0.01390889 \tTraining Acuuarcy 65.286% \tValidation Acuuarcy 65.116%\n","Epoch: 299 \tTraining Loss: 0.01345095 \tValidation Loss 0.01395321 \tTraining Acuuarcy 65.457% \tValidation Acuuarcy 65.116%\n","Epoch: 300 \tTraining Loss: 0.01345928 \tValidation Loss 0.01377402 \tTraining Acuuarcy 65.258% \tValidation Acuuarcy 66.007%\n","Epoch: 301 \tTraining Loss: 0.01346266 \tValidation Loss 0.01379078 \tTraining Acuuarcy 65.262% \tValidation Acuuarcy 66.035%\n","Epoch: 302 \tTraining Loss: 0.01344372 \tValidation Loss 0.01386812 \tTraining Acuuarcy 65.443% \tValidation Acuuarcy 65.506%\n","Epoch: 303 \tTraining Loss: 0.01348993 \tValidation Loss 0.01387901 \tTraining Acuuarcy 64.900% \tValidation Acuuarcy 64.753%\n","Epoch: 304 \tTraining Loss: 0.01344542 \tValidation Loss 0.01374504 \tTraining Acuuarcy 65.481% \tValidation Acuuarcy 65.840%\n","Epoch: 305 \tTraining Loss: 0.01345007 \tValidation Loss 0.01382409 \tTraining Acuuarcy 65.422% \tValidation Acuuarcy 65.394%\n","Epoch: 306 \tTraining Loss: 0.01345263 \tValidation Loss 0.01391722 \tTraining Acuuarcy 65.384% \tValidation Acuuarcy 65.534%\n","Epoch: 307 \tTraining Loss: 0.01347211 \tValidation Loss 0.01389897 \tTraining Acuuarcy 65.164% \tValidation Acuuarcy 65.394%\n","Epoch: 308 \tTraining Loss: 0.01347510 \tValidation Loss 0.01390132 \tTraining Acuuarcy 65.063% \tValidation Acuuarcy 64.391%\n","Epoch: 309 \tTraining Loss: 0.01345803 \tValidation Loss 0.01394110 \tTraining Acuuarcy 65.311% \tValidation Acuuarcy 64.976%\n","Epoch: 310 \tTraining Loss: 0.01345503 \tValidation Loss 0.01378339 \tTraining Acuuarcy 65.415% \tValidation Acuuarcy 65.366%\n","Epoch: 311 \tTraining Loss: 0.01344285 \tValidation Loss 0.01388730 \tTraining Acuuarcy 65.450% \tValidation Acuuarcy 65.283%\n","Epoch: 312 \tTraining Loss: 0.01345591 \tValidation Loss 0.01387737 \tTraining Acuuarcy 65.349% \tValidation Acuuarcy 64.586%\n","Epoch: 313 \tTraining Loss: 0.01345886 \tValidation Loss 0.01375659 \tTraining Acuuarcy 65.265% \tValidation Acuuarcy 65.617%\n","Epoch: 314 \tTraining Loss: 0.01346275 \tValidation Loss 0.01379250 \tTraining Acuuarcy 65.213% \tValidation Acuuarcy 65.979%\n","Epoch: 315 \tTraining Loss: 0.01344910 \tValidation Loss 0.01377935 \tTraining Acuuarcy 65.394% \tValidation Acuuarcy 65.506%\n","Epoch: 316 \tTraining Loss: 0.01345697 \tValidation Loss 0.01381715 \tTraining Acuuarcy 65.352% \tValidation Acuuarcy 65.506%\n","Epoch: 317 \tTraining Loss: 0.01346904 \tValidation Loss 0.01388359 \tTraining Acuuarcy 65.150% \tValidation Acuuarcy 65.143%\n","Epoch: 318 \tTraining Loss: 0.01344255 \tValidation Loss 0.01388358 \tTraining Acuuarcy 65.540% \tValidation Acuuarcy 65.394%\n","Epoch: 319 \tTraining Loss: 0.01343193 \tValidation Loss 0.01392386 \tTraining Acuuarcy 65.641% \tValidation Acuuarcy 65.645%\n","Epoch: 320 \tTraining Loss: 0.01347854 \tValidation Loss 0.01375722 \tTraining Acuuarcy 64.990% \tValidation Acuuarcy 65.701%\n","Epoch: 321 \tTraining Loss: 0.01342732 \tValidation Loss 0.01380107 \tTraining Acuuarcy 65.683% \tValidation Acuuarcy 65.227%\n","Epoch: 322 \tTraining Loss: 0.01348590 \tValidation Loss 0.01385410 \tTraining Acuuarcy 64.980% \tValidation Acuuarcy 64.921%\n","Epoch: 323 \tTraining Loss: 0.01347230 \tValidation Loss 0.01383468 \tTraining Acuuarcy 65.223% \tValidation Acuuarcy 66.230%\n","Epoch: 324 \tTraining Loss: 0.01347020 \tValidation Loss 0.01374179 \tTraining Acuuarcy 65.119% \tValidation Acuuarcy 65.840%\n","Epoch: 325 \tTraining Loss: 0.01346022 \tValidation Loss 0.01383942 \tTraining Acuuarcy 65.234% \tValidation Acuuarcy 65.422%\n","Epoch: 326 \tTraining Loss: 0.01345507 \tValidation Loss 0.01394513 \tTraining Acuuarcy 65.363% \tValidation Acuuarcy 65.227%\n","Epoch: 327 \tTraining Loss: 0.01346190 \tValidation Loss 0.01380728 \tTraining Acuuarcy 65.262% \tValidation Acuuarcy 65.394%\n","Epoch: 328 \tTraining Loss: 0.01347053 \tValidation Loss 0.01387295 \tTraining Acuuarcy 65.171% \tValidation Acuuarcy 65.394%\n","Epoch: 329 \tTraining Loss: 0.01346851 \tValidation Loss 0.01378472 \tTraining Acuuarcy 65.154% \tValidation Acuuarcy 65.311%\n","Epoch: 330 \tTraining Loss: 0.01346255 \tValidation Loss 0.01384836 \tTraining Acuuarcy 65.283% \tValidation Acuuarcy 65.283%\n","Epoch: 331 \tTraining Loss: 0.01345967 \tValidation Loss 0.01386654 \tTraining Acuuarcy 65.216% \tValidation Acuuarcy 65.701%\n","Epoch: 332 \tTraining Loss: 0.01346006 \tValidation Loss 0.01396341 \tTraining Acuuarcy 65.321% \tValidation Acuuarcy 64.670%\n","Epoch: 333 \tTraining Loss: 0.01348017 \tValidation Loss 0.01383020 \tTraining Acuuarcy 65.014% \tValidation Acuuarcy 66.174%\n","Epoch: 334 \tTraining Loss: 0.01345565 \tValidation Loss 0.01388565 \tTraining Acuuarcy 65.321% \tValidation Acuuarcy 65.366%\n","Epoch: 335 \tTraining Loss: 0.01349182 \tValidation Loss 0.01383494 \tTraining Acuuarcy 64.903% \tValidation Acuuarcy 65.589%\n","Epoch: 336 \tTraining Loss: 0.01347713 \tValidation Loss 0.01389385 \tTraining Acuuarcy 65.122% \tValidation Acuuarcy 65.088%\n","Epoch: 337 \tTraining Loss: 0.01345271 \tValidation Loss 0.01383609 \tTraining Acuuarcy 65.338% \tValidation Acuuarcy 65.979%\n","Epoch: 338 \tTraining Loss: 0.01342339 \tValidation Loss 0.01395080 \tTraining Acuuarcy 65.836% \tValidation Acuuarcy 65.060%\n","Epoch: 339 \tTraining Loss: 0.01347088 \tValidation Loss 0.01400042 \tTraining Acuuarcy 65.133% \tValidation Acuuarcy 65.339%\n","Epoch: 340 \tTraining Loss: 0.01344203 \tValidation Loss 0.01388205 \tTraining Acuuarcy 65.540% \tValidation Acuuarcy 64.781%\n","Epoch: 341 \tTraining Loss: 0.01346240 \tValidation Loss 0.01378341 \tTraining Acuuarcy 65.283% \tValidation Acuuarcy 66.035%\n","Epoch: 342 \tTraining Loss: 0.01346463 \tValidation Loss 0.01387288 \tTraining Acuuarcy 65.234% \tValidation Acuuarcy 65.506%\n","Epoch: 343 \tTraining Loss: 0.01347519 \tValidation Loss 0.01376256 \tTraining Acuuarcy 65.115% \tValidation Acuuarcy 66.286%\n","Epoch: 344 \tTraining Loss: 0.01345506 \tValidation Loss 0.01394058 \tTraining Acuuarcy 65.307% \tValidation Acuuarcy 64.558%\n","Epoch: 345 \tTraining Loss: 0.01344611 \tValidation Loss 0.01395366 \tTraining Acuuarcy 65.439% \tValidation Acuuarcy 65.227%\n","Epoch: 346 \tTraining Loss: 0.01345914 \tValidation Loss 0.01385121 \tTraining Acuuarcy 65.265% \tValidation Acuuarcy 65.339%\n","Epoch: 347 \tTraining Loss: 0.01344628 \tValidation Loss 0.01382945 \tTraining Acuuarcy 65.513% \tValidation Acuuarcy 65.478%\n","Epoch: 348 \tTraining Loss: 0.01347665 \tValidation Loss 0.01393203 \tTraining Acuuarcy 65.067% \tValidation Acuuarcy 65.534%\n","Epoch: 349 \tTraining Loss: 0.01347072 \tValidation Loss 0.01394231 \tTraining Acuuarcy 65.192% \tValidation Acuuarcy 65.534%\n","Epoch: 350 \tTraining Loss: 0.01344928 \tValidation Loss 0.01383233 \tTraining Acuuarcy 65.419% \tValidation Acuuarcy 65.032%\n","Epoch: 351 \tTraining Loss: 0.01344881 \tValidation Loss 0.01382232 \tTraining Acuuarcy 65.415% \tValidation Acuuarcy 66.063%\n","Epoch: 352 \tTraining Loss: 0.01346095 \tValidation Loss 0.01387715 \tTraining Acuuarcy 65.304% \tValidation Acuuarcy 65.506%\n","Epoch: 353 \tTraining Loss: 0.01345617 \tValidation Loss 0.01393577 \tTraining Acuuarcy 65.286% \tValidation Acuuarcy 65.116%\n","Epoch: 354 \tTraining Loss: 0.01346901 \tValidation Loss 0.01382682 \tTraining Acuuarcy 65.175% \tValidation Acuuarcy 64.586%\n","Epoch: 355 \tTraining Loss: 0.01345792 \tValidation Loss 0.01391130 \tTraining Acuuarcy 65.384% \tValidation Acuuarcy 65.283%\n","Epoch: 356 \tTraining Loss: 0.01347426 \tValidation Loss 0.01387600 \tTraining Acuuarcy 65.098% \tValidation Acuuarcy 66.091%\n","Epoch: 357 \tTraining Loss: 0.01345371 \tValidation Loss 0.01384326 \tTraining Acuuarcy 65.349% \tValidation Acuuarcy 65.199%\n","Epoch: 358 \tTraining Loss: 0.01348651 \tValidation Loss 0.01378507 \tTraining Acuuarcy 64.994% \tValidation Acuuarcy 65.283%\n","Epoch: 359 \tTraining Loss: 0.01347020 \tValidation Loss 0.01373833 \tTraining Acuuarcy 65.168% \tValidation Acuuarcy 66.147%\n","Epoch: 360 \tTraining Loss: 0.01345568 \tValidation Loss 0.01389238 \tTraining Acuuarcy 65.359% \tValidation Acuuarcy 65.506%\n","Epoch: 361 \tTraining Loss: 0.01346222 \tValidation Loss 0.01387022 \tTraining Acuuarcy 65.220% \tValidation Acuuarcy 65.645%\n","Epoch: 362 \tTraining Loss: 0.01346002 \tValidation Loss 0.01372752 \tTraining Acuuarcy 65.307% \tValidation Acuuarcy 66.899%\n","Epoch: 363 \tTraining Loss: 0.01346226 \tValidation Loss 0.01383384 \tTraining Acuuarcy 65.175% \tValidation Acuuarcy 66.091%\n","Epoch: 364 \tTraining Loss: 0.01343221 \tValidation Loss 0.01377829 \tTraining Acuuarcy 65.621% \tValidation Acuuarcy 65.979%\n","Epoch: 365 \tTraining Loss: 0.01346079 \tValidation Loss 0.01386691 \tTraining Acuuarcy 65.286% \tValidation Acuuarcy 65.534%\n","Epoch: 366 \tTraining Loss: 0.01346692 \tValidation Loss 0.01380594 \tTraining Acuuarcy 65.220% \tValidation Acuuarcy 65.924%\n","Epoch: 367 \tTraining Loss: 0.01347122 \tValidation Loss 0.01383963 \tTraining Acuuarcy 65.074% \tValidation Acuuarcy 65.339%\n","Epoch: 368 \tTraining Loss: 0.01344493 \tValidation Loss 0.01383276 \tTraining Acuuarcy 65.474% \tValidation Acuuarcy 65.394%\n","Epoch: 369 \tTraining Loss: 0.01346598 \tValidation Loss 0.01374483 \tTraining Acuuarcy 65.248% \tValidation Acuuarcy 65.701%\n","Epoch: 370 \tTraining Loss: 0.01347410 \tValidation Loss 0.01397139 \tTraining Acuuarcy 65.185% \tValidation Acuuarcy 65.506%\n","Epoch: 371 \tTraining Loss: 0.01345764 \tValidation Loss 0.01383142 \tTraining Acuuarcy 65.342% \tValidation Acuuarcy 65.394%\n","Epoch: 372 \tTraining Loss: 0.01348500 \tValidation Loss 0.01389669 \tTraining Acuuarcy 64.980% \tValidation Acuuarcy 66.035%\n","Epoch: 373 \tTraining Loss: 0.01345733 \tValidation Loss 0.01385507 \tTraining Acuuarcy 65.352% \tValidation Acuuarcy 65.088%\n","Epoch: 374 \tTraining Loss: 0.01344802 \tValidation Loss 0.01387392 \tTraining Acuuarcy 65.432% \tValidation Acuuarcy 64.531%\n","Epoch: 375 \tTraining Loss: 0.01347733 \tValidation Loss 0.01384650 \tTraining Acuuarcy 65.067% \tValidation Acuuarcy 65.756%\n","Epoch: 376 \tTraining Loss: 0.01346031 \tValidation Loss 0.01377223 \tTraining Acuuarcy 65.290% \tValidation Acuuarcy 65.394%\n","Epoch: 377 \tTraining Loss: 0.01344548 \tValidation Loss 0.01371780 \tTraining Acuuarcy 65.467% \tValidation Acuuarcy 66.091%\n","Epoch: 378 \tTraining Loss: 0.01344311 \tValidation Loss 0.01377910 \tTraining Acuuarcy 65.488% \tValidation Acuuarcy 65.422%\n","Epoch: 379 \tTraining Loss: 0.01344983 \tValidation Loss 0.01391777 \tTraining Acuuarcy 65.359% \tValidation Acuuarcy 64.865%\n","Epoch: 380 \tTraining Loss: 0.01342078 \tValidation Loss 0.01379100 \tTraining Acuuarcy 65.795% \tValidation Acuuarcy 65.032%\n","Epoch: 381 \tTraining Loss: 0.01341616 \tValidation Loss 0.01378611 \tTraining Acuuarcy 65.819% \tValidation Acuuarcy 65.255%\n","Epoch: 382 \tTraining Loss: 0.01345006 \tValidation Loss 0.01386004 \tTraining Acuuarcy 65.425% \tValidation Acuuarcy 64.865%\n","Epoch: 383 \tTraining Loss: 0.01345440 \tValidation Loss 0.01382172 \tTraining Acuuarcy 65.331% \tValidation Acuuarcy 65.366%\n","Epoch: 384 \tTraining Loss: 0.01344303 \tValidation Loss 0.01388882 \tTraining Acuuarcy 65.481% \tValidation Acuuarcy 65.450%\n","Epoch: 385 \tTraining Loss: 0.01346220 \tValidation Loss 0.01387311 \tTraining Acuuarcy 65.338% \tValidation Acuuarcy 65.032%\n","Epoch: 386 \tTraining Loss: 0.01344837 \tValidation Loss 0.01375251 \tTraining Acuuarcy 65.450% \tValidation Acuuarcy 65.673%\n","Epoch: 387 \tTraining Loss: 0.01344777 \tValidation Loss 0.01382563 \tTraining Acuuarcy 65.425% \tValidation Acuuarcy 65.143%\n","Epoch: 388 \tTraining Loss: 0.01343409 \tValidation Loss 0.01380836 \tTraining Acuuarcy 65.607% \tValidation Acuuarcy 64.976%\n","Epoch: 389 \tTraining Loss: 0.01344888 \tValidation Loss 0.01372951 \tTraining Acuuarcy 65.401% \tValidation Acuuarcy 66.035%\n","Epoch: 390 \tTraining Loss: 0.01346133 \tValidation Loss 0.01395014 \tTraining Acuuarcy 65.317% \tValidation Acuuarcy 65.255%\n","Epoch: 391 \tTraining Loss: 0.01344719 \tValidation Loss 0.01373327 \tTraining Acuuarcy 65.492% \tValidation Acuuarcy 66.592%\n","Epoch: 392 \tTraining Loss: 0.01345853 \tValidation Loss 0.01386341 \tTraining Acuuarcy 65.373% \tValidation Acuuarcy 65.589%\n","Epoch: 393 \tTraining Loss: 0.01344253 \tValidation Loss 0.01384203 \tTraining Acuuarcy 65.443% \tValidation Acuuarcy 65.394%\n","Epoch: 394 \tTraining Loss: 0.01343992 \tValidation Loss 0.01392319 \tTraining Acuuarcy 65.544% \tValidation Acuuarcy 64.670%\n","Epoch: 395 \tTraining Loss: 0.01345052 \tValidation Loss 0.01388637 \tTraining Acuuarcy 65.415% \tValidation Acuuarcy 65.227%\n","Epoch: 396 \tTraining Loss: 0.01345939 \tValidation Loss 0.01389127 \tTraining Acuuarcy 65.276% \tValidation Acuuarcy 64.531%\n","Epoch: 397 \tTraining Loss: 0.01344726 \tValidation Loss 0.01394222 \tTraining Acuuarcy 65.398% \tValidation Acuuarcy 65.506%\n","Epoch: 398 \tTraining Loss: 0.01345391 \tValidation Loss 0.01376697 \tTraining Acuuarcy 65.317% \tValidation Acuuarcy 65.422%\n","Epoch: 399 \tTraining Loss: 0.01345875 \tValidation Loss 0.01393502 \tTraining Acuuarcy 65.370% \tValidation Acuuarcy 65.478%\n","Epoch: 400 \tTraining Loss: 0.01346735 \tValidation Loss 0.01376684 \tTraining Acuuarcy 65.237% \tValidation Acuuarcy 66.286%\n","Epoch: 401 \tTraining Loss: 0.01344889 \tValidation Loss 0.01385845 \tTraining Acuuarcy 65.324% \tValidation Acuuarcy 65.840%\n","Epoch: 402 \tTraining Loss: 0.01345665 \tValidation Loss 0.01385258 \tTraining Acuuarcy 65.290% \tValidation Acuuarcy 65.283%\n","Epoch: 403 \tTraining Loss: 0.01347035 \tValidation Loss 0.01386267 \tTraining Acuuarcy 65.070% \tValidation Acuuarcy 65.143%\n","Epoch: 404 \tTraining Loss: 0.01343905 \tValidation Loss 0.01395826 \tTraining Acuuarcy 65.551% \tValidation Acuuarcy 64.586%\n","Epoch: 405 \tTraining Loss: 0.01344328 \tValidation Loss 0.01390402 \tTraining Acuuarcy 65.509% \tValidation Acuuarcy 65.143%\n","Epoch: 406 \tTraining Loss: 0.01346985 \tValidation Loss 0.01382630 \tTraining Acuuarcy 65.154% \tValidation Acuuarcy 65.924%\n","Epoch: 407 \tTraining Loss: 0.01345714 \tValidation Loss 0.01375817 \tTraining Acuuarcy 65.262% \tValidation Acuuarcy 65.561%\n","Epoch: 408 \tTraining Loss: 0.01346211 \tValidation Loss 0.01385084 \tTraining Acuuarcy 65.286% \tValidation Acuuarcy 65.729%\n","Epoch: 409 \tTraining Loss: 0.01344477 \tValidation Loss 0.01376412 \tTraining Acuuarcy 65.488% \tValidation Acuuarcy 65.617%\n","Epoch: 410 \tTraining Loss: 0.01345483 \tValidation Loss 0.01389961 \tTraining Acuuarcy 65.384% \tValidation Acuuarcy 65.478%\n","Epoch: 411 \tTraining Loss: 0.01344213 \tValidation Loss 0.01391525 \tTraining Acuuarcy 65.450% \tValidation Acuuarcy 65.478%\n","Epoch: 412 \tTraining Loss: 0.01342777 \tValidation Loss 0.01398659 \tTraining Acuuarcy 65.621% \tValidation Acuuarcy 65.784%\n","Epoch: 413 \tTraining Loss: 0.01344651 \tValidation Loss 0.01392459 \tTraining Acuuarcy 65.533% \tValidation Acuuarcy 64.837%\n","Epoch: 414 \tTraining Loss: 0.01343336 \tValidation Loss 0.01381744 \tTraining Acuuarcy 65.673% \tValidation Acuuarcy 66.147%\n","Epoch: 415 \tTraining Loss: 0.01345954 \tValidation Loss 0.01391232 \tTraining Acuuarcy 65.248% \tValidation Acuuarcy 65.756%\n","Epoch: 416 \tTraining Loss: 0.01347402 \tValidation Loss 0.01382225 \tTraining Acuuarcy 65.143% \tValidation Acuuarcy 66.202%\n","Epoch: 417 \tTraining Loss: 0.01345683 \tValidation Loss 0.01387871 \tTraining Acuuarcy 65.297% \tValidation Acuuarcy 66.091%\n","Epoch: 418 \tTraining Loss: 0.01346233 \tValidation Loss 0.01394608 \tTraining Acuuarcy 65.210% \tValidation Acuuarcy 65.060%\n","Epoch: 419 \tTraining Loss: 0.01344966 \tValidation Loss 0.01387369 \tTraining Acuuarcy 65.554% \tValidation Acuuarcy 66.202%\n","Epoch: 420 \tTraining Loss: 0.01347100 \tValidation Loss 0.01389492 \tTraining Acuuarcy 65.112% \tValidation Acuuarcy 65.171%\n","Epoch: 421 \tTraining Loss: 0.01343619 \tValidation Loss 0.01375073 \tTraining Acuuarcy 65.610% \tValidation Acuuarcy 66.397%\n","Epoch: 422 \tTraining Loss: 0.01345149 \tValidation Loss 0.01387620 \tTraining Acuuarcy 65.311% \tValidation Acuuarcy 65.394%\n","Epoch: 423 \tTraining Loss: 0.01344991 \tValidation Loss 0.01382919 \tTraining Acuuarcy 65.380% \tValidation Acuuarcy 65.339%\n","Epoch: 424 \tTraining Loss: 0.01345294 \tValidation Loss 0.01392313 \tTraining Acuuarcy 65.394% \tValidation Acuuarcy 65.617%\n","Epoch: 425 \tTraining Loss: 0.01344487 \tValidation Loss 0.01383741 \tTraining Acuuarcy 65.523% \tValidation Acuuarcy 65.979%\n","Epoch: 426 \tTraining Loss: 0.01345669 \tValidation Loss 0.01381412 \tTraining Acuuarcy 65.345% \tValidation Acuuarcy 66.258%\n","Epoch: 427 \tTraining Loss: 0.01346213 \tValidation Loss 0.01376237 \tTraining Acuuarcy 65.234% \tValidation Acuuarcy 65.645%\n","Epoch: 428 \tTraining Loss: 0.01344913 \tValidation Loss 0.01380266 \tTraining Acuuarcy 65.502% \tValidation Acuuarcy 65.060%\n","Epoch: 429 \tTraining Loss: 0.01346406 \tValidation Loss 0.01379578 \tTraining Acuuarcy 65.293% \tValidation Acuuarcy 66.537%\n","Epoch: 430 \tTraining Loss: 0.01346429 \tValidation Loss 0.01384477 \tTraining Acuuarcy 65.258% \tValidation Acuuarcy 65.394%\n","Epoch: 431 \tTraining Loss: 0.01346032 \tValidation Loss 0.01392221 \tTraining Acuuarcy 65.307% \tValidation Acuuarcy 65.561%\n","Epoch: 432 \tTraining Loss: 0.01343971 \tValidation Loss 0.01380613 \tTraining Acuuarcy 65.610% \tValidation Acuuarcy 65.561%\n","Epoch: 433 \tTraining Loss: 0.01344535 \tValidation Loss 0.01389752 \tTraining Acuuarcy 65.464% \tValidation Acuuarcy 66.787%\n","Epoch: 434 \tTraining Loss: 0.01345988 \tValidation Loss 0.01387523 \tTraining Acuuarcy 65.227% \tValidation Acuuarcy 65.143%\n","Epoch: 435 \tTraining Loss: 0.01345296 \tValidation Loss 0.01390790 \tTraining Acuuarcy 65.317% \tValidation Acuuarcy 65.812%\n","Epoch: 436 \tTraining Loss: 0.01343830 \tValidation Loss 0.01387558 \tTraining Acuuarcy 65.582% \tValidation Acuuarcy 65.060%\n","Epoch: 437 \tTraining Loss: 0.01342627 \tValidation Loss 0.01382623 \tTraining Acuuarcy 65.781% \tValidation Acuuarcy 65.311%\n","Epoch: 438 \tTraining Loss: 0.01343749 \tValidation Loss 0.01382497 \tTraining Acuuarcy 65.582% \tValidation Acuuarcy 66.063%\n","Epoch: 439 \tTraining Loss: 0.01345092 \tValidation Loss 0.01376165 \tTraining Acuuarcy 65.460% \tValidation Acuuarcy 65.478%\n","Epoch: 440 \tTraining Loss: 0.01347479 \tValidation Loss 0.01382647 \tTraining Acuuarcy 65.056% \tValidation Acuuarcy 65.311%\n","Epoch: 441 \tTraining Loss: 0.01343980 \tValidation Loss 0.01394895 \tTraining Acuuarcy 65.589% \tValidation Acuuarcy 64.976%\n","Epoch: 442 \tTraining Loss: 0.01344960 \tValidation Loss 0.01389728 \tTraining Acuuarcy 65.485% \tValidation Acuuarcy 65.924%\n","Epoch: 443 \tTraining Loss: 0.01344392 \tValidation Loss 0.01378463 \tTraining Acuuarcy 65.394% \tValidation Acuuarcy 65.924%\n","Epoch: 444 \tTraining Loss: 0.01344578 \tValidation Loss 0.01383492 \tTraining Acuuarcy 65.485% \tValidation Acuuarcy 66.035%\n","Epoch: 445 \tTraining Loss: 0.01343512 \tValidation Loss 0.01395344 \tTraining Acuuarcy 65.565% \tValidation Acuuarcy 65.171%\n","Epoch: 446 \tTraining Loss: 0.01347234 \tValidation Loss 0.01392339 \tTraining Acuuarcy 65.168% \tValidation Acuuarcy 65.534%\n","Epoch: 447 \tTraining Loss: 0.01343269 \tValidation Loss 0.01389306 \tTraining Acuuarcy 65.732% \tValidation Acuuarcy 64.447%\n","Epoch: 448 \tTraining Loss: 0.01345451 \tValidation Loss 0.01382617 \tTraining Acuuarcy 65.314% \tValidation Acuuarcy 65.979%\n","Epoch: 449 \tTraining Loss: 0.01343416 \tValidation Loss 0.01381412 \tTraining Acuuarcy 65.680% \tValidation Acuuarcy 65.729%\n","Epoch: 450 \tTraining Loss: 0.01344103 \tValidation Loss 0.01401150 \tTraining Acuuarcy 65.523% \tValidation Acuuarcy 64.976%\n","Epoch: 451 \tTraining Loss: 0.01345959 \tValidation Loss 0.01385213 \tTraining Acuuarcy 65.286% \tValidation Acuuarcy 65.088%\n","Epoch: 452 \tTraining Loss: 0.01346257 \tValidation Loss 0.01388245 \tTraining Acuuarcy 65.283% \tValidation Acuuarcy 64.642%\n","Epoch: 453 \tTraining Loss: 0.01343130 \tValidation Loss 0.01379153 \tTraining Acuuarcy 65.694% \tValidation Acuuarcy 65.896%\n","Epoch: 454 \tTraining Loss: 0.01344235 \tValidation Loss 0.01383425 \tTraining Acuuarcy 65.530% \tValidation Acuuarcy 66.119%\n","Epoch: 455 \tTraining Loss: 0.01344744 \tValidation Loss 0.01382809 \tTraining Acuuarcy 65.436% \tValidation Acuuarcy 66.091%\n","Epoch: 456 \tTraining Loss: 0.01345479 \tValidation Loss 0.01392282 \tTraining Acuuarcy 65.307% \tValidation Acuuarcy 65.283%\n","Epoch: 457 \tTraining Loss: 0.01345609 \tValidation Loss 0.01390993 \tTraining Acuuarcy 65.342% \tValidation Acuuarcy 65.979%\n","Epoch: 458 \tTraining Loss: 0.01348453 \tValidation Loss 0.01398431 \tTraining Acuuarcy 64.900% \tValidation Acuuarcy 65.617%\n","Epoch: 459 \tTraining Loss: 0.01342810 \tValidation Loss 0.01381720 \tTraining Acuuarcy 65.704% \tValidation Acuuarcy 65.561%\n","Epoch: 460 \tTraining Loss: 0.01340977 \tValidation Loss 0.01390426 \tTraining Acuuarcy 65.944% \tValidation Acuuarcy 65.339%\n","Epoch: 461 \tTraining Loss: 0.01343802 \tValidation Loss 0.01389854 \tTraining Acuuarcy 65.526% \tValidation Acuuarcy 64.224%\n","Epoch: 462 \tTraining Loss: 0.01343240 \tValidation Loss 0.01381811 \tTraining Acuuarcy 65.572% \tValidation Acuuarcy 65.589%\n","Epoch: 463 \tTraining Loss: 0.01346179 \tValidation Loss 0.01386279 \tTraining Acuuarcy 65.230% \tValidation Acuuarcy 65.645%\n","Epoch: 464 \tTraining Loss: 0.01345052 \tValidation Loss 0.01397439 \tTraining Acuuarcy 65.338% \tValidation Acuuarcy 64.948%\n","Epoch: 465 \tTraining Loss: 0.01342644 \tValidation Loss 0.01397819 \tTraining Acuuarcy 65.666% \tValidation Acuuarcy 64.893%\n","Epoch: 466 \tTraining Loss: 0.01345019 \tValidation Loss 0.01386812 \tTraining Acuuarcy 65.419% \tValidation Acuuarcy 65.143%\n","Epoch: 467 \tTraining Loss: 0.01342063 \tValidation Loss 0.01388594 \tTraining Acuuarcy 65.826% \tValidation Acuuarcy 65.227%\n","Epoch: 468 \tTraining Loss: 0.01341979 \tValidation Loss 0.01394571 \tTraining Acuuarcy 65.774% \tValidation Acuuarcy 65.311%\n","Epoch: 469 \tTraining Loss: 0.01345100 \tValidation Loss 0.01392537 \tTraining Acuuarcy 65.467% \tValidation Acuuarcy 64.837%\n","Epoch: 470 \tTraining Loss: 0.01344736 \tValidation Loss 0.01374234 \tTraining Acuuarcy 65.422% \tValidation Acuuarcy 66.648%\n","Epoch: 471 \tTraining Loss: 0.01344708 \tValidation Loss 0.01391085 \tTraining Acuuarcy 65.457% \tValidation Acuuarcy 65.561%\n","Epoch: 472 \tTraining Loss: 0.01344776 \tValidation Loss 0.01382755 \tTraining Acuuarcy 65.425% \tValidation Acuuarcy 65.979%\n","Epoch: 473 \tTraining Loss: 0.01345545 \tValidation Loss 0.01381058 \tTraining Acuuarcy 65.293% \tValidation Acuuarcy 65.478%\n","Epoch: 474 \tTraining Loss: 0.01344091 \tValidation Loss 0.01384398 \tTraining Acuuarcy 65.561% \tValidation Acuuarcy 65.701%\n","Epoch: 475 \tTraining Loss: 0.01344133 \tValidation Loss 0.01386518 \tTraining Acuuarcy 65.568% \tValidation Acuuarcy 64.865%\n","Epoch: 476 \tTraining Loss: 0.01344146 \tValidation Loss 0.01377254 \tTraining Acuuarcy 65.488% \tValidation Acuuarcy 66.063%\n","Epoch: 477 \tTraining Loss: 0.01343553 \tValidation Loss 0.01378535 \tTraining Acuuarcy 65.645% \tValidation Acuuarcy 66.035%\n","Epoch: 478 \tTraining Loss: 0.01344840 \tValidation Loss 0.01383121 \tTraining Acuuarcy 65.419% \tValidation Acuuarcy 65.422%\n","Epoch: 479 \tTraining Loss: 0.01346215 \tValidation Loss 0.01393991 \tTraining Acuuarcy 65.213% \tValidation Acuuarcy 65.896%\n","Epoch: 480 \tTraining Loss: 0.01344747 \tValidation Loss 0.01378936 \tTraining Acuuarcy 65.415% \tValidation Acuuarcy 66.620%\n","Epoch: 481 \tTraining Loss: 0.01345398 \tValidation Loss 0.01387544 \tTraining Acuuarcy 65.321% \tValidation Acuuarcy 64.921%\n","Epoch: 482 \tTraining Loss: 0.01343721 \tValidation Loss 0.01382791 \tTraining Acuuarcy 65.509% \tValidation Acuuarcy 65.812%\n","Epoch: 483 \tTraining Loss: 0.01343050 \tValidation Loss 0.01378422 \tTraining Acuuarcy 65.614% \tValidation Acuuarcy 66.147%\n","Epoch: 484 \tTraining Loss: 0.01345372 \tValidation Loss 0.01377407 \tTraining Acuuarcy 65.352% \tValidation Acuuarcy 66.007%\n","Epoch: 485 \tTraining Loss: 0.01343764 \tValidation Loss 0.01393425 \tTraining Acuuarcy 65.530% \tValidation Acuuarcy 64.948%\n","Epoch: 486 \tTraining Loss: 0.01345757 \tValidation Loss 0.01370726 \tTraining Acuuarcy 65.338% \tValidation Acuuarcy 67.066%\n","Epoch: 487 \tTraining Loss: 0.01345053 \tValidation Loss 0.01385411 \tTraining Acuuarcy 65.443% \tValidation Acuuarcy 65.534%\n","Epoch: 488 \tTraining Loss: 0.01344707 \tValidation Loss 0.01384142 \tTraining Acuuarcy 65.478% \tValidation Acuuarcy 65.116%\n","Epoch: 489 \tTraining Loss: 0.01341699 \tValidation Loss 0.01381426 \tTraining Acuuarcy 65.836% \tValidation Acuuarcy 65.589%\n","Epoch: 490 \tTraining Loss: 0.01343985 \tValidation Loss 0.01390038 \tTraining Acuuarcy 65.628% \tValidation Acuuarcy 65.199%\n","Epoch: 491 \tTraining Loss: 0.01341672 \tValidation Loss 0.01393639 \tTraining Acuuarcy 65.861% \tValidation Acuuarcy 65.032%\n","Epoch: 492 \tTraining Loss: 0.01345787 \tValidation Loss 0.01388527 \tTraining Acuuarcy 65.317% \tValidation Acuuarcy 65.311%\n","Epoch: 493 \tTraining Loss: 0.01344068 \tValidation Loss 0.01379029 \tTraining Acuuarcy 65.495% \tValidation Acuuarcy 65.924%\n","Epoch: 494 \tTraining Loss: 0.01346059 \tValidation Loss 0.01387387 \tTraining Acuuarcy 65.279% \tValidation Acuuarcy 66.147%\n","Epoch: 495 \tTraining Loss: 0.01345817 \tValidation Loss 0.01379018 \tTraining Acuuarcy 65.363% \tValidation Acuuarcy 65.952%\n","Epoch: 496 \tTraining Loss: 0.01344006 \tValidation Loss 0.01390455 \tTraining Acuuarcy 65.540% \tValidation Acuuarcy 65.171%\n","Epoch: 497 \tTraining Loss: 0.01343602 \tValidation Loss 0.01388899 \tTraining Acuuarcy 65.575% \tValidation Acuuarcy 66.091%\n","Epoch: 498 \tTraining Loss: 0.01344932 \tValidation Loss 0.01388092 \tTraining Acuuarcy 65.474% \tValidation Acuuarcy 66.063%\n","Epoch: 499 \tTraining Loss: 0.01344739 \tValidation Loss 0.01378080 \tTraining Acuuarcy 65.356% \tValidation Acuuarcy 66.119%\n","Epoch: 500 \tTraining Loss: 0.01341715 \tValidation Loss 0.01371454 \tTraining Acuuarcy 65.850% \tValidation Acuuarcy 67.066%\n","Epoch: 501 \tTraining Loss: 0.01343161 \tValidation Loss 0.01380425 \tTraining Acuuarcy 65.582% \tValidation Acuuarcy 66.230%\n","Epoch: 502 \tTraining Loss: 0.01344102 \tValidation Loss 0.01378760 \tTraining Acuuarcy 65.495% \tValidation Acuuarcy 65.784%\n","Epoch: 503 \tTraining Loss: 0.01344736 \tValidation Loss 0.01386212 \tTraining Acuuarcy 65.439% \tValidation Acuuarcy 65.589%\n","Epoch: 504 \tTraining Loss: 0.01343063 \tValidation Loss 0.01384817 \tTraining Acuuarcy 65.676% \tValidation Acuuarcy 65.868%\n","Epoch: 505 \tTraining Loss: 0.01344206 \tValidation Loss 0.01395948 \tTraining Acuuarcy 65.506% \tValidation Acuuarcy 65.896%\n","Epoch: 506 \tTraining Loss: 0.01344218 \tValidation Loss 0.01391481 \tTraining Acuuarcy 65.561% \tValidation Acuuarcy 66.397%\n","Epoch: 507 \tTraining Loss: 0.01343450 \tValidation Loss 0.01375143 \tTraining Acuuarcy 65.586% \tValidation Acuuarcy 65.506%\n","Epoch: 508 \tTraining Loss: 0.01345610 \tValidation Loss 0.01373642 \tTraining Acuuarcy 65.415% \tValidation Acuuarcy 66.787%\n","Epoch: 509 \tTraining Loss: 0.01343657 \tValidation Loss 0.01390355 \tTraining Acuuarcy 65.558% \tValidation Acuuarcy 65.171%\n","Epoch: 510 \tTraining Loss: 0.01343018 \tValidation Loss 0.01386760 \tTraining Acuuarcy 65.648% \tValidation Acuuarcy 66.258%\n","Epoch: 511 \tTraining Loss: 0.01344947 \tValidation Loss 0.01377428 \tTraining Acuuarcy 65.342% \tValidation Acuuarcy 65.952%\n","Epoch: 512 \tTraining Loss: 0.01343994 \tValidation Loss 0.01380684 \tTraining Acuuarcy 65.554% \tValidation Acuuarcy 66.453%\n","Epoch: 513 \tTraining Loss: 0.01345155 \tValidation Loss 0.01391419 \tTraining Acuuarcy 65.384% \tValidation Acuuarcy 65.840%\n","Epoch: 514 \tTraining Loss: 0.01343656 \tValidation Loss 0.01370871 \tTraining Acuuarcy 65.502% \tValidation Acuuarcy 66.982%\n","Epoch: 515 \tTraining Loss: 0.01345289 \tValidation Loss 0.01377515 \tTraining Acuuarcy 65.398% \tValidation Acuuarcy 65.283%\n","Epoch: 516 \tTraining Loss: 0.01343300 \tValidation Loss 0.01385199 \tTraining Acuuarcy 65.638% \tValidation Acuuarcy 66.537%\n","Epoch: 517 \tTraining Loss: 0.01342912 \tValidation Loss 0.01387817 \tTraining Acuuarcy 65.676% \tValidation Acuuarcy 65.478%\n","Epoch: 518 \tTraining Loss: 0.01344189 \tValidation Loss 0.01385498 \tTraining Acuuarcy 65.520% \tValidation Acuuarcy 65.060%\n","Epoch: 519 \tTraining Loss: 0.01346661 \tValidation Loss 0.01386671 \tTraining Acuuarcy 65.199% \tValidation Acuuarcy 65.701%\n","Epoch: 520 \tTraining Loss: 0.01343901 \tValidation Loss 0.01388438 \tTraining Acuuarcy 65.551% \tValidation Acuuarcy 65.561%\n","Epoch: 521 \tTraining Loss: 0.01343631 \tValidation Loss 0.01396030 \tTraining Acuuarcy 65.655% \tValidation Acuuarcy 65.032%\n","Epoch: 522 \tTraining Loss: 0.01343700 \tValidation Loss 0.01381768 \tTraining Acuuarcy 65.502% \tValidation Acuuarcy 66.035%\n","Epoch: 523 \tTraining Loss: 0.01344633 \tValidation Loss 0.01381499 \tTraining Acuuarcy 65.485% \tValidation Acuuarcy 65.617%\n","Epoch: 524 \tTraining Loss: 0.01344487 \tValidation Loss 0.01393539 \tTraining Acuuarcy 65.457% \tValidation Acuuarcy 64.586%\n","Epoch: 525 \tTraining Loss: 0.01342904 \tValidation Loss 0.01386861 \tTraining Acuuarcy 65.697% \tValidation Acuuarcy 65.645%\n","Epoch: 526 \tTraining Loss: 0.01343693 \tValidation Loss 0.01377359 \tTraining Acuuarcy 65.610% \tValidation Acuuarcy 66.007%\n","Epoch: 527 \tTraining Loss: 0.01345115 \tValidation Loss 0.01370212 \tTraining Acuuarcy 65.415% \tValidation Acuuarcy 66.369%\n","Epoch: 528 \tTraining Loss: 0.01340875 \tValidation Loss 0.01393903 \tTraining Acuuarcy 65.913% \tValidation Acuuarcy 65.311%\n","Epoch: 529 \tTraining Loss: 0.01341808 \tValidation Loss 0.01370864 \tTraining Acuuarcy 65.882% \tValidation Acuuarcy 66.481%\n","Epoch: 530 \tTraining Loss: 0.01346039 \tValidation Loss 0.01370866 \tTraining Acuuarcy 65.352% \tValidation Acuuarcy 66.174%\n","Epoch: 531 \tTraining Loss: 0.01343191 \tValidation Loss 0.01379828 \tTraining Acuuarcy 65.614% \tValidation Acuuarcy 65.952%\n","Epoch: 532 \tTraining Loss: 0.01342402 \tValidation Loss 0.01381538 \tTraining Acuuarcy 65.725% \tValidation Acuuarcy 65.812%\n","Epoch: 533 \tTraining Loss: 0.01342130 \tValidation Loss 0.01390747 \tTraining Acuuarcy 65.774% \tValidation Acuuarcy 65.088%\n","Epoch: 534 \tTraining Loss: 0.01342840 \tValidation Loss 0.01383779 \tTraining Acuuarcy 65.662% \tValidation Acuuarcy 65.645%\n","Epoch: 535 \tTraining Loss: 0.01342689 \tValidation Loss 0.01381181 \tTraining Acuuarcy 65.662% \tValidation Acuuarcy 64.809%\n","Epoch: 536 \tTraining Loss: 0.01344770 \tValidation Loss 0.01386040 \tTraining Acuuarcy 65.457% \tValidation Acuuarcy 65.617%\n","Epoch: 537 \tTraining Loss: 0.01343358 \tValidation Loss 0.01375342 \tTraining Acuuarcy 65.652% \tValidation Acuuarcy 66.453%\n","Epoch: 538 \tTraining Loss: 0.01343386 \tValidation Loss 0.01378593 \tTraining Acuuarcy 65.673% \tValidation Acuuarcy 66.035%\n","Epoch: 539 \tTraining Loss: 0.01344762 \tValidation Loss 0.01371852 \tTraining Acuuarcy 65.478% \tValidation Acuuarcy 67.010%\n","Epoch: 540 \tTraining Loss: 0.01343514 \tValidation Loss 0.01385848 \tTraining Acuuarcy 65.544% \tValidation Acuuarcy 66.397%\n","Epoch: 541 \tTraining Loss: 0.01344291 \tValidation Loss 0.01380490 \tTraining Acuuarcy 65.520% \tValidation Acuuarcy 64.976%\n","Epoch: 542 \tTraining Loss: 0.01343481 \tValidation Loss 0.01385438 \tTraining Acuuarcy 65.621% \tValidation Acuuarcy 66.592%\n","Epoch: 543 \tTraining Loss: 0.01342354 \tValidation Loss 0.01386469 \tTraining Acuuarcy 65.666% \tValidation Acuuarcy 65.561%\n","Epoch: 544 \tTraining Loss: 0.01343083 \tValidation Loss 0.01383230 \tTraining Acuuarcy 65.683% \tValidation Acuuarcy 65.868%\n","Epoch: 545 \tTraining Loss: 0.01343919 \tValidation Loss 0.01391009 \tTraining Acuuarcy 65.586% \tValidation Acuuarcy 64.837%\n","Epoch: 546 \tTraining Loss: 0.01344328 \tValidation Loss 0.01388808 \tTraining Acuuarcy 65.523% \tValidation Acuuarcy 65.227%\n","Epoch: 547 \tTraining Loss: 0.01345159 \tValidation Loss 0.01386330 \tTraining Acuuarcy 65.363% \tValidation Acuuarcy 64.809%\n","Epoch: 548 \tTraining Loss: 0.01344500 \tValidation Loss 0.01373974 \tTraining Acuuarcy 65.460% \tValidation Acuuarcy 66.509%\n","Epoch: 549 \tTraining Loss: 0.01343988 \tValidation Loss 0.01379925 \tTraining Acuuarcy 65.540% \tValidation Acuuarcy 66.955%\n","Epoch: 550 \tTraining Loss: 0.01343348 \tValidation Loss 0.01378805 \tTraining Acuuarcy 65.579% \tValidation Acuuarcy 65.784%\n","Epoch: 551 \tTraining Loss: 0.01345160 \tValidation Loss 0.01394541 \tTraining Acuuarcy 65.359% \tValidation Acuuarcy 65.534%\n","Epoch: 552 \tTraining Loss: 0.01343771 \tValidation Loss 0.01380153 \tTraining Acuuarcy 65.634% \tValidation Acuuarcy 66.453%\n","Epoch: 553 \tTraining Loss: 0.01343575 \tValidation Loss 0.01383996 \tTraining Acuuarcy 65.579% \tValidation Acuuarcy 65.171%\n","Epoch: 554 \tTraining Loss: 0.01342680 \tValidation Loss 0.01372096 \tTraining Acuuarcy 65.673% \tValidation Acuuarcy 66.063%\n","Epoch: 555 \tTraining Loss: 0.01342873 \tValidation Loss 0.01384894 \tTraining Acuuarcy 65.725% \tValidation Acuuarcy 65.868%\n","Epoch: 556 \tTraining Loss: 0.01342482 \tValidation Loss 0.01387212 \tTraining Acuuarcy 65.711% \tValidation Acuuarcy 65.589%\n","Epoch: 557 \tTraining Loss: 0.01344023 \tValidation Loss 0.01385984 \tTraining Acuuarcy 65.499% \tValidation Acuuarcy 65.673%\n","Epoch: 558 \tTraining Loss: 0.01343133 \tValidation Loss 0.01392013 \tTraining Acuuarcy 65.690% \tValidation Acuuarcy 65.506%\n","Epoch: 559 \tTraining Loss: 0.01342506 \tValidation Loss 0.01392142 \tTraining Acuuarcy 65.711% \tValidation Acuuarcy 65.311%\n","Epoch: 560 \tTraining Loss: 0.01344604 \tValidation Loss 0.01390234 \tTraining Acuuarcy 65.467% \tValidation Acuuarcy 65.255%\n","Epoch: 561 \tTraining Loss: 0.01344939 \tValidation Loss 0.01380928 \tTraining Acuuarcy 65.398% \tValidation Acuuarcy 65.812%\n","Epoch: 562 \tTraining Loss: 0.01342585 \tValidation Loss 0.01387320 \tTraining Acuuarcy 65.816% \tValidation Acuuarcy 65.561%\n","Epoch: 563 \tTraining Loss: 0.01344096 \tValidation Loss 0.01390642 \tTraining Acuuarcy 65.419% \tValidation Acuuarcy 65.812%\n","Epoch: 564 \tTraining Loss: 0.01343731 \tValidation Loss 0.01374497 \tTraining Acuuarcy 65.617% \tValidation Acuuarcy 66.397%\n","Epoch: 565 \tTraining Loss: 0.01342257 \tValidation Loss 0.01383671 \tTraining Acuuarcy 65.749% \tValidation Acuuarcy 65.868%\n","Epoch: 566 \tTraining Loss: 0.01340428 \tValidation Loss 0.01382560 \tTraining Acuuarcy 66.056% \tValidation Acuuarcy 66.286%\n","Epoch: 567 \tTraining Loss: 0.01343103 \tValidation Loss 0.01389737 \tTraining Acuuarcy 65.732% \tValidation Acuuarcy 65.868%\n","Epoch: 568 \tTraining Loss: 0.01343589 \tValidation Loss 0.01375778 \tTraining Acuuarcy 65.589% \tValidation Acuuarcy 66.258%\n","Epoch: 569 \tTraining Loss: 0.01344516 \tValidation Loss 0.01374642 \tTraining Acuuarcy 65.492% \tValidation Acuuarcy 65.534%\n","Epoch: 570 \tTraining Loss: 0.01343455 \tValidation Loss 0.01388955 \tTraining Acuuarcy 65.610% \tValidation Acuuarcy 65.756%\n","Epoch: 571 \tTraining Loss: 0.01341900 \tValidation Loss 0.01391111 \tTraining Acuuarcy 65.843% \tValidation Acuuarcy 66.063%\n","Epoch: 572 \tTraining Loss: 0.01344762 \tValidation Loss 0.01381018 \tTraining Acuuarcy 65.425% \tValidation Acuuarcy 64.809%\n","Epoch: 573 \tTraining Loss: 0.01343958 \tValidation Loss 0.01381717 \tTraining Acuuarcy 65.523% \tValidation Acuuarcy 65.534%\n","Epoch: 574 \tTraining Loss: 0.01344801 \tValidation Loss 0.01392543 \tTraining Acuuarcy 65.457% \tValidation Acuuarcy 64.753%\n","Epoch: 575 \tTraining Loss: 0.01341808 \tValidation Loss 0.01386918 \tTraining Acuuarcy 65.777% \tValidation Acuuarcy 66.258%\n","Epoch: 576 \tTraining Loss: 0.01342757 \tValidation Loss 0.01385492 \tTraining Acuuarcy 65.711% \tValidation Acuuarcy 65.617%\n","Epoch: 577 \tTraining Loss: 0.01345130 \tValidation Loss 0.01375973 \tTraining Acuuarcy 65.422% \tValidation Acuuarcy 66.369%\n","Epoch: 578 \tTraining Loss: 0.01343050 \tValidation Loss 0.01391220 \tTraining Acuuarcy 65.652% \tValidation Acuuarcy 65.617%\n","Epoch: 579 \tTraining Loss: 0.01344593 \tValidation Loss 0.01383908 \tTraining Acuuarcy 65.481% \tValidation Acuuarcy 65.673%\n","Epoch: 580 \tTraining Loss: 0.01342673 \tValidation Loss 0.01389389 \tTraining Acuuarcy 65.652% \tValidation Acuuarcy 65.143%\n","Epoch: 581 \tTraining Loss: 0.01343738 \tValidation Loss 0.01387730 \tTraining Acuuarcy 65.579% \tValidation Acuuarcy 65.339%\n","Epoch: 582 \tTraining Loss: 0.01345941 \tValidation Loss 0.01376353 \tTraining Acuuarcy 65.297% \tValidation Acuuarcy 66.258%\n","Epoch: 583 \tTraining Loss: 0.01344053 \tValidation Loss 0.01370699 \tTraining Acuuarcy 65.481% \tValidation Acuuarcy 66.286%\n","Epoch: 584 \tTraining Loss: 0.01341119 \tValidation Loss 0.01384151 \tTraining Acuuarcy 65.882% \tValidation Acuuarcy 65.227%\n","Epoch: 585 \tTraining Loss: 0.01344646 \tValidation Loss 0.01379475 \tTraining Acuuarcy 65.419% \tValidation Acuuarcy 66.035%\n","Epoch: 586 \tTraining Loss: 0.01341902 \tValidation Loss 0.01390154 \tTraining Acuuarcy 65.732% \tValidation Acuuarcy 65.840%\n","Epoch: 587 \tTraining Loss: 0.01342786 \tValidation Loss 0.01372885 \tTraining Acuuarcy 65.711% \tValidation Acuuarcy 66.592%\n","Epoch: 588 \tTraining Loss: 0.01345261 \tValidation Loss 0.01385872 \tTraining Acuuarcy 65.415% \tValidation Acuuarcy 66.369%\n","Epoch: 589 \tTraining Loss: 0.01344890 \tValidation Loss 0.01386347 \tTraining Acuuarcy 65.380% \tValidation Acuuarcy 65.924%\n","Epoch: 590 \tTraining Loss: 0.01342981 \tValidation Loss 0.01381920 \tTraining Acuuarcy 65.648% \tValidation Acuuarcy 65.478%\n","Epoch: 591 \tTraining Loss: 0.01342361 \tValidation Loss 0.01382083 \tTraining Acuuarcy 65.756% \tValidation Acuuarcy 65.589%\n","Epoch: 592 \tTraining Loss: 0.01342508 \tValidation Loss 0.01379071 \tTraining Acuuarcy 65.694% \tValidation Acuuarcy 66.648%\n","Epoch: 593 \tTraining Loss: 0.01343705 \tValidation Loss 0.01384860 \tTraining Acuuarcy 65.652% \tValidation Acuuarcy 65.868%\n","Epoch: 594 \tTraining Loss: 0.01344340 \tValidation Loss 0.01394541 \tTraining Acuuarcy 65.425% \tValidation Acuuarcy 65.366%\n","Epoch: 595 \tTraining Loss: 0.01344550 \tValidation Loss 0.01391423 \tTraining Acuuarcy 65.439% \tValidation Acuuarcy 64.948%\n","Epoch: 596 \tTraining Loss: 0.01344910 \tValidation Loss 0.01376309 \tTraining Acuuarcy 65.422% \tValidation Acuuarcy 65.450%\n","Epoch: 597 \tTraining Loss: 0.01341972 \tValidation Loss 0.01375958 \tTraining Acuuarcy 65.791% \tValidation Acuuarcy 66.202%\n","Epoch: 598 \tTraining Loss: 0.01342205 \tValidation Loss 0.01379991 \tTraining Acuuarcy 65.798% \tValidation Acuuarcy 66.369%\n","Epoch: 599 \tTraining Loss: 0.01346896 \tValidation Loss 0.01381537 \tTraining Acuuarcy 65.133% \tValidation Acuuarcy 66.397%\n","Epoch: 600 \tTraining Loss: 0.01342955 \tValidation Loss 0.01375895 \tTraining Acuuarcy 65.680% \tValidation Acuuarcy 66.202%\n","Epoch: 601 \tTraining Loss: 0.01344960 \tValidation Loss 0.01381010 \tTraining Acuuarcy 65.443% \tValidation Acuuarcy 65.729%\n","Epoch: 602 \tTraining Loss: 0.01343819 \tValidation Loss 0.01373111 \tTraining Acuuarcy 65.607% \tValidation Acuuarcy 66.174%\n","Epoch: 603 \tTraining Loss: 0.01343000 \tValidation Loss 0.01385403 \tTraining Acuuarcy 65.669% \tValidation Acuuarcy 65.812%\n","Epoch: 604 \tTraining Loss: 0.01345994 \tValidation Loss 0.01386223 \tTraining Acuuarcy 65.230% \tValidation Acuuarcy 65.617%\n","Epoch: 605 \tTraining Loss: 0.01340470 \tValidation Loss 0.01370058 \tTraining Acuuarcy 65.906% \tValidation Acuuarcy 66.314%\n","Epoch: 606 \tTraining Loss: 0.01345842 \tValidation Loss 0.01371788 \tTraining Acuuarcy 65.258% \tValidation Acuuarcy 66.258%\n","Epoch: 607 \tTraining Loss: 0.01343782 \tValidation Loss 0.01382144 \tTraining Acuuarcy 65.607% \tValidation Acuuarcy 65.478%\n","Epoch: 608 \tTraining Loss: 0.01340060 \tValidation Loss 0.01383495 \tTraining Acuuarcy 66.129% \tValidation Acuuarcy 65.924%\n","Epoch: 609 \tTraining Loss: 0.01343015 \tValidation Loss 0.01383143 \tTraining Acuuarcy 65.722% \tValidation Acuuarcy 65.645%\n","Epoch: 610 \tTraining Loss: 0.01342392 \tValidation Loss 0.01378779 \tTraining Acuuarcy 65.742% \tValidation Acuuarcy 66.760%\n","Epoch: 611 \tTraining Loss: 0.01343450 \tValidation Loss 0.01389650 \tTraining Acuuarcy 65.614% \tValidation Acuuarcy 66.119%\n","Epoch: 612 \tTraining Loss: 0.01343624 \tValidation Loss 0.01393504 \tTraining Acuuarcy 65.558% \tValidation Acuuarcy 65.506%\n","Epoch: 613 \tTraining Loss: 0.01342052 \tValidation Loss 0.01391223 \tTraining Acuuarcy 65.826% \tValidation Acuuarcy 65.840%\n","Epoch: 614 \tTraining Loss: 0.01341767 \tValidation Loss 0.01381070 \tTraining Acuuarcy 65.725% \tValidation Acuuarcy 65.868%\n","Epoch: 615 \tTraining Loss: 0.01342468 \tValidation Loss 0.01380414 \tTraining Acuuarcy 65.781% \tValidation Acuuarcy 66.537%\n","Epoch: 616 \tTraining Loss: 0.01341568 \tValidation Loss 0.01380688 \tTraining Acuuarcy 65.826% \tValidation Acuuarcy 65.060%\n","Epoch: 617 \tTraining Loss: 0.01342854 \tValidation Loss 0.01391274 \tTraining Acuuarcy 65.673% \tValidation Acuuarcy 65.171%\n","Epoch: 618 \tTraining Loss: 0.01341336 \tValidation Loss 0.01387229 \tTraining Acuuarcy 65.948% \tValidation Acuuarcy 66.091%\n","Epoch: 619 \tTraining Loss: 0.01344156 \tValidation Loss 0.01394972 \tTraining Acuuarcy 65.526% \tValidation Acuuarcy 65.979%\n","Epoch: 620 \tTraining Loss: 0.01340642 \tValidation Loss 0.01384534 \tTraining Acuuarcy 65.990% \tValidation Acuuarcy 66.035%\n","Epoch: 621 \tTraining Loss: 0.01343477 \tValidation Loss 0.01390079 \tTraining Acuuarcy 65.572% \tValidation Acuuarcy 65.199%\n","Epoch: 622 \tTraining Loss: 0.01344401 \tValidation Loss 0.01376620 \tTraining Acuuarcy 65.488% \tValidation Acuuarcy 66.481%\n","Epoch: 623 \tTraining Loss: 0.01342393 \tValidation Loss 0.01373258 \tTraining Acuuarcy 65.756% \tValidation Acuuarcy 65.924%\n","Epoch: 624 \tTraining Loss: 0.01340474 \tValidation Loss 0.01384981 \tTraining Acuuarcy 65.934% \tValidation Acuuarcy 65.784%\n","Epoch: 625 \tTraining Loss: 0.01342634 \tValidation Loss 0.01385211 \tTraining Acuuarcy 65.617% \tValidation Acuuarcy 65.729%\n","Epoch: 626 \tTraining Loss: 0.01342784 \tValidation Loss 0.01383070 \tTraining Acuuarcy 65.795% \tValidation Acuuarcy 65.394%\n","Epoch: 627 \tTraining Loss: 0.01343421 \tValidation Loss 0.01383048 \tTraining Acuuarcy 65.648% \tValidation Acuuarcy 65.506%\n","Epoch: 628 \tTraining Loss: 0.01344260 \tValidation Loss 0.01378449 \tTraining Acuuarcy 65.516% \tValidation Acuuarcy 66.091%\n","Epoch: 629 \tTraining Loss: 0.01345214 \tValidation Loss 0.01393228 \tTraining Acuuarcy 65.398% \tValidation Acuuarcy 65.450%\n","Epoch: 630 \tTraining Loss: 0.01342894 \tValidation Loss 0.01398507 \tTraining Acuuarcy 65.742% \tValidation Acuuarcy 65.896%\n","Epoch: 631 \tTraining Loss: 0.01341126 \tValidation Loss 0.01383333 \tTraining Acuuarcy 65.962% \tValidation Acuuarcy 65.534%\n","Epoch: 632 \tTraining Loss: 0.01340981 \tValidation Loss 0.01379379 \tTraining Acuuarcy 65.857% \tValidation Acuuarcy 65.088%\n","Epoch: 633 \tTraining Loss: 0.01340348 \tValidation Loss 0.01379104 \tTraining Acuuarcy 65.990% \tValidation Acuuarcy 65.979%\n","Epoch: 634 \tTraining Loss: 0.01344528 \tValidation Loss 0.01379514 \tTraining Acuuarcy 65.495% \tValidation Acuuarcy 65.868%\n","Epoch: 635 \tTraining Loss: 0.01345001 \tValidation Loss 0.01375851 \tTraining Acuuarcy 65.474% \tValidation Acuuarcy 65.701%\n","Epoch: 636 \tTraining Loss: 0.01342356 \tValidation Loss 0.01390238 \tTraining Acuuarcy 65.742% \tValidation Acuuarcy 65.645%\n","Epoch: 637 \tTraining Loss: 0.01342117 \tValidation Loss 0.01389064 \tTraining Acuuarcy 65.812% \tValidation Acuuarcy 65.784%\n","Epoch: 638 \tTraining Loss: 0.01343748 \tValidation Loss 0.01380568 \tTraining Acuuarcy 65.547% \tValidation Acuuarcy 66.537%\n","Epoch: 639 \tTraining Loss: 0.01342246 \tValidation Loss 0.01372832 \tTraining Acuuarcy 65.830% \tValidation Acuuarcy 66.732%\n","Epoch: 640 \tTraining Loss: 0.01341792 \tValidation Loss 0.01383955 \tTraining Acuuarcy 65.830% \tValidation Acuuarcy 66.063%\n","Epoch: 641 \tTraining Loss: 0.01343329 \tValidation Loss 0.01396067 \tTraining Acuuarcy 65.673% \tValidation Acuuarcy 65.729%\n","Epoch: 642 \tTraining Loss: 0.01341376 \tValidation Loss 0.01375852 \tTraining Acuuarcy 65.882% \tValidation Acuuarcy 65.673%\n","Epoch: 643 \tTraining Loss: 0.01342662 \tValidation Loss 0.01369650 \tTraining Acuuarcy 65.749% \tValidation Acuuarcy 67.066%\n","Epoch: 644 \tTraining Loss: 0.01342312 \tValidation Loss 0.01387880 \tTraining Acuuarcy 65.784% \tValidation Acuuarcy 65.422%\n","Epoch: 645 \tTraining Loss: 0.01342045 \tValidation Loss 0.01391093 \tTraining Acuuarcy 65.770% \tValidation Acuuarcy 65.979%\n","Epoch: 646 \tTraining Loss: 0.01342628 \tValidation Loss 0.01390813 \tTraining Acuuarcy 65.711% \tValidation Acuuarcy 64.893%\n","Epoch: 647 \tTraining Loss: 0.01343029 \tValidation Loss 0.01378593 \tTraining Acuuarcy 65.655% \tValidation Acuuarcy 66.007%\n","Epoch: 648 \tTraining Loss: 0.01342476 \tValidation Loss 0.01390261 \tTraining Acuuarcy 65.753% \tValidation Acuuarcy 65.701%\n","Epoch: 649 \tTraining Loss: 0.01341096 \tValidation Loss 0.01387379 \tTraining Acuuarcy 65.857% \tValidation Acuuarcy 65.589%\n","Epoch: 650 \tTraining Loss: 0.01341589 \tValidation Loss 0.01375778 \tTraining Acuuarcy 65.927% \tValidation Acuuarcy 66.397%\n","Epoch: 651 \tTraining Loss: 0.01342539 \tValidation Loss 0.01389807 \tTraining Acuuarcy 65.732% \tValidation Acuuarcy 65.116%\n","Epoch: 652 \tTraining Loss: 0.01343520 \tValidation Loss 0.01374079 \tTraining Acuuarcy 65.589% \tValidation Acuuarcy 66.565%\n","Epoch: 653 \tTraining Loss: 0.01340696 \tValidation Loss 0.01384339 \tTraining Acuuarcy 66.007% \tValidation Acuuarcy 65.116%\n","Epoch: 654 \tTraining Loss: 0.01344204 \tValidation Loss 0.01383416 \tTraining Acuuarcy 65.481% \tValidation Acuuarcy 65.952%\n","Epoch: 655 \tTraining Loss: 0.01343088 \tValidation Loss 0.01376878 \tTraining Acuuarcy 65.624% \tValidation Acuuarcy 66.286%\n","Epoch: 656 \tTraining Loss: 0.01342652 \tValidation Loss 0.01370471 \tTraining Acuuarcy 65.836% \tValidation Acuuarcy 67.066%\n","Epoch: 657 \tTraining Loss: 0.01340514 \tValidation Loss 0.01377224 \tTraining Acuuarcy 66.059% \tValidation Acuuarcy 65.979%\n","Epoch: 658 \tTraining Loss: 0.01341016 \tValidation Loss 0.01392733 \tTraining Acuuarcy 65.931% \tValidation Acuuarcy 65.534%\n","Epoch: 659 \tTraining Loss: 0.01344221 \tValidation Loss 0.01386963 \tTraining Acuuarcy 65.467% \tValidation Acuuarcy 65.561%\n","Epoch: 660 \tTraining Loss: 0.01343596 \tValidation Loss 0.01389277 \tTraining Acuuarcy 65.582% \tValidation Acuuarcy 65.645%\n","Epoch: 661 \tTraining Loss: 0.01343132 \tValidation Loss 0.01386070 \tTraining Acuuarcy 65.708% \tValidation Acuuarcy 65.729%\n","Epoch: 662 \tTraining Loss: 0.01341396 \tValidation Loss 0.01378157 \tTraining Acuuarcy 65.892% \tValidation Acuuarcy 65.868%\n","Epoch: 663 \tTraining Loss: 0.01343366 \tValidation Loss 0.01384718 \tTraining Acuuarcy 65.683% \tValidation Acuuarcy 65.729%\n","Epoch: 664 \tTraining Loss: 0.01344022 \tValidation Loss 0.01379771 \tTraining Acuuarcy 65.516% \tValidation Acuuarcy 66.007%\n","Epoch: 665 \tTraining Loss: 0.01344565 \tValidation Loss 0.01380415 \tTraining Acuuarcy 65.478% \tValidation Acuuarcy 66.481%\n","Epoch: 666 \tTraining Loss: 0.01341181 \tValidation Loss 0.01385052 \tTraining Acuuarcy 65.910% \tValidation Acuuarcy 65.840%\n","Epoch: 667 \tTraining Loss: 0.01340845 \tValidation Loss 0.01386517 \tTraining Acuuarcy 65.885% \tValidation Acuuarcy 66.369%\n","Epoch: 668 \tTraining Loss: 0.01341603 \tValidation Loss 0.01371696 \tTraining Acuuarcy 65.871% \tValidation Acuuarcy 66.119%\n","Epoch: 669 \tTraining Loss: 0.01345058 \tValidation Loss 0.01377381 \tTraining Acuuarcy 65.429% \tValidation Acuuarcy 66.425%\n","Epoch: 670 \tTraining Loss: 0.01342091 \tValidation Loss 0.01383329 \tTraining Acuuarcy 65.847% \tValidation Acuuarcy 66.230%\n","Epoch: 671 \tTraining Loss: 0.01340129 \tValidation Loss 0.01387118 \tTraining Acuuarcy 66.014% \tValidation Acuuarcy 64.893%\n","Epoch: 672 \tTraining Loss: 0.01340073 \tValidation Loss 0.01375794 \tTraining Acuuarcy 66.021% \tValidation Acuuarcy 65.561%\n","Epoch: 673 \tTraining Loss: 0.01341304 \tValidation Loss 0.01378048 \tTraining Acuuarcy 65.843% \tValidation Acuuarcy 65.255%\n","Epoch: 674 \tTraining Loss: 0.01343601 \tValidation Loss 0.01369129 \tTraining Acuuarcy 65.572% \tValidation Acuuarcy 66.425%\n","Epoch: 675 \tTraining Loss: 0.01341824 \tValidation Loss 0.01384910 \tTraining Acuuarcy 65.857% \tValidation Acuuarcy 65.840%\n","Epoch: 676 \tTraining Loss: 0.01343239 \tValidation Loss 0.01383010 \tTraining Acuuarcy 65.722% \tValidation Acuuarcy 65.422%\n","Epoch: 677 \tTraining Loss: 0.01344694 \tValidation Loss 0.01373839 \tTraining Acuuarcy 65.425% \tValidation Acuuarcy 65.924%\n","Epoch: 678 \tTraining Loss: 0.01342103 \tValidation Loss 0.01375901 \tTraining Acuuarcy 65.767% \tValidation Acuuarcy 66.230%\n","Epoch: 679 \tTraining Loss: 0.01342208 \tValidation Loss 0.01380894 \tTraining Acuuarcy 65.732% \tValidation Acuuarcy 66.453%\n","Epoch: 680 \tTraining Loss: 0.01344297 \tValidation Loss 0.01384157 \tTraining Acuuarcy 65.600% \tValidation Acuuarcy 65.896%\n","Epoch: 681 \tTraining Loss: 0.01342901 \tValidation Loss 0.01374975 \tTraining Acuuarcy 65.666% \tValidation Acuuarcy 65.756%\n","Epoch: 682 \tTraining Loss: 0.01343754 \tValidation Loss 0.01375522 \tTraining Acuuarcy 65.638% \tValidation Acuuarcy 66.342%\n","Epoch: 683 \tTraining Loss: 0.01342295 \tValidation Loss 0.01374954 \tTraining Acuuarcy 65.788% \tValidation Acuuarcy 65.812%\n","Epoch: 684 \tTraining Loss: 0.01342556 \tValidation Loss 0.01382366 \tTraining Acuuarcy 65.669% \tValidation Acuuarcy 65.422%\n","Epoch: 685 \tTraining Loss: 0.01341729 \tValidation Loss 0.01375064 \tTraining Acuuarcy 65.899% \tValidation Acuuarcy 66.342%\n","Epoch: 686 \tTraining Loss: 0.01343448 \tValidation Loss 0.01377148 \tTraining Acuuarcy 65.614% \tValidation Acuuarcy 66.063%\n","Epoch: 687 \tTraining Loss: 0.01343088 \tValidation Loss 0.01383078 \tTraining Acuuarcy 65.596% \tValidation Acuuarcy 65.979%\n","Epoch: 688 \tTraining Loss: 0.01341605 \tValidation Loss 0.01381871 \tTraining Acuuarcy 65.850% \tValidation Acuuarcy 65.450%\n","Epoch: 689 \tTraining Loss: 0.01340690 \tValidation Loss 0.01369514 \tTraining Acuuarcy 65.976% \tValidation Acuuarcy 66.369%\n","Epoch: 690 \tTraining Loss: 0.01343298 \tValidation Loss 0.01379083 \tTraining Acuuarcy 65.600% \tValidation Acuuarcy 66.676%\n","Epoch: 691 \tTraining Loss: 0.01340892 \tValidation Loss 0.01388110 \tTraining Acuuarcy 65.955% \tValidation Acuuarcy 65.339%\n","Epoch: 692 \tTraining Loss: 0.01342352 \tValidation Loss 0.01387815 \tTraining Acuuarcy 65.795% \tValidation Acuuarcy 66.815%\n","Epoch: 693 \tTraining Loss: 0.01343964 \tValidation Loss 0.01386498 \tTraining Acuuarcy 65.506% \tValidation Acuuarcy 65.589%\n","Epoch: 694 \tTraining Loss: 0.01342498 \tValidation Loss 0.01371002 \tTraining Acuuarcy 65.704% \tValidation Acuuarcy 66.481%\n","Epoch: 695 \tTraining Loss: 0.01342533 \tValidation Loss 0.01383444 \tTraining Acuuarcy 65.739% \tValidation Acuuarcy 66.174%\n","Epoch: 696 \tTraining Loss: 0.01341647 \tValidation Loss 0.01385532 \tTraining Acuuarcy 65.847% \tValidation Acuuarcy 65.840%\n","Epoch: 697 \tTraining Loss: 0.01341521 \tValidation Loss 0.01390327 \tTraining Acuuarcy 65.795% \tValidation Acuuarcy 66.397%\n","Epoch: 698 \tTraining Loss: 0.01340875 \tValidation Loss 0.01386360 \tTraining Acuuarcy 65.882% \tValidation Acuuarcy 65.701%\n","Epoch: 699 \tTraining Loss: 0.01341681 \tValidation Loss 0.01387308 \tTraining Acuuarcy 65.791% \tValidation Acuuarcy 66.425%\n","Epoch: 700 \tTraining Loss: 0.01338881 \tValidation Loss 0.01394062 \tTraining Acuuarcy 66.129% \tValidation Acuuarcy 65.366%\n","Epoch: 701 \tTraining Loss: 0.01343512 \tValidation Loss 0.01380460 \tTraining Acuuarcy 65.575% \tValidation Acuuarcy 66.286%\n","Epoch: 702 \tTraining Loss: 0.01342920 \tValidation Loss 0.01389876 \tTraining Acuuarcy 65.725% \tValidation Acuuarcy 65.952%\n","Epoch: 703 \tTraining Loss: 0.01342610 \tValidation Loss 0.01388513 \tTraining Acuuarcy 65.732% \tValidation Acuuarcy 65.227%\n","Epoch: 704 \tTraining Loss: 0.01343494 \tValidation Loss 0.01373924 \tTraining Acuuarcy 65.638% \tValidation Acuuarcy 66.230%\n","Epoch: 705 \tTraining Loss: 0.01341543 \tValidation Loss 0.01371559 \tTraining Acuuarcy 65.805% \tValidation Acuuarcy 66.369%\n","Epoch: 706 \tTraining Loss: 0.01344335 \tValidation Loss 0.01377228 \tTraining Acuuarcy 65.572% \tValidation Acuuarcy 66.119%\n","Epoch: 707 \tTraining Loss: 0.01341687 \tValidation Loss 0.01396781 \tTraining Acuuarcy 65.788% \tValidation Acuuarcy 65.227%\n","Epoch: 708 \tTraining Loss: 0.01341912 \tValidation Loss 0.01368050 \tTraining Acuuarcy 65.791% \tValidation Acuuarcy 67.456%\n","Epoch: 709 \tTraining Loss: 0.01340180 \tValidation Loss 0.01371061 \tTraining Acuuarcy 66.028% \tValidation Acuuarcy 66.397%\n","Epoch: 710 \tTraining Loss: 0.01341640 \tValidation Loss 0.01395823 \tTraining Acuuarcy 65.826% \tValidation Acuuarcy 65.673%\n","Epoch: 711 \tTraining Loss: 0.01342273 \tValidation Loss 0.01379164 \tTraining Acuuarcy 65.735% \tValidation Acuuarcy 66.676%\n","Epoch: 712 \tTraining Loss: 0.01342440 \tValidation Loss 0.01378741 \tTraining Acuuarcy 65.767% \tValidation Acuuarcy 66.314%\n","Epoch: 713 \tTraining Loss: 0.01341220 \tValidation Loss 0.01384720 \tTraining Acuuarcy 65.826% \tValidation Acuuarcy 65.812%\n","Epoch: 714 \tTraining Loss: 0.01341448 \tValidation Loss 0.01379683 \tTraining Acuuarcy 65.847% \tValidation Acuuarcy 66.007%\n","Epoch: 715 \tTraining Loss: 0.01340030 \tValidation Loss 0.01385249 \tTraining Acuuarcy 65.986% \tValidation Acuuarcy 65.924%\n","Epoch: 716 \tTraining Loss: 0.01341103 \tValidation Loss 0.01384082 \tTraining Acuuarcy 65.871% \tValidation Acuuarcy 65.534%\n","Epoch: 717 \tTraining Loss: 0.01342004 \tValidation Loss 0.01382884 \tTraining Acuuarcy 65.882% \tValidation Acuuarcy 66.592%\n","Epoch: 718 \tTraining Loss: 0.01342060 \tValidation Loss 0.01395170 \tTraining Acuuarcy 65.770% \tValidation Acuuarcy 65.143%\n","Epoch: 719 \tTraining Loss: 0.01342131 \tValidation Loss 0.01379401 \tTraining Acuuarcy 65.833% \tValidation Acuuarcy 66.676%\n","Epoch: 720 \tTraining Loss: 0.01341789 \tValidation Loss 0.01383885 \tTraining Acuuarcy 65.791% \tValidation Acuuarcy 65.283%\n","Epoch: 721 \tTraining Loss: 0.01343761 \tValidation Loss 0.01387359 \tTraining Acuuarcy 65.624% \tValidation Acuuarcy 66.258%\n","Epoch: 722 \tTraining Loss: 0.01341063 \tValidation Loss 0.01377163 \tTraining Acuuarcy 66.000% \tValidation Acuuarcy 66.147%\n","Epoch: 723 \tTraining Loss: 0.01341406 \tValidation Loss 0.01387681 \tTraining Acuuarcy 65.899% \tValidation Acuuarcy 64.698%\n","Epoch: 724 \tTraining Loss: 0.01341715 \tValidation Loss 0.01381370 \tTraining Acuuarcy 65.857% \tValidation Acuuarcy 66.091%\n","Epoch: 725 \tTraining Loss: 0.01342786 \tValidation Loss 0.01379238 \tTraining Acuuarcy 65.655% \tValidation Acuuarcy 66.620%\n","Epoch: 726 \tTraining Loss: 0.01339973 \tValidation Loss 0.01387654 \tTraining Acuuarcy 66.035% \tValidation Acuuarcy 64.781%\n","Epoch: 727 \tTraining Loss: 0.01344816 \tValidation Loss 0.01388773 \tTraining Acuuarcy 65.412% \tValidation Acuuarcy 65.840%\n","Epoch: 728 \tTraining Loss: 0.01341001 \tValidation Loss 0.01377798 \tTraining Acuuarcy 65.892% \tValidation Acuuarcy 66.119%\n","Epoch: 729 \tTraining Loss: 0.01340786 \tValidation Loss 0.01378581 \tTraining Acuuarcy 65.924% \tValidation Acuuarcy 65.952%\n","Epoch: 730 \tTraining Loss: 0.01343044 \tValidation Loss 0.01382377 \tTraining Acuuarcy 65.641% \tValidation Acuuarcy 66.007%\n","Epoch: 731 \tTraining Loss: 0.01340737 \tValidation Loss 0.01388395 \tTraining Acuuarcy 66.000% \tValidation Acuuarcy 65.143%\n","Epoch: 732 \tTraining Loss: 0.01342593 \tValidation Loss 0.01382936 \tTraining Acuuarcy 65.715% \tValidation Acuuarcy 65.478%\n","Epoch: 733 \tTraining Loss: 0.01341443 \tValidation Loss 0.01380371 \tTraining Acuuarcy 65.927% \tValidation Acuuarcy 65.701%\n","Epoch: 734 \tTraining Loss: 0.01342486 \tValidation Loss 0.01374736 \tTraining Acuuarcy 65.777% \tValidation Acuuarcy 66.620%\n","Epoch: 735 \tTraining Loss: 0.01343745 \tValidation Loss 0.01376364 \tTraining Acuuarcy 65.589% \tValidation Acuuarcy 66.286%\n","Epoch: 736 \tTraining Loss: 0.01341119 \tValidation Loss 0.01385319 \tTraining Acuuarcy 65.836% \tValidation Acuuarcy 65.032%\n","Epoch: 737 \tTraining Loss: 0.01340980 \tValidation Loss 0.01383024 \tTraining Acuuarcy 65.944% \tValidation Acuuarcy 66.035%\n","Epoch: 738 \tTraining Loss: 0.01342164 \tValidation Loss 0.01388392 \tTraining Acuuarcy 65.781% \tValidation Acuuarcy 65.979%\n","Epoch: 739 \tTraining Loss: 0.01340993 \tValidation Loss 0.01387252 \tTraining Acuuarcy 65.965% \tValidation Acuuarcy 66.174%\n","Epoch: 740 \tTraining Loss: 0.01342837 \tValidation Loss 0.01376910 \tTraining Acuuarcy 65.729% \tValidation Acuuarcy 66.147%\n","Epoch: 741 \tTraining Loss: 0.01340903 \tValidation Loss 0.01384607 \tTraining Acuuarcy 65.938% \tValidation Acuuarcy 65.896%\n","Epoch: 742 \tTraining Loss: 0.01343812 \tValidation Loss 0.01383004 \tTraining Acuuarcy 65.572% \tValidation Acuuarcy 66.035%\n","Epoch: 743 \tTraining Loss: 0.01343723 \tValidation Loss 0.01384337 \tTraining Acuuarcy 65.589% \tValidation Acuuarcy 65.952%\n","Epoch: 744 \tTraining Loss: 0.01341051 \tValidation Loss 0.01386671 \tTraining Acuuarcy 65.927% \tValidation Acuuarcy 66.147%\n","Epoch: 745 \tTraining Loss: 0.01343917 \tValidation Loss 0.01383497 \tTraining Acuuarcy 65.614% \tValidation Acuuarcy 65.868%\n","Epoch: 746 \tTraining Loss: 0.01343018 \tValidation Loss 0.01390993 \tTraining Acuuarcy 65.687% \tValidation Acuuarcy 65.784%\n","Epoch: 747 \tTraining Loss: 0.01342779 \tValidation Loss 0.01377414 \tTraining Acuuarcy 65.711% \tValidation Acuuarcy 66.119%\n","Epoch: 748 \tTraining Loss: 0.01339659 \tValidation Loss 0.01393245 \tTraining Acuuarcy 66.073% \tValidation Acuuarcy 66.063%\n","Epoch: 749 \tTraining Loss: 0.01342230 \tValidation Loss 0.01373076 \tTraining Acuuarcy 65.746% \tValidation Acuuarcy 65.868%\n","Epoch: 750 \tTraining Loss: 0.01341925 \tValidation Loss 0.01384682 \tTraining Acuuarcy 65.830% \tValidation Acuuarcy 65.645%\n","Epoch: 751 \tTraining Loss: 0.01340916 \tValidation Loss 0.01392037 \tTraining Acuuarcy 65.983% \tValidation Acuuarcy 66.314%\n","Epoch: 752 \tTraining Loss: 0.01340074 \tValidation Loss 0.01390678 \tTraining Acuuarcy 66.042% \tValidation Acuuarcy 65.032%\n","Epoch: 753 \tTraining Loss: 0.01343356 \tValidation Loss 0.01368564 \tTraining Acuuarcy 65.610% \tValidation Acuuarcy 67.122%\n","Epoch: 754 \tTraining Loss: 0.01340817 \tValidation Loss 0.01384926 \tTraining Acuuarcy 65.965% \tValidation Acuuarcy 66.592%\n","Epoch: 755 \tTraining Loss: 0.01343669 \tValidation Loss 0.01396368 \tTraining Acuuarcy 65.586% \tValidation Acuuarcy 65.784%\n","Epoch: 756 \tTraining Loss: 0.01340845 \tValidation Loss 0.01382185 \tTraining Acuuarcy 65.899% \tValidation Acuuarcy 65.478%\n","Epoch: 757 \tTraining Loss: 0.01342733 \tValidation Loss 0.01369319 \tTraining Acuuarcy 65.662% \tValidation Acuuarcy 66.481%\n","Epoch: 758 \tTraining Loss: 0.01342703 \tValidation Loss 0.01384977 \tTraining Acuuarcy 65.659% \tValidation Acuuarcy 65.979%\n","Epoch: 759 \tTraining Loss: 0.01342881 \tValidation Loss 0.01387014 \tTraining Acuuarcy 65.662% \tValidation Acuuarcy 66.787%\n","Epoch: 760 \tTraining Loss: 0.01341679 \tValidation Loss 0.01377870 \tTraining Acuuarcy 65.864% \tValidation Acuuarcy 65.311%\n","Epoch: 761 \tTraining Loss: 0.01343260 \tValidation Loss 0.01383007 \tTraining Acuuarcy 65.575% \tValidation Acuuarcy 65.589%\n","Epoch: 762 \tTraining Loss: 0.01341203 \tValidation Loss 0.01374173 \tTraining Acuuarcy 65.903% \tValidation Acuuarcy 66.481%\n","Epoch: 763 \tTraining Loss: 0.01344374 \tValidation Loss 0.01395981 \tTraining Acuuarcy 65.457% \tValidation Acuuarcy 65.060%\n","Epoch: 764 \tTraining Loss: 0.01341261 \tValidation Loss 0.01377564 \tTraining Acuuarcy 65.836% \tValidation Acuuarcy 66.119%\n","Epoch: 765 \tTraining Loss: 0.01343139 \tValidation Loss 0.01374184 \tTraining Acuuarcy 65.648% \tValidation Acuuarcy 66.007%\n","Epoch: 766 \tTraining Loss: 0.01344791 \tValidation Loss 0.01377500 \tTraining Acuuarcy 65.345% \tValidation Acuuarcy 66.035%\n","Epoch: 767 \tTraining Loss: 0.01340690 \tValidation Loss 0.01374261 \tTraining Acuuarcy 65.910% \tValidation Acuuarcy 67.010%\n","Epoch: 768 \tTraining Loss: 0.01340371 \tValidation Loss 0.01374799 \tTraining Acuuarcy 66.084% \tValidation Acuuarcy 65.840%\n","Epoch: 769 \tTraining Loss: 0.01342653 \tValidation Loss 0.01384792 \tTraining Acuuarcy 65.669% \tValidation Acuuarcy 65.868%\n","Epoch: 770 \tTraining Loss: 0.01342225 \tValidation Loss 0.01384418 \tTraining Acuuarcy 65.805% \tValidation Acuuarcy 65.004%\n","Epoch: 771 \tTraining Loss: 0.01341889 \tValidation Loss 0.01375222 \tTraining Acuuarcy 65.742% \tValidation Acuuarcy 66.481%\n","Epoch: 772 \tTraining Loss: 0.01341032 \tValidation Loss 0.01372382 \tTraining Acuuarcy 65.917% \tValidation Acuuarcy 66.202%\n","Epoch: 773 \tTraining Loss: 0.01341112 \tValidation Loss 0.01381957 \tTraining Acuuarcy 65.965% \tValidation Acuuarcy 66.007%\n","Epoch: 774 \tTraining Loss: 0.01340149 \tValidation Loss 0.01389918 \tTraining Acuuarcy 66.119% \tValidation Acuuarcy 65.283%\n","Epoch: 775 \tTraining Loss: 0.01342050 \tValidation Loss 0.01377994 \tTraining Acuuarcy 65.864% \tValidation Acuuarcy 65.952%\n","Epoch: 776 \tTraining Loss: 0.01342427 \tValidation Loss 0.01386504 \tTraining Acuuarcy 65.760% \tValidation Acuuarcy 65.701%\n","Epoch: 777 \tTraining Loss: 0.01341167 \tValidation Loss 0.01376337 \tTraining Acuuarcy 66.007% \tValidation Acuuarcy 66.342%\n","Epoch: 778 \tTraining Loss: 0.01341544 \tValidation Loss 0.01386494 \tTraining Acuuarcy 65.899% \tValidation Acuuarcy 66.425%\n","Epoch: 779 \tTraining Loss: 0.01342430 \tValidation Loss 0.01394968 \tTraining Acuuarcy 65.781% \tValidation Acuuarcy 65.979%\n","Epoch: 780 \tTraining Loss: 0.01340770 \tValidation Loss 0.01378436 \tTraining Acuuarcy 65.951% \tValidation Acuuarcy 66.035%\n","Epoch: 781 \tTraining Loss: 0.01341293 \tValidation Loss 0.01372004 \tTraining Acuuarcy 65.847% \tValidation Acuuarcy 66.704%\n","Epoch: 782 \tTraining Loss: 0.01342478 \tValidation Loss 0.01385024 \tTraining Acuuarcy 65.742% \tValidation Acuuarcy 65.896%\n","Epoch: 783 \tTraining Loss: 0.01341211 \tValidation Loss 0.01384784 \tTraining Acuuarcy 65.868% \tValidation Acuuarcy 65.311%\n","Epoch: 784 \tTraining Loss: 0.01340953 \tValidation Loss 0.01385209 \tTraining Acuuarcy 65.965% \tValidation Acuuarcy 65.868%\n","Epoch: 785 \tTraining Loss: 0.01343762 \tValidation Loss 0.01389560 \tTraining Acuuarcy 65.610% \tValidation Acuuarcy 65.729%\n","Epoch: 786 \tTraining Loss: 0.01342490 \tValidation Loss 0.01386613 \tTraining Acuuarcy 65.694% \tValidation Acuuarcy 65.339%\n","Epoch: 787 \tTraining Loss: 0.01340197 \tValidation Loss 0.01381332 \tTraining Acuuarcy 66.073% \tValidation Acuuarcy 66.174%\n","Epoch: 788 \tTraining Loss: 0.01341105 \tValidation Loss 0.01385758 \tTraining Acuuarcy 65.899% \tValidation Acuuarcy 66.453%\n","Epoch: 789 \tTraining Loss: 0.01340033 \tValidation Loss 0.01375789 \tTraining Acuuarcy 66.059% \tValidation Acuuarcy 66.899%\n","Epoch: 790 \tTraining Loss: 0.01340299 \tValidation Loss 0.01386667 \tTraining Acuuarcy 66.052% \tValidation Acuuarcy 66.342%\n","Epoch: 791 \tTraining Loss: 0.01344482 \tValidation Loss 0.01380798 \tTraining Acuuarcy 65.457% \tValidation Acuuarcy 66.202%\n","Epoch: 792 \tTraining Loss: 0.01339957 \tValidation Loss 0.01382267 \tTraining Acuuarcy 66.150% \tValidation Acuuarcy 65.478%\n","Epoch: 793 \tTraining Loss: 0.01341571 \tValidation Loss 0.01392479 \tTraining Acuuarcy 65.847% \tValidation Acuuarcy 64.670%\n","Epoch: 794 \tTraining Loss: 0.01340344 \tValidation Loss 0.01379904 \tTraining Acuuarcy 66.018% \tValidation Acuuarcy 66.481%\n","Epoch: 795 \tTraining Loss: 0.01340396 \tValidation Loss 0.01394465 \tTraining Acuuarcy 66.091% \tValidation Acuuarcy 65.394%\n","Epoch: 796 \tTraining Loss: 0.01343170 \tValidation Loss 0.01394110 \tTraining Acuuarcy 65.614% \tValidation Acuuarcy 65.366%\n","Epoch: 797 \tTraining Loss: 0.01342233 \tValidation Loss 0.01379956 \tTraining Acuuarcy 65.718% \tValidation Acuuarcy 65.701%\n","Epoch: 798 \tTraining Loss: 0.01342664 \tValidation Loss 0.01383218 \tTraining Acuuarcy 65.722% \tValidation Acuuarcy 65.255%\n","Epoch: 799 \tTraining Loss: 0.01341050 \tValidation Loss 0.01374665 \tTraining Acuuarcy 65.875% \tValidation Acuuarcy 65.701%\n","Epoch: 800 \tTraining Loss: 0.01341916 \tValidation Loss 0.01389117 \tTraining Acuuarcy 65.788% \tValidation Acuuarcy 65.143%\n","Epoch: 801 \tTraining Loss: 0.01341090 \tValidation Loss 0.01376317 \tTraining Acuuarcy 66.011% \tValidation Acuuarcy 66.760%\n","Epoch: 802 \tTraining Loss: 0.01339655 \tValidation Loss 0.01379419 \tTraining Acuuarcy 66.105% \tValidation Acuuarcy 66.509%\n","Epoch: 803 \tTraining Loss: 0.01342338 \tValidation Loss 0.01379653 \tTraining Acuuarcy 65.756% \tValidation Acuuarcy 66.565%\n","Epoch: 804 \tTraining Loss: 0.01341986 \tValidation Loss 0.01373754 \tTraining Acuuarcy 65.857% \tValidation Acuuarcy 65.952%\n","Epoch: 805 \tTraining Loss: 0.01344268 \tValidation Loss 0.01379918 \tTraining Acuuarcy 65.516% \tValidation Acuuarcy 66.425%\n","Epoch: 806 \tTraining Loss: 0.01340318 \tValidation Loss 0.01392315 \tTraining Acuuarcy 66.000% \tValidation Acuuarcy 66.286%\n","Epoch: 807 \tTraining Loss: 0.01340587 \tValidation Loss 0.01387343 \tTraining Acuuarcy 65.990% \tValidation Acuuarcy 65.701%\n","Epoch: 808 \tTraining Loss: 0.01342871 \tValidation Loss 0.01368710 \tTraining Acuuarcy 65.708% \tValidation Acuuarcy 66.565%\n","Epoch: 809 \tTraining Loss: 0.01339333 \tValidation Loss 0.01386517 \tTraining Acuuarcy 66.174% \tValidation Acuuarcy 65.561%\n","Epoch: 810 \tTraining Loss: 0.01342378 \tValidation Loss 0.01383441 \tTraining Acuuarcy 65.777% \tValidation Acuuarcy 65.311%\n","Epoch: 811 \tTraining Loss: 0.01343674 \tValidation Loss 0.01373630 \tTraining Acuuarcy 65.551% \tValidation Acuuarcy 66.537%\n","Epoch: 812 \tTraining Loss: 0.01340453 \tValidation Loss 0.01390843 \tTraining Acuuarcy 66.039% \tValidation Acuuarcy 66.369%\n","Epoch: 813 \tTraining Loss: 0.01342137 \tValidation Loss 0.01375439 \tTraining Acuuarcy 65.798% \tValidation Acuuarcy 66.397%\n","Epoch: 814 \tTraining Loss: 0.01340773 \tValidation Loss 0.01384040 \tTraining Acuuarcy 66.032% \tValidation Acuuarcy 66.007%\n","Epoch: 815 \tTraining Loss: 0.01341732 \tValidation Loss 0.01374535 \tTraining Acuuarcy 65.802% \tValidation Acuuarcy 66.258%\n","Epoch: 816 \tTraining Loss: 0.01341775 \tValidation Loss 0.01378008 \tTraining Acuuarcy 65.805% \tValidation Acuuarcy 66.620%\n","Epoch: 817 \tTraining Loss: 0.01340012 \tValidation Loss 0.01375295 \tTraining Acuuarcy 66.021% \tValidation Acuuarcy 66.620%\n","Epoch: 818 \tTraining Loss: 0.01343010 \tValidation Loss 0.01371178 \tTraining Acuuarcy 65.614% \tValidation Acuuarcy 66.314%\n","Epoch: 819 \tTraining Loss: 0.01340373 \tValidation Loss 0.01381343 \tTraining Acuuarcy 66.004% \tValidation Acuuarcy 66.314%\n","Epoch: 820 \tTraining Loss: 0.01342042 \tValidation Loss 0.01384407 \tTraining Acuuarcy 65.767% \tValidation Acuuarcy 65.979%\n","Epoch: 821 \tTraining Loss: 0.01343873 \tValidation Loss 0.01384692 \tTraining Acuuarcy 65.593% \tValidation Acuuarcy 66.704%\n","Epoch: 822 \tTraining Loss: 0.01342450 \tValidation Loss 0.01388807 \tTraining Acuuarcy 65.718% \tValidation Acuuarcy 65.255%\n","Epoch: 823 \tTraining Loss: 0.01343551 \tValidation Loss 0.01381135 \tTraining Acuuarcy 65.749% \tValidation Acuuarcy 66.453%\n","Epoch: 824 \tTraining Loss: 0.01339898 \tValidation Loss 0.01384689 \tTraining Acuuarcy 66.105% \tValidation Acuuarcy 65.645%\n","Epoch: 825 \tTraining Loss: 0.01341974 \tValidation Loss 0.01381961 \tTraining Acuuarcy 65.735% \tValidation Acuuarcy 66.481%\n","Epoch: 826 \tTraining Loss: 0.01340610 \tValidation Loss 0.01376147 \tTraining Acuuarcy 65.868% \tValidation Acuuarcy 66.397%\n","Epoch: 827 \tTraining Loss: 0.01341552 \tValidation Loss 0.01376449 \tTraining Acuuarcy 65.871% \tValidation Acuuarcy 66.760%\n","Epoch: 828 \tTraining Loss: 0.01341255 \tValidation Loss 0.01388055 \tTraining Acuuarcy 65.896% \tValidation Acuuarcy 65.645%\n","Epoch: 829 \tTraining Loss: 0.01341351 \tValidation Loss 0.01375868 \tTraining Acuuarcy 65.917% \tValidation Acuuarcy 65.952%\n","Epoch: 830 \tTraining Loss: 0.01341758 \tValidation Loss 0.01374598 \tTraining Acuuarcy 65.788% \tValidation Acuuarcy 65.729%\n","Epoch: 831 \tTraining Loss: 0.01340730 \tValidation Loss 0.01374352 \tTraining Acuuarcy 65.917% \tValidation Acuuarcy 67.261%\n","Epoch: 832 \tTraining Loss: 0.01341277 \tValidation Loss 0.01389207 \tTraining Acuuarcy 65.871% \tValidation Acuuarcy 66.676%\n","Epoch: 833 \tTraining Loss: 0.01341664 \tValidation Loss 0.01395019 \tTraining Acuuarcy 65.836% \tValidation Acuuarcy 65.840%\n","Epoch: 834 \tTraining Loss: 0.01343007 \tValidation Loss 0.01378745 \tTraining Acuuarcy 65.655% \tValidation Acuuarcy 65.924%\n","Epoch: 835 \tTraining Loss: 0.01340269 \tValidation Loss 0.01386997 \tTraining Acuuarcy 66.105% \tValidation Acuuarcy 66.258%\n","Epoch: 836 \tTraining Loss: 0.01340909 \tValidation Loss 0.01380749 \tTraining Acuuarcy 65.934% \tValidation Acuuarcy 65.589%\n","Epoch: 837 \tTraining Loss: 0.01340641 \tValidation Loss 0.01376054 \tTraining Acuuarcy 65.965% \tValidation Acuuarcy 66.202%\n","Epoch: 838 \tTraining Loss: 0.01341269 \tValidation Loss 0.01379081 \tTraining Acuuarcy 65.931% \tValidation Acuuarcy 65.784%\n","Epoch: 839 \tTraining Loss: 0.01342142 \tValidation Loss 0.01378195 \tTraining Acuuarcy 65.770% \tValidation Acuuarcy 66.007%\n","Epoch: 840 \tTraining Loss: 0.01341550 \tValidation Loss 0.01390924 \tTraining Acuuarcy 65.833% \tValidation Acuuarcy 65.673%\n","Epoch: 841 \tTraining Loss: 0.01343195 \tValidation Loss 0.01381429 \tTraining Acuuarcy 65.589% \tValidation Acuuarcy 66.258%\n","Epoch: 842 \tTraining Loss: 0.01339349 \tValidation Loss 0.01382553 \tTraining Acuuarcy 66.080% \tValidation Acuuarcy 66.174%\n","Epoch: 843 \tTraining Loss: 0.01342348 \tValidation Loss 0.01378672 \tTraining Acuuarcy 65.823% \tValidation Acuuarcy 66.063%\n","Epoch: 844 \tTraining Loss: 0.01342545 \tValidation Loss 0.01375361 \tTraining Acuuarcy 65.826% \tValidation Acuuarcy 66.286%\n","Epoch: 845 \tTraining Loss: 0.01340265 \tValidation Loss 0.01371272 \tTraining Acuuarcy 66.007% \tValidation Acuuarcy 66.927%\n","Epoch: 846 \tTraining Loss: 0.01340209 \tValidation Loss 0.01380244 \tTraining Acuuarcy 66.021% \tValidation Acuuarcy 65.617%\n","Epoch: 847 \tTraining Loss: 0.01341035 \tValidation Loss 0.01369550 \tTraining Acuuarcy 65.934% \tValidation Acuuarcy 66.425%\n","Epoch: 848 \tTraining Loss: 0.01341697 \tValidation Loss 0.01391909 \tTraining Acuuarcy 65.753% \tValidation Acuuarcy 65.617%\n","Epoch: 849 \tTraining Loss: 0.01345345 \tValidation Loss 0.01383398 \tTraining Acuuarcy 65.370% \tValidation Acuuarcy 65.143%\n","Epoch: 850 \tTraining Loss: 0.01341793 \tValidation Loss 0.01395187 \tTraining Acuuarcy 65.833% \tValidation Acuuarcy 65.952%\n","Epoch: 851 \tTraining Loss: 0.01339951 \tValidation Loss 0.01391343 \tTraining Acuuarcy 66.021% \tValidation Acuuarcy 66.425%\n","Epoch: 852 \tTraining Loss: 0.01339381 \tValidation Loss 0.01396306 \tTraining Acuuarcy 66.206% \tValidation Acuuarcy 65.004%\n","Epoch: 853 \tTraining Loss: 0.01342241 \tValidation Loss 0.01393262 \tTraining Acuuarcy 65.805% \tValidation Acuuarcy 65.756%\n","Epoch: 854 \tTraining Loss: 0.01340168 \tValidation Loss 0.01381775 \tTraining Acuuarcy 66.025% \tValidation Acuuarcy 66.787%\n","Epoch: 855 \tTraining Loss: 0.01340116 \tValidation Loss 0.01386864 \tTraining Acuuarcy 66.045% \tValidation Acuuarcy 66.425%\n","Epoch: 856 \tTraining Loss: 0.01340772 \tValidation Loss 0.01383178 \tTraining Acuuarcy 65.979% \tValidation Acuuarcy 65.366%\n","Epoch: 857 \tTraining Loss: 0.01340839 \tValidation Loss 0.01393624 \tTraining Acuuarcy 65.962% \tValidation Acuuarcy 64.586%\n","Epoch: 858 \tTraining Loss: 0.01341889 \tValidation Loss 0.01375477 \tTraining Acuuarcy 65.788% \tValidation Acuuarcy 66.147%\n","Epoch: 859 \tTraining Loss: 0.01339729 \tValidation Loss 0.01379820 \tTraining Acuuarcy 66.126% \tValidation Acuuarcy 65.171%\n","Epoch: 860 \tTraining Loss: 0.01343066 \tValidation Loss 0.01380539 \tTraining Acuuarcy 65.617% \tValidation Acuuarcy 66.787%\n","Epoch: 861 \tTraining Loss: 0.01340116 \tValidation Loss 0.01373119 \tTraining Acuuarcy 65.983% \tValidation Acuuarcy 66.620%\n","Epoch: 862 \tTraining Loss: 0.01342379 \tValidation Loss 0.01377206 \tTraining Acuuarcy 65.701% \tValidation Acuuarcy 66.927%\n","Epoch: 863 \tTraining Loss: 0.01340058 \tValidation Loss 0.01393082 \tTraining Acuuarcy 66.039% \tValidation Acuuarcy 65.478%\n","Epoch: 864 \tTraining Loss: 0.01341272 \tValidation Loss 0.01372991 \tTraining Acuuarcy 65.917% \tValidation Acuuarcy 66.955%\n","Epoch: 865 \tTraining Loss: 0.01341578 \tValidation Loss 0.01375457 \tTraining Acuuarcy 65.896% \tValidation Acuuarcy 66.397%\n","Epoch: 866 \tTraining Loss: 0.01341211 \tValidation Loss 0.01393155 \tTraining Acuuarcy 65.868% \tValidation Acuuarcy 66.035%\n","Epoch: 867 \tTraining Loss: 0.01340383 \tValidation Loss 0.01379303 \tTraining Acuuarcy 66.028% \tValidation Acuuarcy 65.812%\n","Epoch: 868 \tTraining Loss: 0.01341689 \tValidation Loss 0.01385854 \tTraining Acuuarcy 65.854% \tValidation Acuuarcy 65.896%\n","Epoch: 869 \tTraining Loss: 0.01339575 \tValidation Loss 0.01384470 \tTraining Acuuarcy 66.115% \tValidation Acuuarcy 66.314%\n","Epoch: 870 \tTraining Loss: 0.01339379 \tValidation Loss 0.01371691 \tTraining Acuuarcy 66.025% \tValidation Acuuarcy 67.122%\n","Epoch: 871 \tTraining Loss: 0.01340446 \tValidation Loss 0.01381560 \tTraining Acuuarcy 65.979% \tValidation Acuuarcy 65.812%\n","Epoch: 872 \tTraining Loss: 0.01339931 \tValidation Loss 0.01380610 \tTraining Acuuarcy 66.077% \tValidation Acuuarcy 65.673%\n","Epoch: 873 \tTraining Loss: 0.01339093 \tValidation Loss 0.01380258 \tTraining Acuuarcy 66.157% \tValidation Acuuarcy 66.565%\n","Epoch: 874 \tTraining Loss: 0.01341997 \tValidation Loss 0.01390878 \tTraining Acuuarcy 65.816% \tValidation Acuuarcy 66.369%\n","Epoch: 875 \tTraining Loss: 0.01339123 \tValidation Loss 0.01375363 \tTraining Acuuarcy 66.147% \tValidation Acuuarcy 66.982%\n","Epoch: 876 \tTraining Loss: 0.01341394 \tValidation Loss 0.01377988 \tTraining Acuuarcy 65.951% \tValidation Acuuarcy 65.729%\n","Epoch: 877 \tTraining Loss: 0.01338441 \tValidation Loss 0.01382190 \tTraining Acuuarcy 66.230% \tValidation Acuuarcy 66.147%\n","Epoch: 878 \tTraining Loss: 0.01341333 \tValidation Loss 0.01381339 \tTraining Acuuarcy 65.917% \tValidation Acuuarcy 65.701%\n","Epoch: 879 \tTraining Loss: 0.01342615 \tValidation Loss 0.01370633 \tTraining Acuuarcy 65.680% \tValidation Acuuarcy 66.286%\n","Epoch: 880 \tTraining Loss: 0.01341675 \tValidation Loss 0.01387057 \tTraining Acuuarcy 65.847% \tValidation Acuuarcy 65.617%\n","Epoch: 881 \tTraining Loss: 0.01339403 \tValidation Loss 0.01384229 \tTraining Acuuarcy 66.126% \tValidation Acuuarcy 65.589%\n","Epoch: 882 \tTraining Loss: 0.01339057 \tValidation Loss 0.01373506 \tTraining Acuuarcy 66.227% \tValidation Acuuarcy 67.289%\n","Epoch: 883 \tTraining Loss: 0.01340807 \tValidation Loss 0.01378998 \tTraining Acuuarcy 66.028% \tValidation Acuuarcy 66.592%\n","Epoch: 884 \tTraining Loss: 0.01337999 \tValidation Loss 0.01384049 \tTraining Acuuarcy 66.328% \tValidation Acuuarcy 66.592%\n","Epoch: 885 \tTraining Loss: 0.01339530 \tValidation Loss 0.01388259 \tTraining Acuuarcy 66.150% \tValidation Acuuarcy 66.174%\n","Epoch: 886 \tTraining Loss: 0.01340134 \tValidation Loss 0.01377386 \tTraining Acuuarcy 66.028% \tValidation Acuuarcy 66.397%\n","Epoch: 887 \tTraining Loss: 0.01338089 \tValidation Loss 0.01381996 \tTraining Acuuarcy 66.359% \tValidation Acuuarcy 66.119%\n","Epoch: 888 \tTraining Loss: 0.01340643 \tValidation Loss 0.01383394 \tTraining Acuuarcy 66.021% \tValidation Acuuarcy 65.979%\n","Epoch: 889 \tTraining Loss: 0.01338507 \tValidation Loss 0.01387470 \tTraining Acuuarcy 66.258% \tValidation Acuuarcy 66.314%\n","Epoch: 890 \tTraining Loss: 0.01339996 \tValidation Loss 0.01385579 \tTraining Acuuarcy 66.032% \tValidation Acuuarcy 65.812%\n","Epoch: 891 \tTraining Loss: 0.01338953 \tValidation Loss 0.01382803 \tTraining Acuuarcy 66.237% \tValidation Acuuarcy 66.091%\n","Epoch: 892 \tTraining Loss: 0.01340013 \tValidation Loss 0.01370923 \tTraining Acuuarcy 66.042% \tValidation Acuuarcy 67.122%\n","Epoch: 893 \tTraining Loss: 0.01342107 \tValidation Loss 0.01386906 \tTraining Acuuarcy 65.802% \tValidation Acuuarcy 65.478%\n","Epoch: 894 \tTraining Loss: 0.01341616 \tValidation Loss 0.01398134 \tTraining Acuuarcy 65.868% \tValidation Acuuarcy 65.673%\n","Epoch: 895 \tTraining Loss: 0.01340429 \tValidation Loss 0.01395030 \tTraining Acuuarcy 65.934% \tValidation Acuuarcy 65.116%\n","Epoch: 896 \tTraining Loss: 0.01339943 \tValidation Loss 0.01380586 \tTraining Acuuarcy 66.084% \tValidation Acuuarcy 66.565%\n","Epoch: 897 \tTraining Loss: 0.01342870 \tValidation Loss 0.01376292 \tTraining Acuuarcy 65.631% \tValidation Acuuarcy 67.038%\n","Epoch: 898 \tTraining Loss: 0.01342511 \tValidation Loss 0.01387729 \tTraining Acuuarcy 65.833% \tValidation Acuuarcy 65.478%\n","Epoch: 899 \tTraining Loss: 0.01342094 \tValidation Loss 0.01373534 \tTraining Acuuarcy 65.708% \tValidation Acuuarcy 65.896%\n","Epoch: 900 \tTraining Loss: 0.01339445 \tValidation Loss 0.01378738 \tTraining Acuuarcy 66.140% \tValidation Acuuarcy 66.565%\n","Epoch: 901 \tTraining Loss: 0.01341052 \tValidation Loss 0.01380825 \tTraining Acuuarcy 65.892% \tValidation Acuuarcy 65.673%\n","Epoch: 902 \tTraining Loss: 0.01344696 \tValidation Loss 0.01384116 \tTraining Acuuarcy 65.425% \tValidation Acuuarcy 65.896%\n","Epoch: 903 \tTraining Loss: 0.01341435 \tValidation Loss 0.01387944 \tTraining Acuuarcy 65.850% \tValidation Acuuarcy 66.119%\n","Epoch: 904 \tTraining Loss: 0.01341546 \tValidation Loss 0.01364543 \tTraining Acuuarcy 65.840% \tValidation Acuuarcy 67.094%\n","Epoch: 905 \tTraining Loss: 0.01339725 \tValidation Loss 0.01380276 \tTraining Acuuarcy 66.115% \tValidation Acuuarcy 66.481%\n","Epoch: 906 \tTraining Loss: 0.01340208 \tValidation Loss 0.01380020 \tTraining Acuuarcy 65.955% \tValidation Acuuarcy 66.453%\n","Epoch: 907 \tTraining Loss: 0.01341882 \tValidation Loss 0.01399321 \tTraining Acuuarcy 65.798% \tValidation Acuuarcy 65.868%\n","Epoch: 908 \tTraining Loss: 0.01343566 \tValidation Loss 0.01381820 \tTraining Acuuarcy 65.607% \tValidation Acuuarcy 66.314%\n","Epoch: 909 \tTraining Loss: 0.01340964 \tValidation Loss 0.01398113 \tTraining Acuuarcy 65.899% \tValidation Acuuarcy 65.561%\n","Epoch: 910 \tTraining Loss: 0.01341696 \tValidation Loss 0.01373029 \tTraining Acuuarcy 65.840% \tValidation Acuuarcy 66.760%\n","Epoch: 911 \tTraining Loss: 0.01340995 \tValidation Loss 0.01373191 \tTraining Acuuarcy 65.955% \tValidation Acuuarcy 65.952%\n","Epoch: 912 \tTraining Loss: 0.01342375 \tValidation Loss 0.01378116 \tTraining Acuuarcy 65.742% \tValidation Acuuarcy 65.896%\n","Epoch: 913 \tTraining Loss: 0.01341345 \tValidation Loss 0.01381675 \tTraining Acuuarcy 65.816% \tValidation Acuuarcy 66.927%\n","Epoch: 914 \tTraining Loss: 0.01340569 \tValidation Loss 0.01390123 \tTraining Acuuarcy 66.025% \tValidation Acuuarcy 64.948%\n","Epoch: 915 \tTraining Loss: 0.01339675 \tValidation Loss 0.01385104 \tTraining Acuuarcy 66.133% \tValidation Acuuarcy 65.924%\n","Epoch: 916 \tTraining Loss: 0.01340861 \tValidation Loss 0.01376608 \tTraining Acuuarcy 65.951% \tValidation Acuuarcy 66.258%\n","Epoch: 917 \tTraining Loss: 0.01338732 \tValidation Loss 0.01396605 \tTraining Acuuarcy 66.136% \tValidation Acuuarcy 65.645%\n","Epoch: 918 \tTraining Loss: 0.01340417 \tValidation Loss 0.01381111 \tTraining Acuuarcy 66.014% \tValidation Acuuarcy 66.453%\n","Epoch: 919 \tTraining Loss: 0.01342420 \tValidation Loss 0.01382965 \tTraining Acuuarcy 65.816% \tValidation Acuuarcy 66.174%\n","Epoch: 920 \tTraining Loss: 0.01344750 \tValidation Loss 0.01382202 \tTraining Acuuarcy 65.443% \tValidation Acuuarcy 65.422%\n","Epoch: 921 \tTraining Loss: 0.01340995 \tValidation Loss 0.01379770 \tTraining Acuuarcy 65.931% \tValidation Acuuarcy 65.617%\n","Epoch: 922 \tTraining Loss: 0.01340793 \tValidation Loss 0.01376706 \tTraining Acuuarcy 66.000% \tValidation Acuuarcy 67.010%\n","Epoch: 923 \tTraining Loss: 0.01339870 \tValidation Loss 0.01389381 \tTraining Acuuarcy 66.056% \tValidation Acuuarcy 65.924%\n","Epoch: 924 \tTraining Loss: 0.01340000 \tValidation Loss 0.01392632 \tTraining Acuuarcy 66.063% \tValidation Acuuarcy 64.085%\n","Epoch: 925 \tTraining Loss: 0.01343703 \tValidation Loss 0.01379059 \tTraining Acuuarcy 65.537% \tValidation Acuuarcy 66.565%\n","Epoch: 926 \tTraining Loss: 0.01339731 \tValidation Loss 0.01380028 \tTraining Acuuarcy 66.066% \tValidation Acuuarcy 65.756%\n","Epoch: 927 \tTraining Loss: 0.01341554 \tValidation Loss 0.01382471 \tTraining Acuuarcy 65.913% \tValidation Acuuarcy 66.258%\n","Epoch: 928 \tTraining Loss: 0.01343064 \tValidation Loss 0.01382098 \tTraining Acuuarcy 65.621% \tValidation Acuuarcy 66.342%\n","Epoch: 929 \tTraining Loss: 0.01343960 \tValidation Loss 0.01375562 \tTraining Acuuarcy 65.530% \tValidation Acuuarcy 66.815%\n","Epoch: 930 \tTraining Loss: 0.01340583 \tValidation Loss 0.01366126 \tTraining Acuuarcy 66.018% \tValidation Acuuarcy 66.787%\n","Epoch: 931 \tTraining Loss: 0.01341414 \tValidation Loss 0.01393172 \tTraining Acuuarcy 65.896% \tValidation Acuuarcy 65.478%\n","Epoch: 932 \tTraining Loss: 0.01339899 \tValidation Loss 0.01378466 \tTraining Acuuarcy 66.112% \tValidation Acuuarcy 66.035%\n","Epoch: 933 \tTraining Loss: 0.01339107 \tValidation Loss 0.01383270 \tTraining Acuuarcy 66.129% \tValidation Acuuarcy 66.035%\n","Epoch: 934 \tTraining Loss: 0.01341995 \tValidation Loss 0.01375544 \tTraining Acuuarcy 65.788% \tValidation Acuuarcy 66.732%\n","Epoch: 935 \tTraining Loss: 0.01341787 \tValidation Loss 0.01372793 \tTraining Acuuarcy 65.868% \tValidation Acuuarcy 66.091%\n","Epoch: 936 \tTraining Loss: 0.01341531 \tValidation Loss 0.01386733 \tTraining Acuuarcy 65.885% \tValidation Acuuarcy 66.453%\n","Epoch: 937 \tTraining Loss: 0.01343159 \tValidation Loss 0.01375034 \tTraining Acuuarcy 65.690% \tValidation Acuuarcy 66.509%\n","Epoch: 938 \tTraining Loss: 0.01339988 \tValidation Loss 0.01379970 \tTraining Acuuarcy 66.014% \tValidation Acuuarcy 66.035%\n","Epoch: 939 \tTraining Loss: 0.01340116 \tValidation Loss 0.01378415 \tTraining Acuuarcy 65.986% \tValidation Acuuarcy 66.119%\n","Epoch: 940 \tTraining Loss: 0.01340187 \tValidation Loss 0.01384232 \tTraining Acuuarcy 65.938% \tValidation Acuuarcy 65.784%\n","Epoch: 941 \tTraining Loss: 0.01339897 \tValidation Loss 0.01387386 \tTraining Acuuarcy 65.983% \tValidation Acuuarcy 65.589%\n","Epoch: 942 \tTraining Loss: 0.01340302 \tValidation Loss 0.01373295 \tTraining Acuuarcy 66.070% \tValidation Acuuarcy 66.648%\n","Epoch: 943 \tTraining Loss: 0.01340588 \tValidation Loss 0.01383038 \tTraining Acuuarcy 66.039% \tValidation Acuuarcy 65.478%\n","Epoch: 944 \tTraining Loss: 0.01340432 \tValidation Loss 0.01389186 \tTraining Acuuarcy 65.955% \tValidation Acuuarcy 66.063%\n","Epoch: 945 \tTraining Loss: 0.01341931 \tValidation Loss 0.01374322 \tTraining Acuuarcy 65.739% \tValidation Acuuarcy 65.924%\n","Epoch: 946 \tTraining Loss: 0.01340168 \tValidation Loss 0.01388023 \tTraining Acuuarcy 66.056% \tValidation Acuuarcy 66.202%\n","Epoch: 947 \tTraining Loss: 0.01340567 \tValidation Loss 0.01378150 \tTraining Acuuarcy 65.990% \tValidation Acuuarcy 66.732%\n","Epoch: 948 \tTraining Loss: 0.01341254 \tValidation Loss 0.01399123 \tTraining Acuuarcy 65.899% \tValidation Acuuarcy 65.366%\n","Epoch: 949 \tTraining Loss: 0.01338455 \tValidation Loss 0.01385477 \tTraining Acuuarcy 66.307% \tValidation Acuuarcy 65.812%\n","Epoch: 950 \tTraining Loss: 0.01342241 \tValidation Loss 0.01383572 \tTraining Acuuarcy 65.735% \tValidation Acuuarcy 66.147%\n","Epoch: 951 \tTraining Loss: 0.01338278 \tValidation Loss 0.01392993 \tTraining Acuuarcy 66.289% \tValidation Acuuarcy 65.422%\n","Epoch: 952 \tTraining Loss: 0.01339363 \tValidation Loss 0.01380617 \tTraining Acuuarcy 66.059% \tValidation Acuuarcy 65.784%\n","Epoch: 953 \tTraining Loss: 0.01338595 \tValidation Loss 0.01375020 \tTraining Acuuarcy 66.227% \tValidation Acuuarcy 65.701%\n","Epoch: 954 \tTraining Loss: 0.01340057 \tValidation Loss 0.01368181 \tTraining Acuuarcy 66.080% \tValidation Acuuarcy 66.648%\n","Epoch: 955 \tTraining Loss: 0.01340491 \tValidation Loss 0.01377659 \tTraining Acuuarcy 65.983% \tValidation Acuuarcy 66.648%\n","Epoch: 956 \tTraining Loss: 0.01339592 \tValidation Loss 0.01382077 \tTraining Acuuarcy 66.077% \tValidation Acuuarcy 66.314%\n","Epoch: 957 \tTraining Loss: 0.01340403 \tValidation Loss 0.01387837 \tTraining Acuuarcy 65.955% \tValidation Acuuarcy 66.174%\n","Epoch: 958 \tTraining Loss: 0.01342852 \tValidation Loss 0.01385721 \tTraining Acuuarcy 65.729% \tValidation Acuuarcy 65.673%\n","Epoch: 959 \tTraining Loss: 0.01342106 \tValidation Loss 0.01382139 \tTraining Acuuarcy 65.795% \tValidation Acuuarcy 66.286%\n","Epoch: 960 \tTraining Loss: 0.01340481 \tValidation Loss 0.01374979 \tTraining Acuuarcy 66.052% \tValidation Acuuarcy 66.453%\n","Epoch: 961 \tTraining Loss: 0.01341724 \tValidation Loss 0.01392842 \tTraining Acuuarcy 65.882% \tValidation Acuuarcy 65.812%\n","Epoch: 962 \tTraining Loss: 0.01341168 \tValidation Loss 0.01373882 \tTraining Acuuarcy 65.944% \tValidation Acuuarcy 66.760%\n","Epoch: 963 \tTraining Loss: 0.01340111 \tValidation Loss 0.01381287 \tTraining Acuuarcy 66.028% \tValidation Acuuarcy 65.756%\n","Epoch: 964 \tTraining Loss: 0.01340552 \tValidation Loss 0.01398208 \tTraining Acuuarcy 66.059% \tValidation Acuuarcy 65.924%\n","Epoch: 965 \tTraining Loss: 0.01339465 \tValidation Loss 0.01376508 \tTraining Acuuarcy 66.147% \tValidation Acuuarcy 66.955%\n","Epoch: 966 \tTraining Loss: 0.01342466 \tValidation Loss 0.01373348 \tTraining Acuuarcy 65.683% \tValidation Acuuarcy 66.397%\n","Epoch: 967 \tTraining Loss: 0.01339324 \tValidation Loss 0.01370954 \tTraining Acuuarcy 66.115% \tValidation Acuuarcy 66.537%\n","Epoch: 968 \tTraining Loss: 0.01340272 \tValidation Loss 0.01385211 \tTraining Acuuarcy 65.972% \tValidation Acuuarcy 65.896%\n","Epoch: 969 \tTraining Loss: 0.01341329 \tValidation Loss 0.01383968 \tTraining Acuuarcy 65.924% \tValidation Acuuarcy 66.732%\n","Epoch: 970 \tTraining Loss: 0.01341759 \tValidation Loss 0.01370142 \tTraining Acuuarcy 65.868% \tValidation Acuuarcy 67.233%\n","Epoch: 971 \tTraining Loss: 0.01341074 \tValidation Loss 0.01360906 \tTraining Acuuarcy 65.882% \tValidation Acuuarcy 67.568%\n","Epoch: 972 \tTraining Loss: 0.01341211 \tValidation Loss 0.01368034 \tTraining Acuuarcy 65.927% \tValidation Acuuarcy 66.704%\n","Epoch: 973 \tTraining Loss: 0.01340807 \tValidation Loss 0.01386242 \tTraining Acuuarcy 65.910% \tValidation Acuuarcy 65.673%\n","Epoch: 974 \tTraining Loss: 0.01337527 \tValidation Loss 0.01376067 \tTraining Acuuarcy 66.457% \tValidation Acuuarcy 66.258%\n","Epoch: 975 \tTraining Loss: 0.01339797 \tValidation Loss 0.01381499 \tTraining Acuuarcy 66.000% \tValidation Acuuarcy 66.007%\n","Epoch: 976 \tTraining Loss: 0.01339764 \tValidation Loss 0.01383968 \tTraining Acuuarcy 66.119% \tValidation Acuuarcy 66.676%\n","Epoch: 977 \tTraining Loss: 0.01338591 \tValidation Loss 0.01376416 \tTraining Acuuarcy 66.216% \tValidation Acuuarcy 65.450%\n","Epoch: 978 \tTraining Loss: 0.01338945 \tValidation Loss 0.01371116 \tTraining Acuuarcy 66.112% \tValidation Acuuarcy 66.091%\n","Epoch: 979 \tTraining Loss: 0.01342074 \tValidation Loss 0.01377241 \tTraining Acuuarcy 65.774% \tValidation Acuuarcy 66.760%\n","Epoch: 980 \tTraining Loss: 0.01342673 \tValidation Loss 0.01371315 \tTraining Acuuarcy 65.742% \tValidation Acuuarcy 67.595%\n","Epoch: 981 \tTraining Loss: 0.01339054 \tValidation Loss 0.01374573 \tTraining Acuuarcy 66.234% \tValidation Acuuarcy 65.868%\n","Epoch: 982 \tTraining Loss: 0.01340165 \tValidation Loss 0.01384183 \tTraining Acuuarcy 65.938% \tValidation Acuuarcy 65.924%\n","Epoch: 983 \tTraining Loss: 0.01340099 \tValidation Loss 0.01381799 \tTraining Acuuarcy 66.084% \tValidation Acuuarcy 66.955%\n","Epoch: 984 \tTraining Loss: 0.01340738 \tValidation Loss 0.01386789 \tTraining Acuuarcy 65.986% \tValidation Acuuarcy 66.230%\n","Epoch: 985 \tTraining Loss: 0.01339453 \tValidation Loss 0.01383476 \tTraining Acuuarcy 66.108% \tValidation Acuuarcy 66.007%\n","Epoch: 986 \tTraining Loss: 0.01341384 \tValidation Loss 0.01387988 \tTraining Acuuarcy 65.986% \tValidation Acuuarcy 66.258%\n","Epoch: 987 \tTraining Loss: 0.01340096 \tValidation Loss 0.01379125 \tTraining Acuuarcy 66.045% \tValidation Acuuarcy 65.311%\n","Epoch: 988 \tTraining Loss: 0.01339467 \tValidation Loss 0.01390474 \tTraining Acuuarcy 66.098% \tValidation Acuuarcy 65.673%\n","Epoch: 989 \tTraining Loss: 0.01340447 \tValidation Loss 0.01391322 \tTraining Acuuarcy 65.941% \tValidation Acuuarcy 65.534%\n","Epoch: 990 \tTraining Loss: 0.01342228 \tValidation Loss 0.01376935 \tTraining Acuuarcy 65.802% \tValidation Acuuarcy 66.091%\n","Epoch: 991 \tTraining Loss: 0.01339107 \tValidation Loss 0.01378319 \tTraining Acuuarcy 66.105% \tValidation Acuuarcy 65.924%\n","Epoch: 992 \tTraining Loss: 0.01341118 \tValidation Loss 0.01366139 \tTraining Acuuarcy 65.924% \tValidation Acuuarcy 66.732%\n","Epoch: 993 \tTraining Loss: 0.01338707 \tValidation Loss 0.01376652 \tTraining Acuuarcy 66.213% \tValidation Acuuarcy 66.007%\n","Epoch: 994 \tTraining Loss: 0.01338948 \tValidation Loss 0.01368869 \tTraining Acuuarcy 66.213% \tValidation Acuuarcy 66.620%\n","Epoch: 995 \tTraining Loss: 0.01339593 \tValidation Loss 0.01393349 \tTraining Acuuarcy 66.136% \tValidation Acuuarcy 66.787%\n","Epoch: 996 \tTraining Loss: 0.01339916 \tValidation Loss 0.01379381 \tTraining Acuuarcy 66.119% \tValidation Acuuarcy 66.509%\n","Epoch: 997 \tTraining Loss: 0.01338867 \tValidation Loss 0.01379458 \tTraining Acuuarcy 66.268% \tValidation Acuuarcy 66.592%\n","Epoch: 998 \tTraining Loss: 0.01339203 \tValidation Loss 0.01385672 \tTraining Acuuarcy 66.147% \tValidation Acuuarcy 65.645%\n","Epoch: 999 \tTraining Loss: 0.01340064 \tValidation Loss 0.01377749 \tTraining Acuuarcy 66.032% \tValidation Acuuarcy 66.174%\n","Epoch: 1000 \tTraining Loss: 0.01339528 \tValidation Loss 0.01372645 \tTraining Acuuarcy 66.119% \tValidation Acuuarcy 66.955%\n","Epoch: 1001 \tTraining Loss: 0.01341996 \tValidation Loss 0.01374680 \tTraining Acuuarcy 65.795% \tValidation Acuuarcy 67.233%\n","Epoch: 1002 \tTraining Loss: 0.01338604 \tValidation Loss 0.01373955 \tTraining Acuuarcy 66.227% \tValidation Acuuarcy 66.815%\n","Epoch: 1003 \tTraining Loss: 0.01338274 \tValidation Loss 0.01385292 \tTraining Acuuarcy 66.265% \tValidation Acuuarcy 65.784%\n","Epoch: 1004 \tTraining Loss: 0.01338501 \tValidation Loss 0.01382585 \tTraining Acuuarcy 66.216% \tValidation Acuuarcy 66.760%\n","Epoch: 1005 \tTraining Loss: 0.01341093 \tValidation Loss 0.01380790 \tTraining Acuuarcy 65.889% \tValidation Acuuarcy 67.122%\n","Epoch: 1006 \tTraining Loss: 0.01342361 \tValidation Loss 0.01386363 \tTraining Acuuarcy 65.788% \tValidation Acuuarcy 65.617%\n","Epoch: 1007 \tTraining Loss: 0.01339412 \tValidation Loss 0.01372415 \tTraining Acuuarcy 66.122% \tValidation Acuuarcy 66.871%\n","Epoch: 1008 \tTraining Loss: 0.01339740 \tValidation Loss 0.01385364 \tTraining Acuuarcy 66.140% \tValidation Acuuarcy 66.565%\n","Epoch: 1009 \tTraining Loss: 0.01338896 \tValidation Loss 0.01377462 \tTraining Acuuarcy 66.147% \tValidation Acuuarcy 66.147%\n","Epoch: 1010 \tTraining Loss: 0.01341284 \tValidation Loss 0.01375196 \tTraining Acuuarcy 65.843% \tValidation Acuuarcy 66.174%\n","Epoch: 1011 \tTraining Loss: 0.01338912 \tValidation Loss 0.01373517 \tTraining Acuuarcy 66.108% \tValidation Acuuarcy 67.122%\n","Epoch: 1012 \tTraining Loss: 0.01341347 \tValidation Loss 0.01369103 \tTraining Acuuarcy 65.847% \tValidation Acuuarcy 66.676%\n","Epoch: 1013 \tTraining Loss: 0.01342514 \tValidation Loss 0.01382955 \tTraining Acuuarcy 65.749% \tValidation Acuuarcy 65.896%\n","Epoch: 1014 \tTraining Loss: 0.01340606 \tValidation Loss 0.01386896 \tTraining Acuuarcy 65.938% \tValidation Acuuarcy 66.119%\n","Epoch: 1015 \tTraining Loss: 0.01338206 \tValidation Loss 0.01379562 \tTraining Acuuarcy 66.376% \tValidation Acuuarcy 65.756%\n","Epoch: 1016 \tTraining Loss: 0.01338249 \tValidation Loss 0.01384832 \tTraining Acuuarcy 66.195% \tValidation Acuuarcy 67.289%\n","Epoch: 1017 \tTraining Loss: 0.01340842 \tValidation Loss 0.01388177 \tTraining Acuuarcy 65.913% \tValidation Acuuarcy 65.896%\n","Epoch: 1018 \tTraining Loss: 0.01340963 \tValidation Loss 0.01387453 \tTraining Acuuarcy 65.931% \tValidation Acuuarcy 65.589%\n","Epoch: 1019 \tTraining Loss: 0.01340098 \tValidation Loss 0.01366866 \tTraining Acuuarcy 65.983% \tValidation Acuuarcy 66.955%\n","Epoch: 1020 \tTraining Loss: 0.01341278 \tValidation Loss 0.01373271 \tTraining Acuuarcy 65.906% \tValidation Acuuarcy 67.289%\n","Epoch: 1021 \tTraining Loss: 0.01342177 \tValidation Loss 0.01381853 \tTraining Acuuarcy 65.718% \tValidation Acuuarcy 65.924%\n","Epoch: 1022 \tTraining Loss: 0.01339150 \tValidation Loss 0.01380772 \tTraining Acuuarcy 66.143% \tValidation Acuuarcy 66.425%\n","Epoch: 1023 \tTraining Loss: 0.01340746 \tValidation Loss 0.01393926 \tTraining Acuuarcy 66.004% \tValidation Acuuarcy 65.534%\n","Epoch: 1024 \tTraining Loss: 0.01341355 \tValidation Loss 0.01387837 \tTraining Acuuarcy 65.857% \tValidation Acuuarcy 65.561%\n","Epoch: 1025 \tTraining Loss: 0.01339350 \tValidation Loss 0.01375825 \tTraining Acuuarcy 66.171% \tValidation Acuuarcy 67.010%\n","Epoch: 1026 \tTraining Loss: 0.01339535 \tValidation Loss 0.01380887 \tTraining Acuuarcy 66.178% \tValidation Acuuarcy 65.868%\n","Epoch: 1027 \tTraining Loss: 0.01339483 \tValidation Loss 0.01385662 \tTraining Acuuarcy 66.195% \tValidation Acuuarcy 65.868%\n","Epoch: 1028 \tTraining Loss: 0.01340774 \tValidation Loss 0.01378190 \tTraining Acuuarcy 66.004% \tValidation Acuuarcy 66.397%\n","Epoch: 1029 \tTraining Loss: 0.01337878 \tValidation Loss 0.01385554 \tTraining Acuuarcy 66.390% \tValidation Acuuarcy 65.645%\n","Epoch: 1030 \tTraining Loss: 0.01340501 \tValidation Loss 0.01385155 \tTraining Acuuarcy 65.931% \tValidation Acuuarcy 65.840%\n","Epoch: 1031 \tTraining Loss: 0.01341248 \tValidation Loss 0.01385551 \tTraining Acuuarcy 65.938% \tValidation Acuuarcy 66.342%\n","Epoch: 1032 \tTraining Loss: 0.01340456 \tValidation Loss 0.01374824 \tTraining Acuuarcy 66.000% \tValidation Acuuarcy 66.397%\n","Epoch: 1033 \tTraining Loss: 0.01339064 \tValidation Loss 0.01384428 \tTraining Acuuarcy 66.227% \tValidation Acuuarcy 65.924%\n","Epoch: 1034 \tTraining Loss: 0.01338062 \tValidation Loss 0.01380632 \tTraining Acuuarcy 66.335% \tValidation Acuuarcy 66.565%\n","Epoch: 1035 \tTraining Loss: 0.01338998 \tValidation Loss 0.01384756 \tTraining Acuuarcy 66.268% \tValidation Acuuarcy 66.565%\n","Epoch: 1036 \tTraining Loss: 0.01339174 \tValidation Loss 0.01381045 \tTraining Acuuarcy 66.153% \tValidation Acuuarcy 65.617%\n","Epoch: 1037 \tTraining Loss: 0.01339021 \tValidation Loss 0.01370994 \tTraining Acuuarcy 66.202% \tValidation Acuuarcy 66.955%\n","Epoch: 1038 \tTraining Loss: 0.01339557 \tValidation Loss 0.01376885 \tTraining Acuuarcy 66.164% \tValidation Acuuarcy 66.174%\n","Epoch: 1039 \tTraining Loss: 0.01336862 \tValidation Loss 0.01374709 \tTraining Acuuarcy 66.446% \tValidation Acuuarcy 65.812%\n","Epoch: 1040 \tTraining Loss: 0.01340473 \tValidation Loss 0.01383323 \tTraining Acuuarcy 66.059% \tValidation Acuuarcy 66.202%\n","Epoch: 1041 \tTraining Loss: 0.01341410 \tValidation Loss 0.01397695 \tTraining Acuuarcy 65.931% \tValidation Acuuarcy 65.422%\n","Epoch: 1042 \tTraining Loss: 0.01338666 \tValidation Loss 0.01376960 \tTraining Acuuarcy 66.199% \tValidation Acuuarcy 65.478%\n","Epoch: 1043 \tTraining Loss: 0.01339479 \tValidation Loss 0.01381307 \tTraining Acuuarcy 66.147% \tValidation Acuuarcy 66.314%\n","Epoch: 1044 \tTraining Loss: 0.01340093 \tValidation Loss 0.01373588 \tTraining Acuuarcy 66.025% \tValidation Acuuarcy 66.565%\n","Epoch: 1045 \tTraining Loss: 0.01338233 \tValidation Loss 0.01378409 \tTraining Acuuarcy 66.307% \tValidation Acuuarcy 65.255%\n","Epoch: 1046 \tTraining Loss: 0.01338166 \tValidation Loss 0.01375656 \tTraining Acuuarcy 66.321% \tValidation Acuuarcy 66.369%\n","Epoch: 1047 \tTraining Loss: 0.01342535 \tValidation Loss 0.01390133 \tTraining Acuuarcy 65.732% \tValidation Acuuarcy 66.314%\n","Epoch: 1048 \tTraining Loss: 0.01341943 \tValidation Loss 0.01374828 \tTraining Acuuarcy 65.885% \tValidation Acuuarcy 66.258%\n","Epoch: 1049 \tTraining Loss: 0.01338621 \tValidation Loss 0.01382387 \tTraining Acuuarcy 66.216% \tValidation Acuuarcy 66.453%\n","Epoch: 1050 \tTraining Loss: 0.01342954 \tValidation Loss 0.01377659 \tTraining Acuuarcy 65.621% \tValidation Acuuarcy 66.927%\n","Epoch: 1051 \tTraining Loss: 0.01339816 \tValidation Loss 0.01373113 \tTraining Acuuarcy 66.073% \tValidation Acuuarcy 66.537%\n","Epoch: 1052 \tTraining Loss: 0.01340627 \tValidation Loss 0.01372382 \tTraining Acuuarcy 65.997% \tValidation Acuuarcy 67.066%\n","Epoch: 1053 \tTraining Loss: 0.01340426 \tValidation Loss 0.01389646 \tTraining Acuuarcy 65.986% \tValidation Acuuarcy 66.565%\n","Epoch: 1054 \tTraining Loss: 0.01341008 \tValidation Loss 0.01386769 \tTraining Acuuarcy 65.938% \tValidation Acuuarcy 65.311%\n","Epoch: 1055 \tTraining Loss: 0.01336604 \tValidation Loss 0.01383257 \tTraining Acuuarcy 66.537% \tValidation Acuuarcy 66.787%\n","Epoch: 1056 \tTraining Loss: 0.01340115 \tValidation Loss 0.01375909 \tTraining Acuuarcy 66.035% \tValidation Acuuarcy 66.565%\n","Epoch: 1057 \tTraining Loss: 0.01341018 \tValidation Loss 0.01376491 \tTraining Acuuarcy 65.896% \tValidation Acuuarcy 67.038%\n","Epoch: 1058 \tTraining Loss: 0.01341308 \tValidation Loss 0.01388577 \tTraining Acuuarcy 65.774% \tValidation Acuuarcy 65.450%\n","Epoch: 1059 \tTraining Loss: 0.01341073 \tValidation Loss 0.01370273 \tTraining Acuuarcy 65.962% \tValidation Acuuarcy 66.202%\n","Epoch: 1060 \tTraining Loss: 0.01340827 \tValidation Loss 0.01376999 \tTraining Acuuarcy 65.972% \tValidation Acuuarcy 66.147%\n","Epoch: 1061 \tTraining Loss: 0.01338970 \tValidation Loss 0.01379501 \tTraining Acuuarcy 66.171% \tValidation Acuuarcy 66.230%\n","Epoch: 1062 \tTraining Loss: 0.01341326 \tValidation Loss 0.01377922 \tTraining Acuuarcy 65.896% \tValidation Acuuarcy 66.063%\n","Epoch: 1063 \tTraining Loss: 0.01338579 \tValidation Loss 0.01385445 \tTraining Acuuarcy 66.254% \tValidation Acuuarcy 66.202%\n","Epoch: 1064 \tTraining Loss: 0.01340485 \tValidation Loss 0.01373159 \tTraining Acuuarcy 66.018% \tValidation Acuuarcy 65.840%\n","Epoch: 1065 \tTraining Loss: 0.01342110 \tValidation Loss 0.01371520 \tTraining Acuuarcy 65.850% \tValidation Acuuarcy 67.010%\n","Epoch: 1066 \tTraining Loss: 0.01339853 \tValidation Loss 0.01383654 \tTraining Acuuarcy 66.059% \tValidation Acuuarcy 66.119%\n","Epoch: 1067 \tTraining Loss: 0.01341558 \tValidation Loss 0.01385824 \tTraining Acuuarcy 65.791% \tValidation Acuuarcy 65.673%\n","Epoch: 1068 \tTraining Loss: 0.01339878 \tValidation Loss 0.01375481 \tTraining Acuuarcy 66.091% \tValidation Acuuarcy 66.955%\n","Epoch: 1069 \tTraining Loss: 0.01338119 \tValidation Loss 0.01391892 \tTraining Acuuarcy 66.254% \tValidation Acuuarcy 65.673%\n","Epoch: 1070 \tTraining Loss: 0.01338667 \tValidation Loss 0.01372173 \tTraining Acuuarcy 66.206% \tValidation Acuuarcy 66.147%\n","Epoch: 1071 \tTraining Loss: 0.01339268 \tValidation Loss 0.01367072 \tTraining Acuuarcy 66.101% \tValidation Acuuarcy 66.787%\n","Epoch: 1072 \tTraining Loss: 0.01340946 \tValidation Loss 0.01390637 \tTraining Acuuarcy 65.962% \tValidation Acuuarcy 65.756%\n","Epoch: 1073 \tTraining Loss: 0.01339413 \tValidation Loss 0.01384617 \tTraining Acuuarcy 66.115% \tValidation Acuuarcy 66.899%\n","Epoch: 1074 \tTraining Loss: 0.01337095 \tValidation Loss 0.01362083 \tTraining Acuuarcy 66.390% \tValidation Acuuarcy 67.205%\n","Epoch: 1075 \tTraining Loss: 0.01339230 \tValidation Loss 0.01379045 \tTraining Acuuarcy 66.160% \tValidation Acuuarcy 66.537%\n","Epoch: 1076 \tTraining Loss: 0.01338372 \tValidation Loss 0.01376018 \tTraining Acuuarcy 66.300% \tValidation Acuuarcy 67.400%\n","Epoch: 1077 \tTraining Loss: 0.01339906 \tValidation Loss 0.01386238 \tTraining Acuuarcy 65.997% \tValidation Acuuarcy 66.007%\n","Epoch: 1078 \tTraining Loss: 0.01341244 \tValidation Loss 0.01366462 \tTraining Acuuarcy 66.000% \tValidation Acuuarcy 66.871%\n","Epoch: 1079 \tTraining Loss: 0.01339297 \tValidation Loss 0.01369963 \tTraining Acuuarcy 66.174% \tValidation Acuuarcy 66.425%\n","Epoch: 1080 \tTraining Loss: 0.01339025 \tValidation Loss 0.01384233 \tTraining Acuuarcy 66.209% \tValidation Acuuarcy 66.955%\n","Epoch: 1081 \tTraining Loss: 0.01340088 \tValidation Loss 0.01377030 \tTraining Acuuarcy 66.052% \tValidation Acuuarcy 66.174%\n","Epoch: 1082 \tTraining Loss: 0.01338181 \tValidation Loss 0.01390446 \tTraining Acuuarcy 66.300% \tValidation Acuuarcy 65.896%\n","Epoch: 1083 \tTraining Loss: 0.01337989 \tValidation Loss 0.01377818 \tTraining Acuuarcy 66.317% \tValidation Acuuarcy 66.119%\n","Epoch: 1084 \tTraining Loss: 0.01338900 \tValidation Loss 0.01398510 \tTraining Acuuarcy 66.188% \tValidation Acuuarcy 64.531%\n","Epoch: 1085 \tTraining Loss: 0.01338698 \tValidation Loss 0.01383882 \tTraining Acuuarcy 66.275% \tValidation Acuuarcy 66.704%\n","Epoch: 1086 \tTraining Loss: 0.01339950 \tValidation Loss 0.01377808 \tTraining Acuuarcy 66.063% \tValidation Acuuarcy 65.868%\n","Epoch: 1087 \tTraining Loss: 0.01337774 \tValidation Loss 0.01389029 \tTraining Acuuarcy 66.446% \tValidation Acuuarcy 66.592%\n","Epoch: 1088 \tTraining Loss: 0.01339594 \tValidation Loss 0.01376892 \tTraining Acuuarcy 66.066% \tValidation Acuuarcy 67.261%\n","Epoch: 1089 \tTraining Loss: 0.01339102 \tValidation Loss 0.01373916 \tTraining Acuuarcy 66.157% \tValidation Acuuarcy 66.787%\n","Epoch: 1090 \tTraining Loss: 0.01340280 \tValidation Loss 0.01369987 \tTraining Acuuarcy 65.986% \tValidation Acuuarcy 66.509%\n","Epoch: 1091 \tTraining Loss: 0.01338720 \tValidation Loss 0.01386661 \tTraining Acuuarcy 66.321% \tValidation Acuuarcy 66.286%\n","Epoch: 1092 \tTraining Loss: 0.01339967 \tValidation Loss 0.01379305 \tTraining Acuuarcy 65.983% \tValidation Acuuarcy 66.592%\n","Epoch: 1093 \tTraining Loss: 0.01340059 \tValidation Loss 0.01372126 \tTraining Acuuarcy 66.035% \tValidation Acuuarcy 66.843%\n","Epoch: 1094 \tTraining Loss: 0.01340069 \tValidation Loss 0.01376532 \tTraining Acuuarcy 65.993% \tValidation Acuuarcy 67.066%\n","Epoch: 1095 \tTraining Loss: 0.01341053 \tValidation Loss 0.01377299 \tTraining Acuuarcy 65.976% \tValidation Acuuarcy 66.815%\n","Epoch: 1096 \tTraining Loss: 0.01339320 \tValidation Loss 0.01393927 \tTraining Acuuarcy 66.119% \tValidation Acuuarcy 65.952%\n","Epoch: 1097 \tTraining Loss: 0.01340438 \tValidation Loss 0.01371136 \tTraining Acuuarcy 66.049% \tValidation Acuuarcy 66.843%\n","Epoch: 1098 \tTraining Loss: 0.01336725 \tValidation Loss 0.01384318 \tTraining Acuuarcy 66.446% \tValidation Acuuarcy 65.255%\n","Epoch: 1099 \tTraining Loss: 0.01339350 \tValidation Loss 0.01382852 \tTraining Acuuarcy 66.136% \tValidation Acuuarcy 66.202%\n","Epoch: 1100 \tTraining Loss: 0.01341282 \tValidation Loss 0.01385638 \tTraining Acuuarcy 65.854% \tValidation Acuuarcy 65.784%\n","Epoch: 1101 \tTraining Loss: 0.01339581 \tValidation Loss 0.01369316 \tTraining Acuuarcy 66.143% \tValidation Acuuarcy 67.233%\n","Epoch: 1102 \tTraining Loss: 0.01339214 \tValidation Loss 0.01376663 \tTraining Acuuarcy 66.181% \tValidation Acuuarcy 66.119%\n","Epoch: 1103 \tTraining Loss: 0.01340935 \tValidation Loss 0.01379039 \tTraining Acuuarcy 65.861% \tValidation Acuuarcy 65.812%\n","Epoch: 1104 \tTraining Loss: 0.01340454 \tValidation Loss 0.01379485 \tTraining Acuuarcy 66.066% \tValidation Acuuarcy 66.509%\n","Epoch: 1105 \tTraining Loss: 0.01337497 \tValidation Loss 0.01388132 \tTraining Acuuarcy 66.349% \tValidation Acuuarcy 64.642%\n","Epoch: 1106 \tTraining Loss: 0.01342097 \tValidation Loss 0.01374135 \tTraining Acuuarcy 65.722% \tValidation Acuuarcy 66.453%\n","Epoch: 1107 \tTraining Loss: 0.01337320 \tValidation Loss 0.01378313 \tTraining Acuuarcy 66.394% \tValidation Acuuarcy 65.979%\n","Epoch: 1108 \tTraining Loss: 0.01340492 \tValidation Loss 0.01394335 \tTraining Acuuarcy 65.931% \tValidation Acuuarcy 65.506%\n","Epoch: 1109 \tTraining Loss: 0.01338874 \tValidation Loss 0.01374269 \tTraining Acuuarcy 66.209% \tValidation Acuuarcy 66.676%\n","Epoch: 1110 \tTraining Loss: 0.01338408 \tValidation Loss 0.01379718 \tTraining Acuuarcy 66.293% \tValidation Acuuarcy 66.537%\n","Epoch: 1111 \tTraining Loss: 0.01337601 \tValidation Loss 0.01372738 \tTraining Acuuarcy 66.394% \tValidation Acuuarcy 67.317%\n","Epoch: 1112 \tTraining Loss: 0.01341064 \tValidation Loss 0.01384580 \tTraining Acuuarcy 65.990% \tValidation Acuuarcy 66.565%\n","Epoch: 1113 \tTraining Loss: 0.01339361 \tValidation Loss 0.01383190 \tTraining Acuuarcy 66.101% \tValidation Acuuarcy 66.843%\n","Epoch: 1114 \tTraining Loss: 0.01339308 \tValidation Loss 0.01374541 \tTraining Acuuarcy 66.202% \tValidation Acuuarcy 66.509%\n","Epoch: 1115 \tTraining Loss: 0.01341256 \tValidation Loss 0.01373126 \tTraining Acuuarcy 65.885% \tValidation Acuuarcy 66.760%\n","Epoch: 1116 \tTraining Loss: 0.01342769 \tValidation Loss 0.01386243 \tTraining Acuuarcy 65.701% \tValidation Acuuarcy 65.561%\n","Epoch: 1117 \tTraining Loss: 0.01340479 \tValidation Loss 0.01390482 \tTraining Acuuarcy 65.993% \tValidation Acuuarcy 65.116%\n","Epoch: 1118 \tTraining Loss: 0.01341260 \tValidation Loss 0.01371810 \tTraining Acuuarcy 65.854% \tValidation Acuuarcy 66.843%\n","Epoch: 1119 \tTraining Loss: 0.01340300 \tValidation Loss 0.01392597 \tTraining Acuuarcy 66.059% \tValidation Acuuarcy 65.561%\n","Epoch: 1120 \tTraining Loss: 0.01340907 \tValidation Loss 0.01369967 \tTraining Acuuarcy 65.965% \tValidation Acuuarcy 66.286%\n","Epoch: 1121 \tTraining Loss: 0.01339650 \tValidation Loss 0.01373546 \tTraining Acuuarcy 66.084% \tValidation Acuuarcy 66.425%\n","Epoch: 1122 \tTraining Loss: 0.01337215 \tValidation Loss 0.01375290 \tTraining Acuuarcy 66.394% \tValidation Acuuarcy 67.205%\n","Epoch: 1123 \tTraining Loss: 0.01338765 \tValidation Loss 0.01376360 \tTraining Acuuarcy 66.234% \tValidation Acuuarcy 66.258%\n","Epoch: 1124 \tTraining Loss: 0.01338290 \tValidation Loss 0.01375399 \tTraining Acuuarcy 66.251% \tValidation Acuuarcy 66.342%\n","Epoch: 1125 \tTraining Loss: 0.01338350 \tValidation Loss 0.01373086 \tTraining Acuuarcy 66.279% \tValidation Acuuarcy 66.760%\n","Epoch: 1126 \tTraining Loss: 0.01339383 \tValidation Loss 0.01382009 \tTraining Acuuarcy 66.066% \tValidation Acuuarcy 65.450%\n","Epoch: 1127 \tTraining Loss: 0.01341543 \tValidation Loss 0.01372321 \tTraining Acuuarcy 65.864% \tValidation Acuuarcy 67.400%\n","Epoch: 1128 \tTraining Loss: 0.01339983 \tValidation Loss 0.01385651 \tTraining Acuuarcy 65.997% \tValidation Acuuarcy 66.342%\n","Epoch: 1129 \tTraining Loss: 0.01340655 \tValidation Loss 0.01382203 \tTraining Acuuarcy 66.011% \tValidation Acuuarcy 66.286%\n","Epoch: 1130 \tTraining Loss: 0.01341365 \tValidation Loss 0.01373395 \tTraining Acuuarcy 65.847% \tValidation Acuuarcy 66.927%\n","Epoch: 1131 \tTraining Loss: 0.01341228 \tValidation Loss 0.01376719 \tTraining Acuuarcy 65.840% \tValidation Acuuarcy 66.147%\n","Epoch: 1132 \tTraining Loss: 0.01337905 \tValidation Loss 0.01377052 \tTraining Acuuarcy 66.335% \tValidation Acuuarcy 66.927%\n","Epoch: 1133 \tTraining Loss: 0.01341251 \tValidation Loss 0.01368999 \tTraining Acuuarcy 65.934% \tValidation Acuuarcy 67.400%\n","Epoch: 1134 \tTraining Loss: 0.01338886 \tValidation Loss 0.01383637 \tTraining Acuuarcy 66.181% \tValidation Acuuarcy 65.896%\n","Epoch: 1135 \tTraining Loss: 0.01337700 \tValidation Loss 0.01379158 \tTraining Acuuarcy 66.345% \tValidation Acuuarcy 65.701%\n","Epoch: 1136 \tTraining Loss: 0.01336692 \tValidation Loss 0.01377990 \tTraining Acuuarcy 66.491% \tValidation Acuuarcy 66.732%\n","Epoch: 1137 \tTraining Loss: 0.01338395 \tValidation Loss 0.01385405 \tTraining Acuuarcy 66.230% \tValidation Acuuarcy 65.673%\n","Epoch: 1138 \tTraining Loss: 0.01339948 \tValidation Loss 0.01375413 \tTraining Acuuarcy 66.049% \tValidation Acuuarcy 67.177%\n","Epoch: 1139 \tTraining Loss: 0.01340544 \tValidation Loss 0.01382154 \tTraining Acuuarcy 66.028% \tValidation Acuuarcy 66.927%\n","Epoch: 1140 \tTraining Loss: 0.01339098 \tValidation Loss 0.01375081 \tTraining Acuuarcy 66.244% \tValidation Acuuarcy 67.150%\n","Epoch: 1141 \tTraining Loss: 0.01342137 \tValidation Loss 0.01381275 \tTraining Acuuarcy 65.711% \tValidation Acuuarcy 66.314%\n","Epoch: 1142 \tTraining Loss: 0.01339325 \tValidation Loss 0.01374094 \tTraining Acuuarcy 66.157% \tValidation Acuuarcy 66.481%\n","Epoch: 1143 \tTraining Loss: 0.01338310 \tValidation Loss 0.01369374 \tTraining Acuuarcy 66.248% \tValidation Acuuarcy 67.066%\n","Epoch: 1144 \tTraining Loss: 0.01342030 \tValidation Loss 0.01371350 \tTraining Acuuarcy 65.788% \tValidation Acuuarcy 66.230%\n","Epoch: 1145 \tTraining Loss: 0.01338424 \tValidation Loss 0.01390751 \tTraining Acuuarcy 66.331% \tValidation Acuuarcy 65.617%\n","Epoch: 1146 \tTraining Loss: 0.01337747 \tValidation Loss 0.01368961 \tTraining Acuuarcy 66.331% \tValidation Acuuarcy 66.676%\n","Epoch: 1147 \tTraining Loss: 0.01336991 \tValidation Loss 0.01372263 \tTraining Acuuarcy 66.505% \tValidation Acuuarcy 66.843%\n","Epoch: 1148 \tTraining Loss: 0.01339666 \tValidation Loss 0.01381066 \tTraining Acuuarcy 66.035% \tValidation Acuuarcy 66.035%\n","Epoch: 1149 \tTraining Loss: 0.01337329 \tValidation Loss 0.01383243 \tTraining Acuuarcy 66.422% \tValidation Acuuarcy 66.704%\n","Epoch: 1150 \tTraining Loss: 0.01338505 \tValidation Loss 0.01376935 \tTraining Acuuarcy 66.282% \tValidation Acuuarcy 66.174%\n","Epoch: 1151 \tTraining Loss: 0.01340846 \tValidation Loss 0.01375863 \tTraining Acuuarcy 65.875% \tValidation Acuuarcy 66.314%\n","Epoch: 1152 \tTraining Loss: 0.01340037 \tValidation Loss 0.01367192 \tTraining Acuuarcy 66.039% \tValidation Acuuarcy 67.317%\n","Epoch: 1153 \tTraining Loss: 0.01339910 \tValidation Loss 0.01382352 \tTraining Acuuarcy 66.066% \tValidation Acuuarcy 66.147%\n","Epoch: 1154 \tTraining Loss: 0.01340586 \tValidation Loss 0.01388936 \tTraining Acuuarcy 66.035% \tValidation Acuuarcy 65.840%\n","Epoch: 1155 \tTraining Loss: 0.01338454 \tValidation Loss 0.01381821 \tTraining Acuuarcy 66.244% \tValidation Acuuarcy 65.812%\n","Epoch: 1156 \tTraining Loss: 0.01339025 \tValidation Loss 0.01384777 \tTraining Acuuarcy 66.206% \tValidation Acuuarcy 66.732%\n","Epoch: 1157 \tTraining Loss: 0.01339736 \tValidation Loss 0.01384830 \tTraining Acuuarcy 66.077% \tValidation Acuuarcy 65.952%\n","Epoch: 1158 \tTraining Loss: 0.01339085 \tValidation Loss 0.01382755 \tTraining Acuuarcy 66.140% \tValidation Acuuarcy 66.230%\n","Epoch: 1159 \tTraining Loss: 0.01337469 \tValidation Loss 0.01381826 \tTraining Acuuarcy 66.387% \tValidation Acuuarcy 66.369%\n","Epoch: 1160 \tTraining Loss: 0.01341587 \tValidation Loss 0.01379526 \tTraining Acuuarcy 65.795% \tValidation Acuuarcy 66.397%\n","Epoch: 1161 \tTraining Loss: 0.01336774 \tValidation Loss 0.01386182 \tTraining Acuuarcy 66.457% \tValidation Acuuarcy 65.617%\n","Epoch: 1162 \tTraining Loss: 0.01339605 \tValidation Loss 0.01375742 \tTraining Acuuarcy 66.094% \tValidation Acuuarcy 66.314%\n","Epoch: 1163 \tTraining Loss: 0.01338471 \tValidation Loss 0.01377167 \tTraining Acuuarcy 66.296% \tValidation Acuuarcy 66.899%\n","Epoch: 1164 \tTraining Loss: 0.01338245 \tValidation Loss 0.01387768 \tTraining Acuuarcy 66.195% \tValidation Acuuarcy 65.534%\n","Epoch: 1165 \tTraining Loss: 0.01339064 \tValidation Loss 0.01388179 \tTraining Acuuarcy 66.178% \tValidation Acuuarcy 66.147%\n","Epoch: 1166 \tTraining Loss: 0.01340124 \tValidation Loss 0.01374510 \tTraining Acuuarcy 66.028% \tValidation Acuuarcy 67.205%\n","Epoch: 1167 \tTraining Loss: 0.01340452 \tValidation Loss 0.01371841 \tTraining Acuuarcy 66.007% \tValidation Acuuarcy 66.230%\n","Epoch: 1168 \tTraining Loss: 0.01342584 \tValidation Loss 0.01374532 \tTraining Acuuarcy 65.711% \tValidation Acuuarcy 65.756%\n","Epoch: 1169 \tTraining Loss: 0.01340639 \tValidation Loss 0.01377409 \tTraining Acuuarcy 65.850% \tValidation Acuuarcy 65.979%\n","Epoch: 1170 \tTraining Loss: 0.01337773 \tValidation Loss 0.01389361 \tTraining Acuuarcy 66.380% \tValidation Acuuarcy 65.979%\n","Epoch: 1171 \tTraining Loss: 0.01338574 \tValidation Loss 0.01382688 \tTraining Acuuarcy 66.244% \tValidation Acuuarcy 66.592%\n","Epoch: 1172 \tTraining Loss: 0.01338404 \tValidation Loss 0.01381775 \tTraining Acuuarcy 66.286% \tValidation Acuuarcy 65.478%\n","Epoch: 1173 \tTraining Loss: 0.01337251 \tValidation Loss 0.01385519 \tTraining Acuuarcy 66.418% \tValidation Acuuarcy 65.784%\n","Epoch: 1174 \tTraining Loss: 0.01339273 \tValidation Loss 0.01383538 \tTraining Acuuarcy 66.147% \tValidation Acuuarcy 65.952%\n","Epoch: 1175 \tTraining Loss: 0.01338544 \tValidation Loss 0.01382159 \tTraining Acuuarcy 66.265% \tValidation Acuuarcy 65.394%\n","Epoch: 1176 \tTraining Loss: 0.01340348 \tValidation Loss 0.01377350 \tTraining Acuuarcy 65.955% \tValidation Acuuarcy 66.676%\n","Epoch: 1177 \tTraining Loss: 0.01335985 \tValidation Loss 0.01383313 \tTraining Acuuarcy 66.659% \tValidation Acuuarcy 66.509%\n","Epoch: 1178 \tTraining Loss: 0.01339135 \tValidation Loss 0.01371700 \tTraining Acuuarcy 66.167% \tValidation Acuuarcy 66.760%\n","Epoch: 1179 \tTraining Loss: 0.01339352 \tValidation Loss 0.01373390 \tTraining Acuuarcy 66.147% \tValidation Acuuarcy 66.620%\n","Epoch: 1180 \tTraining Loss: 0.01337471 \tValidation Loss 0.01382176 \tTraining Acuuarcy 66.425% \tValidation Acuuarcy 66.147%\n","Epoch: 1181 \tTraining Loss: 0.01338529 \tValidation Loss 0.01374530 \tTraining Acuuarcy 66.160% \tValidation Acuuarcy 66.425%\n","Epoch: 1182 \tTraining Loss: 0.01341058 \tValidation Loss 0.01383061 \tTraining Acuuarcy 65.889% \tValidation Acuuarcy 66.787%\n","Epoch: 1183 \tTraining Loss: 0.01339204 \tValidation Loss 0.01372360 \tTraining Acuuarcy 66.119% \tValidation Acuuarcy 66.899%\n","Epoch: 1184 \tTraining Loss: 0.01339923 \tValidation Loss 0.01377736 \tTraining Acuuarcy 66.035% \tValidation Acuuarcy 67.010%\n","Epoch: 1185 \tTraining Loss: 0.01342199 \tValidation Loss 0.01371852 \tTraining Acuuarcy 65.732% \tValidation Acuuarcy 67.066%\n","Epoch: 1186 \tTraining Loss: 0.01338928 \tValidation Loss 0.01385108 \tTraining Acuuarcy 66.167% \tValidation Acuuarcy 65.812%\n","Epoch: 1187 \tTraining Loss: 0.01336689 \tValidation Loss 0.01389585 \tTraining Acuuarcy 66.422% \tValidation Acuuarcy 66.732%\n","Epoch: 1188 \tTraining Loss: 0.01338833 \tValidation Loss 0.01382164 \tTraining Acuuarcy 66.140% \tValidation Acuuarcy 66.899%\n","Epoch: 1189 \tTraining Loss: 0.01337935 \tValidation Loss 0.01376140 \tTraining Acuuarcy 66.237% \tValidation Acuuarcy 66.342%\n","Epoch: 1190 \tTraining Loss: 0.01337690 \tValidation Loss 0.01368772 \tTraining Acuuarcy 66.355% \tValidation Acuuarcy 66.509%\n","Epoch: 1191 \tTraining Loss: 0.01340737 \tValidation Loss 0.01380859 \tTraining Acuuarcy 65.986% \tValidation Acuuarcy 66.342%\n","Epoch: 1192 \tTraining Loss: 0.01339867 \tValidation Loss 0.01387289 \tTraining Acuuarcy 66.108% \tValidation Acuuarcy 66.147%\n","Epoch: 1193 \tTraining Loss: 0.01339133 \tValidation Loss 0.01384355 \tTraining Acuuarcy 66.133% \tValidation Acuuarcy 66.676%\n","Epoch: 1194 \tTraining Loss: 0.01338004 \tValidation Loss 0.01382496 \tTraining Acuuarcy 66.286% \tValidation Acuuarcy 65.394%\n","Epoch: 1195 \tTraining Loss: 0.01341940 \tValidation Loss 0.01373167 \tTraining Acuuarcy 65.725% \tValidation Acuuarcy 67.568%\n","Epoch: 1196 \tTraining Loss: 0.01337877 \tValidation Loss 0.01382504 \tTraining Acuuarcy 66.324% \tValidation Acuuarcy 66.815%\n","Epoch: 1197 \tTraining Loss: 0.01338211 \tValidation Loss 0.01381843 \tTraining Acuuarcy 66.328% \tValidation Acuuarcy 66.509%\n","Epoch: 1198 \tTraining Loss: 0.01337569 \tValidation Loss 0.01369242 \tTraining Acuuarcy 66.338% \tValidation Acuuarcy 67.122%\n","Epoch: 1199 \tTraining Loss: 0.01339477 \tValidation Loss 0.01381633 \tTraining Acuuarcy 66.063% \tValidation Acuuarcy 66.676%\n","Epoch: 1200 \tTraining Loss: 0.01340755 \tValidation Loss 0.01381367 \tTraining Acuuarcy 65.924% \tValidation Acuuarcy 66.342%\n","Epoch: 1201 \tTraining Loss: 0.01339664 \tValidation Loss 0.01376727 \tTraining Acuuarcy 66.073% \tValidation Acuuarcy 66.147%\n","Epoch: 1202 \tTraining Loss: 0.01340352 \tValidation Loss 0.01380478 \tTraining Acuuarcy 65.976% \tValidation Acuuarcy 66.760%\n","Epoch: 1203 \tTraining Loss: 0.01338838 \tValidation Loss 0.01375137 \tTraining Acuuarcy 66.178% \tValidation Acuuarcy 66.369%\n","Epoch: 1204 \tTraining Loss: 0.01337486 \tValidation Loss 0.01381171 \tTraining Acuuarcy 66.394% \tValidation Acuuarcy 66.258%\n","Epoch: 1205 \tTraining Loss: 0.01339178 \tValidation Loss 0.01372023 \tTraining Acuuarcy 66.174% \tValidation Acuuarcy 66.397%\n","Epoch: 1206 \tTraining Loss: 0.01340025 \tValidation Loss 0.01383633 \tTraining Acuuarcy 65.997% \tValidation Acuuarcy 65.756%\n","Epoch: 1207 \tTraining Loss: 0.01338435 \tValidation Loss 0.01364943 \tTraining Acuuarcy 66.272% \tValidation Acuuarcy 67.150%\n","Epoch: 1208 \tTraining Loss: 0.01339166 \tValidation Loss 0.01373937 \tTraining Acuuarcy 66.122% \tValidation Acuuarcy 66.119%\n","Epoch: 1209 \tTraining Loss: 0.01338502 \tValidation Loss 0.01381535 \tTraining Acuuarcy 66.303% \tValidation Acuuarcy 65.952%\n","Epoch: 1210 \tTraining Loss: 0.01339181 \tValidation Loss 0.01373910 \tTraining Acuuarcy 66.112% \tValidation Acuuarcy 67.345%\n","Epoch: 1211 \tTraining Loss: 0.01335602 \tValidation Loss 0.01378670 \tTraining Acuuarcy 66.641% \tValidation Acuuarcy 65.952%\n","Epoch: 1212 \tTraining Loss: 0.01337841 \tValidation Loss 0.01368199 \tTraining Acuuarcy 66.314% \tValidation Acuuarcy 67.122%\n","Epoch: 1213 \tTraining Loss: 0.01340297 \tValidation Loss 0.01380789 \tTraining Acuuarcy 66.056% \tValidation Acuuarcy 66.314%\n","Epoch: 1214 \tTraining Loss: 0.01338033 \tValidation Loss 0.01380174 \tTraining Acuuarcy 66.310% \tValidation Acuuarcy 66.369%\n","Epoch: 1215 \tTraining Loss: 0.01339744 \tValidation Loss 0.01379916 \tTraining Acuuarcy 66.094% \tValidation Acuuarcy 67.205%\n","Epoch: 1216 \tTraining Loss: 0.01337806 \tValidation Loss 0.01374584 \tTraining Acuuarcy 66.331% \tValidation Acuuarcy 66.063%\n","Epoch: 1217 \tTraining Loss: 0.01338013 \tValidation Loss 0.01375717 \tTraining Acuuarcy 66.296% \tValidation Acuuarcy 66.704%\n","Epoch: 1218 \tTraining Loss: 0.01337334 \tValidation Loss 0.01384922 \tTraining Acuuarcy 66.439% \tValidation Acuuarcy 66.397%\n","Epoch: 1219 \tTraining Loss: 0.01339720 \tValidation Loss 0.01381200 \tTraining Acuuarcy 66.094% \tValidation Acuuarcy 66.314%\n","Epoch: 1220 \tTraining Loss: 0.01339055 \tValidation Loss 0.01363438 \tTraining Acuuarcy 66.129% \tValidation Acuuarcy 67.038%\n","Epoch: 1221 \tTraining Loss: 0.01334364 \tValidation Loss 0.01375899 \tTraining Acuuarcy 66.753% \tValidation Acuuarcy 66.871%\n","Epoch: 1222 \tTraining Loss: 0.01338243 \tValidation Loss 0.01383772 \tTraining Acuuarcy 66.331% \tValidation Acuuarcy 65.924%\n","Epoch: 1223 \tTraining Loss: 0.01339696 \tValidation Loss 0.01367488 \tTraining Acuuarcy 66.115% \tValidation Acuuarcy 67.595%\n","Epoch: 1224 \tTraining Loss: 0.01339868 \tValidation Loss 0.01382639 \tTraining Acuuarcy 66.136% \tValidation Acuuarcy 66.286%\n","Epoch: 1225 \tTraining Loss: 0.01338725 \tValidation Loss 0.01385934 \tTraining Acuuarcy 66.258% \tValidation Acuuarcy 66.787%\n","Epoch: 1226 \tTraining Loss: 0.01337575 \tValidation Loss 0.01379619 \tTraining Acuuarcy 66.352% \tValidation Acuuarcy 65.812%\n","Epoch: 1227 \tTraining Loss: 0.01335013 \tValidation Loss 0.01382832 \tTraining Acuuarcy 66.669% \tValidation Acuuarcy 66.063%\n","Epoch: 1228 \tTraining Loss: 0.01336967 \tValidation Loss 0.01378413 \tTraining Acuuarcy 66.443% \tValidation Acuuarcy 65.812%\n","Epoch: 1229 \tTraining Loss: 0.01338037 \tValidation Loss 0.01381585 \tTraining Acuuarcy 66.324% \tValidation Acuuarcy 66.147%\n","Epoch: 1230 \tTraining Loss: 0.01338873 \tValidation Loss 0.01374454 \tTraining Acuuarcy 66.209% \tValidation Acuuarcy 66.314%\n","Epoch: 1231 \tTraining Loss: 0.01338318 \tValidation Loss 0.01374754 \tTraining Acuuarcy 66.234% \tValidation Acuuarcy 66.425%\n","Epoch: 1232 \tTraining Loss: 0.01339389 \tValidation Loss 0.01382348 \tTraining Acuuarcy 66.070% \tValidation Acuuarcy 65.422%\n","Epoch: 1233 \tTraining Loss: 0.01340042 \tValidation Loss 0.01371667 \tTraining Acuuarcy 65.924% \tValidation Acuuarcy 66.927%\n","Epoch: 1234 \tTraining Loss: 0.01339470 \tValidation Loss 0.01381816 \tTraining Acuuarcy 66.129% \tValidation Acuuarcy 66.202%\n","Epoch: 1235 \tTraining Loss: 0.01337973 \tValidation Loss 0.01378586 \tTraining Acuuarcy 66.314% \tValidation Acuuarcy 66.704%\n","Epoch: 1236 \tTraining Loss: 0.01338987 \tValidation Loss 0.01383156 \tTraining Acuuarcy 66.185% \tValidation Acuuarcy 65.952%\n","Epoch: 1237 \tTraining Loss: 0.01338294 \tValidation Loss 0.01387841 \tTraining Acuuarcy 66.275% \tValidation Acuuarcy 66.592%\n","Epoch: 1238 \tTraining Loss: 0.01340701 \tValidation Loss 0.01383445 \tTraining Acuuarcy 65.938% \tValidation Acuuarcy 66.648%\n","Epoch: 1239 \tTraining Loss: 0.01340115 \tValidation Loss 0.01372569 \tTraining Acuuarcy 65.997% \tValidation Acuuarcy 66.592%\n","Epoch: 1240 \tTraining Loss: 0.01338069 \tValidation Loss 0.01367975 \tTraining Acuuarcy 66.328% \tValidation Acuuarcy 67.595%\n","Epoch: 1241 \tTraining Loss: 0.01336887 \tValidation Loss 0.01361445 \tTraining Acuuarcy 66.383% \tValidation Acuuarcy 67.568%\n","Epoch: 1242 \tTraining Loss: 0.01338196 \tValidation Loss 0.01375053 \tTraining Acuuarcy 66.279% \tValidation Acuuarcy 66.286%\n","Epoch: 1243 \tTraining Loss: 0.01337967 \tValidation Loss 0.01369531 \tTraining Acuuarcy 66.272% \tValidation Acuuarcy 67.177%\n","Epoch: 1244 \tTraining Loss: 0.01337948 \tValidation Loss 0.01380345 \tTraining Acuuarcy 66.380% \tValidation Acuuarcy 65.673%\n","Epoch: 1245 \tTraining Loss: 0.01335679 \tValidation Loss 0.01371694 \tTraining Acuuarcy 66.585% \tValidation Acuuarcy 66.843%\n","Epoch: 1246 \tTraining Loss: 0.01337495 \tValidation Loss 0.01382764 \tTraining Acuuarcy 66.408% \tValidation Acuuarcy 66.899%\n","Epoch: 1247 \tTraining Loss: 0.01336207 \tValidation Loss 0.01379104 \tTraining Acuuarcy 66.502% \tValidation Acuuarcy 66.565%\n","Epoch: 1248 \tTraining Loss: 0.01340183 \tValidation Loss 0.01393296 \tTraining Acuuarcy 66.052% \tValidation Acuuarcy 65.394%\n","Epoch: 1249 \tTraining Loss: 0.01338552 \tValidation Loss 0.01381081 \tTraining Acuuarcy 66.275% \tValidation Acuuarcy 66.314%\n","Epoch: 1250 \tTraining Loss: 0.01338658 \tValidation Loss 0.01375998 \tTraining Acuuarcy 66.174% \tValidation Acuuarcy 66.425%\n","Epoch: 1251 \tTraining Loss: 0.01339074 \tValidation Loss 0.01367094 \tTraining Acuuarcy 66.150% \tValidation Acuuarcy 66.899%\n","Epoch: 1252 \tTraining Loss: 0.01339946 \tValidation Loss 0.01379680 \tTraining Acuuarcy 66.077% \tValidation Acuuarcy 66.565%\n","Epoch: 1253 \tTraining Loss: 0.01339110 \tValidation Loss 0.01384379 \tTraining Acuuarcy 66.178% \tValidation Acuuarcy 65.617%\n","Epoch: 1254 \tTraining Loss: 0.01336766 \tValidation Loss 0.01381977 \tTraining Acuuarcy 66.491% \tValidation Acuuarcy 66.147%\n","Epoch: 1255 \tTraining Loss: 0.01339923 \tValidation Loss 0.01378629 \tTraining Acuuarcy 66.032% \tValidation Acuuarcy 65.840%\n","Epoch: 1256 \tTraining Loss: 0.01338462 \tValidation Loss 0.01368874 \tTraining Acuuarcy 66.122% \tValidation Acuuarcy 67.289%\n","Epoch: 1257 \tTraining Loss: 0.01338767 \tValidation Loss 0.01376997 \tTraining Acuuarcy 66.181% \tValidation Acuuarcy 66.927%\n","Epoch: 1258 \tTraining Loss: 0.01338414 \tValidation Loss 0.01366401 \tTraining Acuuarcy 66.237% \tValidation Acuuarcy 67.066%\n","Epoch: 1259 \tTraining Loss: 0.01336979 \tValidation Loss 0.01384089 \tTraining Acuuarcy 66.418% \tValidation Acuuarcy 66.453%\n","Epoch: 1260 \tTraining Loss: 0.01341095 \tValidation Loss 0.01377665 \tTraining Acuuarcy 65.836% \tValidation Acuuarcy 66.314%\n","Epoch: 1261 \tTraining Loss: 0.01337108 \tValidation Loss 0.01383480 \tTraining Acuuarcy 66.439% \tValidation Acuuarcy 66.091%\n","Epoch: 1262 \tTraining Loss: 0.01339014 \tValidation Loss 0.01366398 \tTraining Acuuarcy 66.119% \tValidation Acuuarcy 66.732%\n","Epoch: 1263 \tTraining Loss: 0.01340355 \tValidation Loss 0.01379157 \tTraining Acuuarcy 66.091% \tValidation Acuuarcy 65.756%\n","Epoch: 1264 \tTraining Loss: 0.01337711 \tValidation Loss 0.01378243 \tTraining Acuuarcy 66.289% \tValidation Acuuarcy 65.840%\n","Epoch: 1265 \tTraining Loss: 0.01335436 \tValidation Loss 0.01376346 \tTraining Acuuarcy 66.641% \tValidation Acuuarcy 66.899%\n","Epoch: 1266 \tTraining Loss: 0.01337944 \tValidation Loss 0.01375499 \tTraining Acuuarcy 66.296% \tValidation Acuuarcy 66.955%\n","Epoch: 1267 \tTraining Loss: 0.01339407 \tValidation Loss 0.01376214 \tTraining Acuuarcy 66.094% \tValidation Acuuarcy 66.342%\n","Epoch: 1268 \tTraining Loss: 0.01338273 \tValidation Loss 0.01390713 \tTraining Acuuarcy 66.258% \tValidation Acuuarcy 66.425%\n","Epoch: 1269 \tTraining Loss: 0.01337787 \tValidation Loss 0.01393046 \tTraining Acuuarcy 66.369% \tValidation Acuuarcy 66.147%\n","Epoch: 1270 \tTraining Loss: 0.01342757 \tValidation Loss 0.01382425 \tTraining Acuuarcy 65.722% \tValidation Acuuarcy 66.258%\n","Epoch: 1271 \tTraining Loss: 0.01338482 \tValidation Loss 0.01377534 \tTraining Acuuarcy 66.254% \tValidation Acuuarcy 66.091%\n","Epoch: 1272 \tTraining Loss: 0.01338143 \tValidation Loss 0.01384013 \tTraining Acuuarcy 66.286% \tValidation Acuuarcy 65.729%\n","Epoch: 1273 \tTraining Loss: 0.01337774 \tValidation Loss 0.01379103 \tTraining Acuuarcy 66.359% \tValidation Acuuarcy 66.174%\n","Epoch: 1274 \tTraining Loss: 0.01336749 \tValidation Loss 0.01377228 \tTraining Acuuarcy 66.523% \tValidation Acuuarcy 66.787%\n","Epoch: 1275 \tTraining Loss: 0.01336964 \tValidation Loss 0.01370233 \tTraining Acuuarcy 66.415% \tValidation Acuuarcy 66.509%\n","Epoch: 1276 \tTraining Loss: 0.01340111 \tValidation Loss 0.01359697 \tTraining Acuuarcy 65.969% \tValidation Acuuarcy 67.679%\n","Epoch: 1277 \tTraining Loss: 0.01341433 \tValidation Loss 0.01368790 \tTraining Acuuarcy 65.830% \tValidation Acuuarcy 67.317%\n","Epoch: 1278 \tTraining Loss: 0.01336926 \tValidation Loss 0.01378034 \tTraining Acuuarcy 66.540% \tValidation Acuuarcy 66.620%\n","Epoch: 1279 \tTraining Loss: 0.01337850 \tValidation Loss 0.01385380 \tTraining Acuuarcy 66.352% \tValidation Acuuarcy 65.673%\n","Epoch: 1280 \tTraining Loss: 0.01338433 \tValidation Loss 0.01385046 \tTraining Acuuarcy 66.178% \tValidation Acuuarcy 66.397%\n","Epoch: 1281 \tTraining Loss: 0.01338791 \tValidation Loss 0.01384962 \tTraining Acuuarcy 66.223% \tValidation Acuuarcy 65.896%\n","Epoch: 1282 \tTraining Loss: 0.01336122 \tValidation Loss 0.01378443 \tTraining Acuuarcy 66.613% \tValidation Acuuarcy 65.896%\n","Epoch: 1283 \tTraining Loss: 0.01339986 \tValidation Loss 0.01378920 \tTraining Acuuarcy 66.126% \tValidation Acuuarcy 66.537%\n","Epoch: 1284 \tTraining Loss: 0.01338905 \tValidation Loss 0.01383727 \tTraining Acuuarcy 66.244% \tValidation Acuuarcy 66.063%\n","Epoch: 1285 \tTraining Loss: 0.01337574 \tValidation Loss 0.01375584 \tTraining Acuuarcy 66.335% \tValidation Acuuarcy 67.094%\n","Epoch: 1286 \tTraining Loss: 0.01340193 \tValidation Loss 0.01369564 \tTraining Acuuarcy 65.993% \tValidation Acuuarcy 66.927%\n","Epoch: 1287 \tTraining Loss: 0.01340184 \tValidation Loss 0.01374968 \tTraining Acuuarcy 66.035% \tValidation Acuuarcy 66.592%\n","Epoch: 1288 \tTraining Loss: 0.01338788 \tValidation Loss 0.01367150 \tTraining Acuuarcy 66.213% \tValidation Acuuarcy 67.484%\n","Epoch: 1289 \tTraining Loss: 0.01339646 \tValidation Loss 0.01371357 \tTraining Acuuarcy 66.091% \tValidation Acuuarcy 66.258%\n","Epoch: 1290 \tTraining Loss: 0.01339088 \tValidation Loss 0.01380704 \tTraining Acuuarcy 66.164% \tValidation Acuuarcy 66.369%\n","Epoch: 1291 \tTraining Loss: 0.01335963 \tValidation Loss 0.01378182 \tTraining Acuuarcy 66.554% \tValidation Acuuarcy 66.314%\n","Epoch: 1292 \tTraining Loss: 0.01337761 \tValidation Loss 0.01372363 \tTraining Acuuarcy 66.383% \tValidation Acuuarcy 66.787%\n","Epoch: 1293 \tTraining Loss: 0.01340993 \tValidation Loss 0.01370666 \tTraining Acuuarcy 65.983% \tValidation Acuuarcy 67.038%\n","Epoch: 1294 \tTraining Loss: 0.01337679 \tValidation Loss 0.01381487 \tTraining Acuuarcy 66.279% \tValidation Acuuarcy 66.481%\n","Epoch: 1295 \tTraining Loss: 0.01338151 \tValidation Loss 0.01386888 \tTraining Acuuarcy 66.342% \tValidation Acuuarcy 66.091%\n","Epoch: 1296 \tTraining Loss: 0.01338714 \tValidation Loss 0.01382759 \tTraining Acuuarcy 66.153% \tValidation Acuuarcy 66.648%\n","Epoch: 1297 \tTraining Loss: 0.01339512 \tValidation Loss 0.01382379 \tTraining Acuuarcy 65.951% \tValidation Acuuarcy 66.676%\n","Epoch: 1298 \tTraining Loss: 0.01340261 \tValidation Loss 0.01387179 \tTraining Acuuarcy 65.955% \tValidation Acuuarcy 66.063%\n","Epoch: 1299 \tTraining Loss: 0.01338230 \tValidation Loss 0.01373401 \tTraining Acuuarcy 66.254% \tValidation Acuuarcy 67.373%\n","Epoch: 1300 \tTraining Loss: 0.01338883 \tValidation Loss 0.01379310 \tTraining Acuuarcy 66.150% \tValidation Acuuarcy 66.425%\n","Epoch: 1301 \tTraining Loss: 0.01335970 \tValidation Loss 0.01380671 \tTraining Acuuarcy 66.526% \tValidation Acuuarcy 66.425%\n","Epoch: 1302 \tTraining Loss: 0.01338182 \tValidation Loss 0.01373395 \tTraining Acuuarcy 66.401% \tValidation Acuuarcy 66.369%\n","Epoch: 1303 \tTraining Loss: 0.01337678 \tValidation Loss 0.01364612 \tTraining Acuuarcy 66.258% \tValidation Acuuarcy 67.010%\n","Epoch: 1304 \tTraining Loss: 0.01336715 \tValidation Loss 0.01378375 \tTraining Acuuarcy 66.509% \tValidation Acuuarcy 66.091%\n","Epoch: 1305 \tTraining Loss: 0.01337539 \tValidation Loss 0.01383249 \tTraining Acuuarcy 66.345% \tValidation Acuuarcy 66.481%\n","Epoch: 1306 \tTraining Loss: 0.01333875 \tValidation Loss 0.01382062 \tTraining Acuuarcy 66.854% \tValidation Acuuarcy 66.174%\n","Epoch: 1307 \tTraining Loss: 0.01338849 \tValidation Loss 0.01373964 \tTraining Acuuarcy 66.174% \tValidation Acuuarcy 67.038%\n","Epoch: 1308 \tTraining Loss: 0.01337408 \tValidation Loss 0.01380221 \tTraining Acuuarcy 66.418% \tValidation Acuuarcy 66.230%\n","Epoch: 1309 \tTraining Loss: 0.01336893 \tValidation Loss 0.01368774 \tTraining Acuuarcy 66.467% \tValidation Acuuarcy 67.205%\n","Epoch: 1310 \tTraining Loss: 0.01338521 \tValidation Loss 0.01380868 \tTraining Acuuarcy 66.282% \tValidation Acuuarcy 67.066%\n","Epoch: 1311 \tTraining Loss: 0.01336650 \tValidation Loss 0.01375893 \tTraining Acuuarcy 66.498% \tValidation Acuuarcy 66.286%\n","Epoch: 1312 \tTraining Loss: 0.01338166 \tValidation Loss 0.01380907 \tTraining Acuuarcy 66.254% \tValidation Acuuarcy 67.177%\n","Epoch: 1313 \tTraining Loss: 0.01336193 \tValidation Loss 0.01371013 \tTraining Acuuarcy 66.523% \tValidation Acuuarcy 67.261%\n","Epoch: 1314 \tTraining Loss: 0.01336952 \tValidation Loss 0.01384293 \tTraining Acuuarcy 66.411% \tValidation Acuuarcy 66.927%\n","Epoch: 1315 \tTraining Loss: 0.01335857 \tValidation Loss 0.01378412 \tTraining Acuuarcy 66.551% \tValidation Acuuarcy 66.007%\n","Epoch: 1316 \tTraining Loss: 0.01339019 \tValidation Loss 0.01370251 \tTraining Acuuarcy 66.140% \tValidation Acuuarcy 67.010%\n","Epoch: 1317 \tTraining Loss: 0.01339469 \tValidation Loss 0.01383993 \tTraining Acuuarcy 66.052% \tValidation Acuuarcy 66.592%\n","Epoch: 1318 \tTraining Loss: 0.01338189 \tValidation Loss 0.01374964 \tTraining Acuuarcy 66.223% \tValidation Acuuarcy 66.425%\n","Epoch: 1319 \tTraining Loss: 0.01339150 \tValidation Loss 0.01384686 \tTraining Acuuarcy 66.237% \tValidation Acuuarcy 66.592%\n","Epoch: 1320 \tTraining Loss: 0.01340499 \tValidation Loss 0.01381161 \tTraining Acuuarcy 66.045% \tValidation Acuuarcy 66.815%\n","Epoch: 1321 \tTraining Loss: 0.01338720 \tValidation Loss 0.01386351 \tTraining Acuuarcy 66.174% \tValidation Acuuarcy 65.840%\n","Epoch: 1322 \tTraining Loss: 0.01337613 \tValidation Loss 0.01382140 \tTraining Acuuarcy 66.383% \tValidation Acuuarcy 66.063%\n","Epoch: 1323 \tTraining Loss: 0.01338565 \tValidation Loss 0.01374099 \tTraining Acuuarcy 66.216% \tValidation Acuuarcy 66.565%\n","Epoch: 1324 \tTraining Loss: 0.01338987 \tValidation Loss 0.01391234 \tTraining Acuuarcy 66.167% \tValidation Acuuarcy 66.174%\n","Epoch: 1325 \tTraining Loss: 0.01339321 \tValidation Loss 0.01373038 \tTraining Acuuarcy 66.056% \tValidation Acuuarcy 66.091%\n","Epoch: 1326 \tTraining Loss: 0.01340059 \tValidation Loss 0.01384504 \tTraining Acuuarcy 66.039% \tValidation Acuuarcy 66.537%\n","Epoch: 1327 \tTraining Loss: 0.01337231 \tValidation Loss 0.01373456 \tTraining Acuuarcy 66.324% \tValidation Acuuarcy 66.007%\n","Epoch: 1328 \tTraining Loss: 0.01337926 \tValidation Loss 0.01380978 \tTraining Acuuarcy 66.349% \tValidation Acuuarcy 67.205%\n","Epoch: 1329 \tTraining Loss: 0.01338219 \tValidation Loss 0.01378543 \tTraining Acuuarcy 66.272% \tValidation Acuuarcy 66.648%\n","Epoch: 1330 \tTraining Loss: 0.01338929 \tValidation Loss 0.01368724 \tTraining Acuuarcy 66.178% \tValidation Acuuarcy 67.484%\n","Epoch: 1331 \tTraining Loss: 0.01337549 \tValidation Loss 0.01371777 \tTraining Acuuarcy 66.359% \tValidation Acuuarcy 66.704%\n","Epoch: 1332 \tTraining Loss: 0.01337489 \tValidation Loss 0.01382555 \tTraining Acuuarcy 66.303% \tValidation Acuuarcy 66.676%\n","Epoch: 1333 \tTraining Loss: 0.01338010 \tValidation Loss 0.01361745 \tTraining Acuuarcy 66.230% \tValidation Acuuarcy 67.289%\n","Epoch: 1334 \tTraining Loss: 0.01337041 \tValidation Loss 0.01371129 \tTraining Acuuarcy 66.467% \tValidation Acuuarcy 66.899%\n","Epoch: 1335 \tTraining Loss: 0.01338738 \tValidation Loss 0.01387539 \tTraining Acuuarcy 66.251% \tValidation Acuuarcy 65.868%\n","Epoch: 1336 \tTraining Loss: 0.01337505 \tValidation Loss 0.01385032 \tTraining Acuuarcy 66.355% \tValidation Acuuarcy 65.812%\n","Epoch: 1337 \tTraining Loss: 0.01337840 \tValidation Loss 0.01384901 \tTraining Acuuarcy 66.394% \tValidation Acuuarcy 66.425%\n","Epoch: 1338 \tTraining Loss: 0.01335872 \tValidation Loss 0.01370444 \tTraining Acuuarcy 66.620% \tValidation Acuuarcy 67.038%\n","Epoch: 1339 \tTraining Loss: 0.01338368 \tValidation Loss 0.01374093 \tTraining Acuuarcy 66.234% \tValidation Acuuarcy 66.620%\n","Epoch: 1340 \tTraining Loss: 0.01338646 \tValidation Loss 0.01382656 \tTraining Acuuarcy 66.188% \tValidation Acuuarcy 67.010%\n","Epoch: 1341 \tTraining Loss: 0.01337830 \tValidation Loss 0.01369953 \tTraining Acuuarcy 66.307% \tValidation Acuuarcy 67.707%\n","Epoch: 1342 \tTraining Loss: 0.01337163 \tValidation Loss 0.01372665 \tTraining Acuuarcy 66.380% \tValidation Acuuarcy 67.400%\n","Epoch: 1343 \tTraining Loss: 0.01339566 \tValidation Loss 0.01382019 \tTraining Acuuarcy 66.133% \tValidation Acuuarcy 66.509%\n","Epoch: 1344 \tTraining Loss: 0.01339116 \tValidation Loss 0.01384360 \tTraining Acuuarcy 66.244% \tValidation Acuuarcy 65.952%\n","Epoch: 1345 \tTraining Loss: 0.01339129 \tValidation Loss 0.01360503 \tTraining Acuuarcy 66.181% \tValidation Acuuarcy 67.623%\n","Epoch: 1346 \tTraining Loss: 0.01337749 \tValidation Loss 0.01381182 \tTraining Acuuarcy 66.251% \tValidation Acuuarcy 67.038%\n","Epoch: 1347 \tTraining Loss: 0.01338868 \tValidation Loss 0.01377909 \tTraining Acuuarcy 66.178% \tValidation Acuuarcy 66.843%\n","Epoch: 1348 \tTraining Loss: 0.01338482 \tValidation Loss 0.01385591 \tTraining Acuuarcy 66.342% \tValidation Acuuarcy 65.478%\n","Epoch: 1349 \tTraining Loss: 0.01336872 \tValidation Loss 0.01365501 \tTraining Acuuarcy 66.439% \tValidation Acuuarcy 67.735%\n","Epoch: 1350 \tTraining Loss: 0.01338441 \tValidation Loss 0.01385229 \tTraining Acuuarcy 66.216% \tValidation Acuuarcy 66.620%\n","Epoch: 1351 \tTraining Loss: 0.01339025 \tValidation Loss 0.01386743 \tTraining Acuuarcy 66.140% \tValidation Acuuarcy 65.589%\n","Epoch: 1352 \tTraining Loss: 0.01339950 \tValidation Loss 0.01377407 \tTraining Acuuarcy 66.021% \tValidation Acuuarcy 66.174%\n","Epoch: 1353 \tTraining Loss: 0.01334240 \tValidation Loss 0.01374215 \tTraining Acuuarcy 66.760% \tValidation Acuuarcy 66.732%\n","Epoch: 1354 \tTraining Loss: 0.01337356 \tValidation Loss 0.01384363 \tTraining Acuuarcy 66.359% \tValidation Acuuarcy 66.035%\n","Epoch: 1355 \tTraining Loss: 0.01338768 \tValidation Loss 0.01374378 \tTraining Acuuarcy 66.206% \tValidation Acuuarcy 66.342%\n","Epoch: 1356 \tTraining Loss: 0.01334891 \tValidation Loss 0.01389794 \tTraining Acuuarcy 66.638% \tValidation Acuuarcy 66.397%\n","Epoch: 1357 \tTraining Loss: 0.01339864 \tValidation Loss 0.01370673 \tTraining Acuuarcy 66.042% \tValidation Acuuarcy 66.397%\n","Epoch: 1358 \tTraining Loss: 0.01336106 \tValidation Loss 0.01371920 \tTraining Acuuarcy 66.470% \tValidation Acuuarcy 66.787%\n","Epoch: 1359 \tTraining Loss: 0.01339512 \tValidation Loss 0.01373888 \tTraining Acuuarcy 66.143% \tValidation Acuuarcy 66.425%\n","Epoch: 1360 \tTraining Loss: 0.01338203 \tValidation Loss 0.01374799 \tTraining Acuuarcy 66.230% \tValidation Acuuarcy 66.537%\n","Epoch: 1361 \tTraining Loss: 0.01339021 \tValidation Loss 0.01378205 \tTraining Acuuarcy 66.167% \tValidation Acuuarcy 66.397%\n","Epoch: 1362 \tTraining Loss: 0.01337288 \tValidation Loss 0.01386027 \tTraining Acuuarcy 66.376% \tValidation Acuuarcy 67.038%\n","Epoch: 1363 \tTraining Loss: 0.01336108 \tValidation Loss 0.01368059 \tTraining Acuuarcy 66.613% \tValidation Acuuarcy 66.760%\n","Epoch: 1364 \tTraining Loss: 0.01338791 \tValidation Loss 0.01374883 \tTraining Acuuarcy 66.150% \tValidation Acuuarcy 66.258%\n","Epoch: 1365 \tTraining Loss: 0.01337233 \tValidation Loss 0.01376295 \tTraining Acuuarcy 66.355% \tValidation Acuuarcy 66.955%\n","Epoch: 1366 \tTraining Loss: 0.01337852 \tValidation Loss 0.01386995 \tTraining Acuuarcy 66.373% \tValidation Acuuarcy 66.509%\n","Epoch: 1367 \tTraining Loss: 0.01340594 \tValidation Loss 0.01382749 \tTraining Acuuarcy 66.011% \tValidation Acuuarcy 66.955%\n","Epoch: 1368 \tTraining Loss: 0.01334227 \tValidation Loss 0.01368063 \tTraining Acuuarcy 66.791% \tValidation Acuuarcy 67.317%\n","Epoch: 1369 \tTraining Loss: 0.01338229 \tValidation Loss 0.01380247 \tTraining Acuuarcy 66.275% \tValidation Acuuarcy 66.425%\n","Epoch: 1370 \tTraining Loss: 0.01339006 \tValidation Loss 0.01377873 \tTraining Acuuarcy 66.188% \tValidation Acuuarcy 66.509%\n","Epoch: 1371 \tTraining Loss: 0.01338742 \tValidation Loss 0.01372366 \tTraining Acuuarcy 66.237% \tValidation Acuuarcy 66.648%\n","Epoch: 1372 \tTraining Loss: 0.01337237 \tValidation Loss 0.01384059 \tTraining Acuuarcy 66.443% \tValidation Acuuarcy 65.784%\n","Epoch: 1373 \tTraining Loss: 0.01334947 \tValidation Loss 0.01363995 \tTraining Acuuarcy 66.773% \tValidation Acuuarcy 67.317%\n","Epoch: 1374 \tTraining Loss: 0.01338953 \tValidation Loss 0.01377302 \tTraining Acuuarcy 66.223% \tValidation Acuuarcy 66.899%\n","Epoch: 1375 \tTraining Loss: 0.01337858 \tValidation Loss 0.01379277 \tTraining Acuuarcy 66.331% \tValidation Acuuarcy 65.784%\n","Epoch: 1376 \tTraining Loss: 0.01336488 \tValidation Loss 0.01385168 \tTraining Acuuarcy 66.533% \tValidation Acuuarcy 66.509%\n","Epoch: 1377 \tTraining Loss: 0.01336128 \tValidation Loss 0.01381400 \tTraining Acuuarcy 66.652% \tValidation Acuuarcy 66.955%\n","Epoch: 1378 \tTraining Loss: 0.01338997 \tValidation Loss 0.01371627 \tTraining Acuuarcy 66.199% \tValidation Acuuarcy 66.899%\n","Epoch: 1379 \tTraining Loss: 0.01338494 \tValidation Loss 0.01385788 \tTraining Acuuarcy 66.265% \tValidation Acuuarcy 65.701%\n","Epoch: 1380 \tTraining Loss: 0.01336040 \tValidation Loss 0.01369317 \tTraining Acuuarcy 66.596% \tValidation Acuuarcy 66.425%\n","Epoch: 1381 \tTraining Loss: 0.01337880 \tValidation Loss 0.01378349 \tTraining Acuuarcy 66.251% \tValidation Acuuarcy 66.035%\n","Epoch: 1382 \tTraining Loss: 0.01338926 \tValidation Loss 0.01387634 \tTraining Acuuarcy 66.213% \tValidation Acuuarcy 66.453%\n","Epoch: 1383 \tTraining Loss: 0.01337053 \tValidation Loss 0.01376751 \tTraining Acuuarcy 66.505% \tValidation Acuuarcy 66.927%\n","Epoch: 1384 \tTraining Loss: 0.01338823 \tValidation Loss 0.01369400 \tTraining Acuuarcy 66.272% \tValidation Acuuarcy 67.150%\n","Epoch: 1385 \tTraining Loss: 0.01339752 \tValidation Loss 0.01370276 \tTraining Acuuarcy 66.049% \tValidation Acuuarcy 66.174%\n","Epoch: 1386 \tTraining Loss: 0.01336653 \tValidation Loss 0.01373366 \tTraining Acuuarcy 66.502% \tValidation Acuuarcy 66.676%\n","Epoch: 1387 \tTraining Loss: 0.01336661 \tValidation Loss 0.01393332 \tTraining Acuuarcy 66.533% \tValidation Acuuarcy 66.063%\n","Epoch: 1388 \tTraining Loss: 0.01337752 \tValidation Loss 0.01383233 \tTraining Acuuarcy 66.338% \tValidation Acuuarcy 66.815%\n","Epoch: 1389 \tTraining Loss: 0.01337927 \tValidation Loss 0.01380169 \tTraining Acuuarcy 66.380% \tValidation Acuuarcy 66.369%\n","Epoch: 1390 \tTraining Loss: 0.01338860 \tValidation Loss 0.01392654 \tTraining Acuuarcy 66.171% \tValidation Acuuarcy 65.868%\n","Epoch: 1391 \tTraining Loss: 0.01339280 \tValidation Loss 0.01377684 \tTraining Acuuarcy 66.167% \tValidation Acuuarcy 66.843%\n","Epoch: 1392 \tTraining Loss: 0.01339117 \tValidation Loss 0.01386123 \tTraining Acuuarcy 66.237% \tValidation Acuuarcy 65.645%\n","Epoch: 1393 \tTraining Loss: 0.01339281 \tValidation Loss 0.01369347 \tTraining Acuuarcy 66.160% \tValidation Acuuarcy 67.289%\n","Epoch: 1394 \tTraining Loss: 0.01338462 \tValidation Loss 0.01383256 \tTraining Acuuarcy 66.181% \tValidation Acuuarcy 66.704%\n","Epoch: 1395 \tTraining Loss: 0.01337573 \tValidation Loss 0.01384433 \tTraining Acuuarcy 66.383% \tValidation Acuuarcy 67.400%\n","Epoch: 1396 \tTraining Loss: 0.01338392 \tValidation Loss 0.01376543 \tTraining Acuuarcy 66.296% \tValidation Acuuarcy 66.147%\n","Epoch: 1397 \tTraining Loss: 0.01339337 \tValidation Loss 0.01375947 \tTraining Acuuarcy 66.185% \tValidation Acuuarcy 65.589%\n","Epoch: 1398 \tTraining Loss: 0.01337370 \tValidation Loss 0.01377403 \tTraining Acuuarcy 66.387% \tValidation Acuuarcy 66.063%\n","Epoch: 1399 \tTraining Loss: 0.01336579 \tValidation Loss 0.01379596 \tTraining Acuuarcy 66.540% \tValidation Acuuarcy 65.924%\n","Epoch: 1400 \tTraining Loss: 0.01339633 \tValidation Loss 0.01384233 \tTraining Acuuarcy 66.133% \tValidation Acuuarcy 66.760%\n","Epoch: 1401 \tTraining Loss: 0.01339369 \tValidation Loss 0.01369938 \tTraining Acuuarcy 66.174% \tValidation Acuuarcy 67.930%\n","Epoch: 1402 \tTraining Loss: 0.01338607 \tValidation Loss 0.01367867 \tTraining Acuuarcy 66.192% \tValidation Acuuarcy 67.735%\n","Epoch: 1403 \tTraining Loss: 0.01337789 \tValidation Loss 0.01373158 \tTraining Acuuarcy 66.345% \tValidation Acuuarcy 66.676%\n","Epoch: 1404 \tTraining Loss: 0.01338606 \tValidation Loss 0.01380630 \tTraining Acuuarcy 66.174% \tValidation Acuuarcy 65.701%\n","Epoch: 1405 \tTraining Loss: 0.01339436 \tValidation Loss 0.01378268 \tTraining Acuuarcy 66.129% \tValidation Acuuarcy 66.676%\n","Epoch: 1406 \tTraining Loss: 0.01336192 \tValidation Loss 0.01375721 \tTraining Acuuarcy 66.523% \tValidation Acuuarcy 67.122%\n","Epoch: 1407 \tTraining Loss: 0.01337199 \tValidation Loss 0.01387325 \tTraining Acuuarcy 66.484% \tValidation Acuuarcy 66.007%\n","Epoch: 1408 \tTraining Loss: 0.01338396 \tValidation Loss 0.01373886 \tTraining Acuuarcy 66.220% \tValidation Acuuarcy 66.760%\n","Epoch: 1409 \tTraining Loss: 0.01338320 \tValidation Loss 0.01379793 \tTraining Acuuarcy 66.227% \tValidation Acuuarcy 66.565%\n","Epoch: 1410 \tTraining Loss: 0.01338829 \tValidation Loss 0.01371228 \tTraining Acuuarcy 66.206% \tValidation Acuuarcy 66.815%\n","Epoch: 1411 \tTraining Loss: 0.01336660 \tValidation Loss 0.01373342 \tTraining Acuuarcy 66.509% \tValidation Acuuarcy 66.509%\n","Epoch: 1412 \tTraining Loss: 0.01336640 \tValidation Loss 0.01376794 \tTraining Acuuarcy 66.568% \tValidation Acuuarcy 66.843%\n","Epoch: 1413 \tTraining Loss: 0.01338155 \tValidation Loss 0.01374369 \tTraining Acuuarcy 66.324% \tValidation Acuuarcy 67.261%\n","Epoch: 1414 \tTraining Loss: 0.01339308 \tValidation Loss 0.01376175 \tTraining Acuuarcy 66.112% \tValidation Acuuarcy 67.010%\n","Epoch: 1415 \tTraining Loss: 0.01337464 \tValidation Loss 0.01371025 \tTraining Acuuarcy 66.376% \tValidation Acuuarcy 66.760%\n","Epoch: 1416 \tTraining Loss: 0.01336171 \tValidation Loss 0.01382216 \tTraining Acuuarcy 66.578% \tValidation Acuuarcy 66.843%\n","Epoch: 1417 \tTraining Loss: 0.01338468 \tValidation Loss 0.01374365 \tTraining Acuuarcy 66.153% \tValidation Acuuarcy 66.592%\n","Epoch: 1418 \tTraining Loss: 0.01338401 \tValidation Loss 0.01374996 \tTraining Acuuarcy 66.216% \tValidation Acuuarcy 67.177%\n","Epoch: 1419 \tTraining Loss: 0.01336317 \tValidation Loss 0.01378553 \tTraining Acuuarcy 66.467% \tValidation Acuuarcy 66.620%\n","Epoch: 1420 \tTraining Loss: 0.01337803 \tValidation Loss 0.01376806 \tTraining Acuuarcy 66.310% \tValidation Acuuarcy 66.760%\n","Epoch: 1421 \tTraining Loss: 0.01336179 \tValidation Loss 0.01384727 \tTraining Acuuarcy 66.547% \tValidation Acuuarcy 65.617%\n","Epoch: 1422 \tTraining Loss: 0.01337516 \tValidation Loss 0.01380113 \tTraining Acuuarcy 66.457% \tValidation Acuuarcy 66.258%\n","Epoch: 1423 \tTraining Loss: 0.01337105 \tValidation Loss 0.01370108 \tTraining Acuuarcy 66.505% \tValidation Acuuarcy 67.122%\n","Epoch: 1424 \tTraining Loss: 0.01337217 \tValidation Loss 0.01372093 \tTraining Acuuarcy 66.345% \tValidation Acuuarcy 66.147%\n","Epoch: 1425 \tTraining Loss: 0.01336765 \tValidation Loss 0.01384243 \tTraining Acuuarcy 66.422% \tValidation Acuuarcy 66.481%\n","Epoch: 1426 \tTraining Loss: 0.01335897 \tValidation Loss 0.01362722 \tTraining Acuuarcy 66.624% \tValidation Acuuarcy 67.373%\n","Epoch: 1427 \tTraining Loss: 0.01336995 \tValidation Loss 0.01373491 \tTraining Acuuarcy 66.450% \tValidation Acuuarcy 67.428%\n","Epoch: 1428 \tTraining Loss: 0.01338405 \tValidation Loss 0.01373021 \tTraining Acuuarcy 66.314% \tValidation Acuuarcy 66.899%\n","Epoch: 1429 \tTraining Loss: 0.01338797 \tValidation Loss 0.01366854 \tTraining Acuuarcy 66.265% \tValidation Acuuarcy 67.568%\n","Epoch: 1430 \tTraining Loss: 0.01341759 \tValidation Loss 0.01373806 \tTraining Acuuarcy 65.847% \tValidation Acuuarcy 66.648%\n","Epoch: 1431 \tTraining Loss: 0.01338048 \tValidation Loss 0.01363840 \tTraining Acuuarcy 66.359% \tValidation Acuuarcy 67.317%\n","Epoch: 1432 \tTraining Loss: 0.01337355 \tValidation Loss 0.01390351 \tTraining Acuuarcy 66.415% \tValidation Acuuarcy 66.592%\n","Epoch: 1433 \tTraining Loss: 0.01337692 \tValidation Loss 0.01381445 \tTraining Acuuarcy 66.369% \tValidation Acuuarcy 66.202%\n","Epoch: 1434 \tTraining Loss: 0.01335824 \tValidation Loss 0.01374692 \tTraining Acuuarcy 66.561% \tValidation Acuuarcy 66.537%\n","Epoch: 1435 \tTraining Loss: 0.01337986 \tValidation Loss 0.01377164 \tTraining Acuuarcy 66.248% \tValidation Acuuarcy 66.760%\n","Epoch: 1436 \tTraining Loss: 0.01338808 \tValidation Loss 0.01382092 \tTraining Acuuarcy 66.272% \tValidation Acuuarcy 66.202%\n","Epoch: 1437 \tTraining Loss: 0.01339844 \tValidation Loss 0.01375424 \tTraining Acuuarcy 66.052% \tValidation Acuuarcy 66.314%\n","Epoch: 1438 \tTraining Loss: 0.01338017 \tValidation Loss 0.01376571 \tTraining Acuuarcy 66.254% \tValidation Acuuarcy 66.147%\n","Epoch: 1439 \tTraining Loss: 0.01336735 \tValidation Loss 0.01373165 \tTraining Acuuarcy 66.432% \tValidation Acuuarcy 67.205%\n","Epoch: 1440 \tTraining Loss: 0.01340013 \tValidation Loss 0.01376674 \tTraining Acuuarcy 66.049% \tValidation Acuuarcy 67.066%\n","Epoch: 1441 \tTraining Loss: 0.01336911 \tValidation Loss 0.01381353 \tTraining Acuuarcy 66.429% \tValidation Acuuarcy 66.955%\n","Epoch: 1442 \tTraining Loss: 0.01336693 \tValidation Loss 0.01389786 \tTraining Acuuarcy 66.484% \tValidation Acuuarcy 65.840%\n","Epoch: 1443 \tTraining Loss: 0.01336512 \tValidation Loss 0.01373543 \tTraining Acuuarcy 66.512% \tValidation Acuuarcy 65.756%\n","Epoch: 1444 \tTraining Loss: 0.01338332 \tValidation Loss 0.01377091 \tTraining Acuuarcy 66.202% \tValidation Acuuarcy 66.314%\n","Epoch: 1445 \tTraining Loss: 0.01338177 \tValidation Loss 0.01373170 \tTraining Acuuarcy 66.282% \tValidation Acuuarcy 66.397%\n","Epoch: 1446 \tTraining Loss: 0.01336828 \tValidation Loss 0.01385367 \tTraining Acuuarcy 66.390% \tValidation Acuuarcy 66.537%\n","Epoch: 1447 \tTraining Loss: 0.01335914 \tValidation Loss 0.01384140 \tTraining Acuuarcy 66.575% \tValidation Acuuarcy 65.924%\n","Epoch: 1448 \tTraining Loss: 0.01338460 \tValidation Loss 0.01374857 \tTraining Acuuarcy 66.237% \tValidation Acuuarcy 67.289%\n","Epoch: 1449 \tTraining Loss: 0.01338647 \tValidation Loss 0.01377150 \tTraining Acuuarcy 66.251% \tValidation Acuuarcy 66.119%\n","Epoch: 1450 \tTraining Loss: 0.01338557 \tValidation Loss 0.01377658 \tTraining Acuuarcy 66.147% \tValidation Acuuarcy 66.676%\n","Epoch: 1451 \tTraining Loss: 0.01336567 \tValidation Loss 0.01383330 \tTraining Acuuarcy 66.481% \tValidation Acuuarcy 66.620%\n","Epoch: 1452 \tTraining Loss: 0.01339884 \tValidation Loss 0.01376633 \tTraining Acuuarcy 66.119% \tValidation Acuuarcy 66.286%\n","Epoch: 1453 \tTraining Loss: 0.01337742 \tValidation Loss 0.01375579 \tTraining Acuuarcy 66.314% \tValidation Acuuarcy 66.314%\n","Epoch: 1454 \tTraining Loss: 0.01337729 \tValidation Loss 0.01377079 \tTraining Acuuarcy 66.185% \tValidation Acuuarcy 65.422%\n","Epoch: 1455 \tTraining Loss: 0.01340299 \tValidation Loss 0.01388970 \tTraining Acuuarcy 66.077% \tValidation Acuuarcy 65.979%\n","Epoch: 1456 \tTraining Loss: 0.01336451 \tValidation Loss 0.01387526 \tTraining Acuuarcy 66.512% \tValidation Acuuarcy 65.673%\n","Epoch: 1457 \tTraining Loss: 0.01338472 \tValidation Loss 0.01386421 \tTraining Acuuarcy 66.254% \tValidation Acuuarcy 66.314%\n","Epoch: 1458 \tTraining Loss: 0.01340494 \tValidation Loss 0.01369233 \tTraining Acuuarcy 66.007% \tValidation Acuuarcy 66.425%\n","Epoch: 1459 \tTraining Loss: 0.01338636 \tValidation Loss 0.01390152 \tTraining Acuuarcy 66.261% \tValidation Acuuarcy 66.871%\n","Epoch: 1460 \tTraining Loss: 0.01338198 \tValidation Loss 0.01370759 \tTraining Acuuarcy 66.272% \tValidation Acuuarcy 67.679%\n","Epoch: 1461 \tTraining Loss: 0.01339523 \tValidation Loss 0.01382151 \tTraining Acuuarcy 66.059% \tValidation Acuuarcy 66.007%\n","Epoch: 1462 \tTraining Loss: 0.01337657 \tValidation Loss 0.01378575 \tTraining Acuuarcy 66.310% \tValidation Acuuarcy 66.592%\n","Epoch: 1463 \tTraining Loss: 0.01337195 \tValidation Loss 0.01398337 \tTraining Acuuarcy 66.453% \tValidation Acuuarcy 65.255%\n","Epoch: 1464 \tTraining Loss: 0.01336719 \tValidation Loss 0.01392342 \tTraining Acuuarcy 66.484% \tValidation Acuuarcy 65.589%\n","Epoch: 1465 \tTraining Loss: 0.01336194 \tValidation Loss 0.01377211 \tTraining Acuuarcy 66.568% \tValidation Acuuarcy 66.648%\n","Epoch: 1466 \tTraining Loss: 0.01336342 \tValidation Loss 0.01366394 \tTraining Acuuarcy 66.558% \tValidation Acuuarcy 66.871%\n","Epoch: 1467 \tTraining Loss: 0.01337793 \tValidation Loss 0.01376289 \tTraining Acuuarcy 66.289% \tValidation Acuuarcy 66.871%\n","Epoch: 1468 \tTraining Loss: 0.01338077 \tValidation Loss 0.01389368 \tTraining Acuuarcy 66.254% \tValidation Acuuarcy 65.812%\n","Epoch: 1469 \tTraining Loss: 0.01337048 \tValidation Loss 0.01387143 \tTraining Acuuarcy 66.404% \tValidation Acuuarcy 66.871%\n","Epoch: 1470 \tTraining Loss: 0.01337013 \tValidation Loss 0.01381948 \tTraining Acuuarcy 66.362% \tValidation Acuuarcy 66.147%\n","Epoch: 1471 \tTraining Loss: 0.01338453 \tValidation Loss 0.01377386 \tTraining Acuuarcy 66.261% \tValidation Acuuarcy 66.732%\n","Epoch: 1472 \tTraining Loss: 0.01337093 \tValidation Loss 0.01371979 \tTraining Acuuarcy 66.387% \tValidation Acuuarcy 67.512%\n","Epoch: 1473 \tTraining Loss: 0.01335095 \tValidation Loss 0.01372871 \tTraining Acuuarcy 66.742% \tValidation Acuuarcy 65.952%\n","Epoch: 1474 \tTraining Loss: 0.01339169 \tValidation Loss 0.01385733 \tTraining Acuuarcy 66.140% \tValidation Acuuarcy 67.150%\n","Epoch: 1475 \tTraining Loss: 0.01336813 \tValidation Loss 0.01374639 \tTraining Acuuarcy 66.463% \tValidation Acuuarcy 66.592%\n","Epoch: 1476 \tTraining Loss: 0.01338856 \tValidation Loss 0.01378779 \tTraining Acuuarcy 66.206% \tValidation Acuuarcy 66.815%\n","Epoch: 1477 \tTraining Loss: 0.01337479 \tValidation Loss 0.01378330 \tTraining Acuuarcy 66.359% \tValidation Acuuarcy 66.704%\n","Epoch: 1478 \tTraining Loss: 0.01337388 \tValidation Loss 0.01380323 \tTraining Acuuarcy 66.450% \tValidation Acuuarcy 67.205%\n","Epoch: 1479 \tTraining Loss: 0.01338030 \tValidation Loss 0.01386915 \tTraining Acuuarcy 66.317% \tValidation Acuuarcy 66.286%\n","Epoch: 1480 \tTraining Loss: 0.01336474 \tValidation Loss 0.01365555 \tTraining Acuuarcy 66.502% \tValidation Acuuarcy 67.818%\n","Epoch: 1481 \tTraining Loss: 0.01337592 \tValidation Loss 0.01371849 \tTraining Acuuarcy 66.408% \tValidation Acuuarcy 66.871%\n","Epoch: 1482 \tTraining Loss: 0.01337440 \tValidation Loss 0.01381523 \tTraining Acuuarcy 66.366% \tValidation Acuuarcy 66.732%\n","Epoch: 1483 \tTraining Loss: 0.01337896 \tValidation Loss 0.01372231 \tTraining Acuuarcy 66.390% \tValidation Acuuarcy 66.843%\n","Epoch: 1484 \tTraining Loss: 0.01335663 \tValidation Loss 0.01367300 \tTraining Acuuarcy 66.575% \tValidation Acuuarcy 67.540%\n","Epoch: 1485 \tTraining Loss: 0.01335799 \tValidation Loss 0.01387643 \tTraining Acuuarcy 66.568% \tValidation Acuuarcy 66.815%\n","Epoch: 1486 \tTraining Loss: 0.01336829 \tValidation Loss 0.01375327 \tTraining Acuuarcy 66.467% \tValidation Acuuarcy 66.871%\n","Epoch: 1487 \tTraining Loss: 0.01336847 \tValidation Loss 0.01387471 \tTraining Acuuarcy 66.571% \tValidation Acuuarcy 66.286%\n","Epoch: 1488 \tTraining Loss: 0.01340213 \tValidation Loss 0.01374086 \tTraining Acuuarcy 66.066% \tValidation Acuuarcy 66.676%\n","Epoch: 1489 \tTraining Loss: 0.01335380 \tValidation Loss 0.01380795 \tTraining Acuuarcy 66.693% \tValidation Acuuarcy 66.982%\n","Epoch: 1490 \tTraining Loss: 0.01338727 \tValidation Loss 0.01381655 \tTraining Acuuarcy 66.181% \tValidation Acuuarcy 66.676%\n","Epoch: 1491 \tTraining Loss: 0.01335782 \tValidation Loss 0.01387261 \tTraining Acuuarcy 66.634% \tValidation Acuuarcy 66.147%\n","Epoch: 1492 \tTraining Loss: 0.01337937 \tValidation Loss 0.01372333 \tTraining Acuuarcy 66.286% \tValidation Acuuarcy 67.595%\n","Epoch: 1493 \tTraining Loss: 0.01336633 \tValidation Loss 0.01368683 \tTraining Acuuarcy 66.495% \tValidation Acuuarcy 67.205%\n","Epoch: 1494 \tTraining Loss: 0.01334795 \tValidation Loss 0.01381589 \tTraining Acuuarcy 66.620% \tValidation Acuuarcy 66.955%\n","Epoch: 1495 \tTraining Loss: 0.01336240 \tValidation Loss 0.01372812 \tTraining Acuuarcy 66.610% \tValidation Acuuarcy 66.676%\n","Epoch: 1496 \tTraining Loss: 0.01338385 \tValidation Loss 0.01382874 \tTraining Acuuarcy 66.279% \tValidation Acuuarcy 65.896%\n","Epoch: 1497 \tTraining Loss: 0.01337505 \tValidation Loss 0.01382006 \tTraining Acuuarcy 66.296% \tValidation Acuuarcy 66.091%\n","Epoch: 1498 \tTraining Loss: 0.01338038 \tValidation Loss 0.01371054 \tTraining Acuuarcy 66.362% \tValidation Acuuarcy 66.815%\n","Epoch: 1499 \tTraining Loss: 0.01338830 \tValidation Loss 0.01381950 \tTraining Acuuarcy 66.338% \tValidation Acuuarcy 66.035%\n","Epoch: 1500 \tTraining Loss: 0.01337259 \tValidation Loss 0.01375229 \tTraining Acuuarcy 66.436% \tValidation Acuuarcy 67.038%\n","===================================Training Finished===================================\n"]}],"source":["Train(epochs, train_loader, val_loader, criterion, optmizer, device)"]},{"cell_type":"code","execution_count":null,"id":"bae03f1e","metadata":{"id":"bae03f1e","outputId":"2b572b1f-b37b-442f-dce5-9e10826dbb1d"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":null,"id":"aa316231","metadata":{"id":"aa316231"},"outputs":[],"source":["GD = Generate_data(\"fer2013.csv\")"]},{"cell_type":"code","execution_count":null,"id":"644edb48","metadata":{"id":"644edb48","outputId":"c6239e31-04fe-4ce2-82e3-2f34554b9236"},"outputs":[{"name":"stdout","output_type":"stream","text":["(28709, 3)\n","Done splitting the test file into validation & final test file\n"]}],"source":["GD.split_test()"]},{"cell_type":"code","execution_count":null,"id":"72086292","metadata":{"id":"72086292","outputId":"131b5006-b041-4ab1-f8e9-d8b1d76eaa1d"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|| 28709/28709 [00:09<00:00, 2929.81it/s]"]},{"name":"stdout","output_type":"stream","text":["Done saving data/train data\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["GD.save_images(\"train\")"]},{"cell_type":"code","execution_count":null,"id":"ec50b86b","metadata":{"id":"ec50b86b","outputId":"75ea21ba-4226-4269-dfaf-10cd85c37a4b"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|| 3589/3589 [00:01<00:00, 2441.81it/s]"]},{"name":"stdout","output_type":"stream","text":["Done saving data/test data\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["GD.save_images(\"test\")"]},{"cell_type":"code","execution_count":null,"id":"105123b4","metadata":{"id":"105123b4","outputId":"d780ab6b-50d2-45e2-f05c-7833b3cd2458"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|| 3589/3589 [00:01<00:00, 2612.39it/s]"]},{"name":"stdout","output_type":"stream","text":["Done saving data/val data\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["GD.save_images(\"val\")"]},{"cell_type":"code","execution_count":null,"id":"85b03ef0","metadata":{"id":"85b03ef0","outputId":"7eb95125-5e88-4798-95b6-b3137924935b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Final accuracy on the training set =  66.742\n"]}],"source":["print(\"Final accuracy on the training set = \", 66.742)"]},{"cell_type":"code","execution_count":null,"id":"a8073bb8","metadata":{"id":"a8073bb8","outputId":"3ff25716-c917-44b9-8993-8a7fea64ec05"},"outputs":[{"name":"stdout","output_type":"stream","text":["Final accuracy on the training set =  65.952\n"]}],"source":["print(\"Final accuracy on the training set = \", 65.952)"]},{"cell_type":"code","execution_count":null,"id":"999ef5b0","metadata":{"id":"999ef5b0"},"outputs":[],"source":["test_dataset= Plain_Dataset(csv_file=\"data/test.csv\", img_dir = \"data/test/\", datatype = 'test', transform = transformation)\n"]},{"cell_type":"code","execution_count":null,"id":"ca55943b","metadata":{"id":"ca55943b","outputId":"3f5ffc7c-9615-4d67-b0f3-5c6ba5b96deb"},"outputs":[{"name":"stdout","output_type":"stream","text":["3589\n"]}],"source":["print(test_dataset.__len__())"]},{"cell_type":"code","execution_count":null,"id":"c28cee9c","metadata":{"id":"c28cee9c"},"outputs":[],"source":["test_loader=   DataLoader(test_dataset,batch_size=batchsize,shuffle = True,num_workers=0)"]},{"cell_type":"code","execution_count":null,"id":"162c6ba5","metadata":{"id":"162c6ba5"},"outputs":[],"source":["test_result = []\n","test_loss = 0\n","test_correct = 0\n","for data,labels in test_loader:\n","            data, labels = data.to(device), labels.to(device)\n","            test_outputs = net(data)\n","            #print(val_outputs[0][3])\n","            #print(labels[0])\n","            t_loss = criterion(test_outputs, labels)\n","            test_loss += t_loss.item()\n","            _, test_preds = torch.max(test_outputs,1)\n","            test_result.append(torch.max(test_outputs,1))\n","            test_correct += torch.sum(test_preds == labels.data)"]},{"cell_type":"code","execution_count":null,"id":"3481eced","metadata":{"id":"3481eced","outputId":"07a1f503-bc60-4084-d3b7-832067ead551"},"outputs":[{"name":"stdout","output_type":"stream","text":["Final accuracy on the testing set =  tensor(0.6643)\n"]}],"source":["print(\"Final accuracy on the testing set = \", test_correct/3589)"]},{"cell_type":"code","execution_count":null,"id":"aad033c5","metadata":{"id":"aad033c5","outputId":"7dc5046c-0258-44b1-de20-59aadaad5984"},"outputs":[{"data":{"text/plain":["[torch.return_types.max(\n"," values=tensor([1.0000, 1.0000, 1.0000, 0.9987, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9999, 0.8856, 1.0000, 1.0000, 1.0000, 1.0000, 0.9938, 1.0000, 0.9332,\n","         0.9178, 0.8048, 1.0000, 0.9946, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9906, 0.9635, 0.8155, 0.9587, 1.0000, 0.9857, 0.6486, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9999, 1.0000, 0.9826, 1.0000, 0.9990, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 0.9995, 0.8748, 0.9997,\n","         0.9994, 1.0000, 1.0000, 0.9845, 0.9991, 1.0000, 1.0000, 0.9980, 1.0000,\n","         0.9113, 1.0000, 1.0000, 1.0000, 1.0000, 0.9964, 1.0000, 0.9979, 0.9998,\n","         1.0000, 1.0000, 0.8722, 0.9975, 0.9995, 1.0000, 1.0000, 0.9963, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7980, 1.0000, 1.0000, 1.0000,\n","         0.9836, 1.0000, 1.0000, 1.0000, 0.9764, 1.0000, 0.9761, 0.7647, 1.0000,\n","         1.0000, 0.5954, 0.5830, 0.9415, 1.0000, 1.0000, 1.0000, 0.9934, 0.9999,\n","         1.0000, 0.9862, 0.9920, 0.6712, 1.0000, 1.0000, 1.0000, 1.0000, 0.9855,\n","         1.0000, 0.9987, 0.9995, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([6, 4, 4, 6, 4, 3, 4, 0, 3, 2, 2, 3, 3, 3, 5, 2, 4, 5, 0, 6, 0, 3, 3, 3,\n","         4, 5, 6, 2, 6, 6, 6, 3, 6, 6, 5, 2, 6, 5, 6, 0, 2, 6, 4, 5, 5, 3, 4, 4,\n","         4, 0, 5, 5, 0, 6, 0, 3, 6, 2, 6, 3, 4, 6, 4, 5, 3, 0, 2, 3, 2, 3, 2, 2,\n","         6, 4, 4, 6, 5, 3, 3, 2, 6, 2, 3, 0, 3, 3, 6, 0, 0, 6, 2, 2, 5, 6, 6, 4,\n","         6, 4, 3, 0, 6, 3, 6, 3, 3, 3, 4, 6, 0, 3, 6, 2, 5, 6, 3, 0, 3, 5, 6, 0,\n","         3, 5, 6, 3, 5, 4, 3, 0])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 1.0000, 1.0000, 0.9982, 1.0000, 1.0000, 0.5502, 0.9352, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9996, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.9432, 1.0000, 1.0000, 1.0000, 0.7929, 0.7338, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8507, 1.0000, 0.9739, 1.0000,\n","         1.0000, 1.0000, 0.9999, 1.0000, 0.9655, 1.0000, 1.0000, 1.0000, 0.9914,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9504, 1.0000, 0.9940,\n","         1.0000, 0.9957, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 0.9845, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 0.9997, 1.0000, 1.0000,\n","         1.0000, 0.9995, 1.0000, 0.9994, 0.6343, 1.0000, 0.9999, 0.9999, 1.0000,\n","         0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9931, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8082, 0.9921, 1.0000, 1.0000,\n","         1.0000, 0.6010, 0.9913, 1.0000, 0.9986, 1.0000, 0.7348, 0.9979, 1.0000,\n","         1.0000, 0.4912, 1.0000, 0.9972, 1.0000, 0.9864, 0.9648, 1.0000, 0.9989,\n","         1.0000, 0.9992, 1.0000, 1.0000, 1.0000, 0.9950, 1.0000, 0.9966, 0.9067,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([0, 0, 3, 0, 5, 0, 4, 2, 3, 3, 2, 3, 3, 3, 4, 3, 5, 3, 3, 4, 0, 2, 6, 5,\n","         2, 6, 3, 3, 2, 2, 3, 3, 3, 6, 4, 5, 3, 0, 0, 4, 4, 4, 0, 0, 0, 6, 3, 5,\n","         4, 6, 3, 2, 3, 2, 4, 4, 4, 6, 3, 2, 3, 6, 6, 4, 3, 5, 6, 2, 4, 3, 4, 0,\n","         0, 2, 2, 0, 4, 0, 3, 6, 2, 2, 6, 5, 5, 3, 3, 2, 2, 3, 5, 6, 6, 0, 6, 6,\n","         2, 5, 5, 3, 0, 5, 3, 3, 2, 6, 6, 2, 2, 5, 3, 0, 2, 0, 0, 6, 5, 6, 4, 3,\n","         3, 3, 3, 3, 4, 4, 3, 0])),\n"," torch.return_types.max(\n"," values=tensor([0.9774, 1.0000, 1.0000, 0.9998, 0.9914, 1.0000, 0.9966, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9986, 1.0000, 0.6473, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.5747,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9935, 1.0000, 0.9999, 1.0000, 1.0000,\n","         0.9867, 0.9997, 0.9896, 1.0000, 1.0000, 0.9989, 1.0000, 0.8957, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.5954, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.6718, 0.9997, 1.0000, 1.0000, 1.0000,\n","         0.9070, 1.0000, 0.9999, 0.9977, 0.9993, 0.9997, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9986, 0.9118, 1.0000, 1.0000,\n","         1.0000, 0.9763, 0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.9996, 1.0000, 1.0000, 0.9186, 1.0000, 0.9940,\n","         1.0000, 1.0000, 0.8436, 0.5594, 1.0000, 0.9974, 0.5040, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9977, 0.7384, 1.0000, 1.0000,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([4, 6, 0, 4, 3, 3, 6, 3, 6, 3, 3, 6, 2, 3, 3, 2, 3, 6, 2, 6, 6, 0, 0, 3,\n","         3, 0, 4, 6, 3, 3, 4, 6, 4, 3, 4, 6, 5, 3, 4, 4, 6, 3, 3, 6, 0, 0, 3, 0,\n","         6, 4, 3, 6, 2, 5, 3, 3, 3, 0, 2, 0, 6, 5, 3, 4, 5, 3, 2, 3, 3, 6, 5, 0,\n","         6, 3, 4, 2, 3, 4, 5, 3, 4, 6, 3, 4, 3, 3, 6, 2, 5, 3, 6, 3, 4, 4, 3, 4,\n","         4, 5, 3, 3, 3, 6, 6, 3, 2, 5, 5, 4, 6, 6, 0, 6, 3, 0, 0, 3, 3, 3, 4, 3,\n","         5, 4, 6, 2, 5, 3, 6, 6])),\n"," torch.return_types.max(\n"," values=tensor([0.9727, 1.0000, 0.4970, 1.0000, 0.8325, 1.0000, 1.0000, 0.7152, 1.0000,\n","         0.6735, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9996,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.8429, 0.9993, 0.9945, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9556, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9994, 0.9674, 0.8318, 1.0000, 1.0000, 1.0000, 1.0000, 0.9923, 1.0000,\n","         1.0000, 1.0000, 0.9365, 1.0000, 0.9999, 1.0000, 1.0000, 0.9956, 0.9972,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9968,\n","         0.9999, 1.0000, 1.0000, 1.0000, 0.9998, 1.0000, 1.0000, 0.9998, 1.0000,\n","         1.0000, 0.9996, 1.0000, 1.0000, 0.9947, 1.0000, 1.0000, 1.0000, 0.9958,\n","         1.0000, 1.0000, 0.9979, 0.9912, 0.9982, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.5039, 1.0000, 0.8336, 0.9957, 1.0000, 1.0000, 1.0000, 0.9996, 1.0000,\n","         1.0000, 0.9997, 1.0000, 1.0000, 0.8771, 1.0000, 1.0000, 1.0000, 0.9996,\n","         1.0000, 0.9937, 1.0000, 1.0000, 0.7716, 1.0000, 0.7224, 0.9994, 1.0000,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([5, 3, 0, 6, 0, 3, 0, 6, 6, 6, 4, 4, 3, 4, 4, 4, 3, 4, 0, 3, 0, 6, 4, 0,\n","         4, 5, 6, 4, 4, 0, 5, 5, 3, 6, 4, 0, 6, 4, 4, 3, 5, 3, 6, 4, 3, 0, 6, 4,\n","         4, 2, 3, 3, 3, 0, 3, 3, 4, 6, 6, 3, 3, 4, 5, 4, 2, 3, 3, 4, 4, 2, 3, 4,\n","         4, 6, 4, 3, 6, 4, 3, 0, 3, 6, 4, 3, 3, 6, 3, 6, 5, 6, 5, 5, 3, 4, 6, 5,\n","         3, 6, 6, 4, 3, 4, 2, 3, 3, 3, 2, 3, 4, 0, 2, 6, 2, 6, 5, 3, 4, 6, 2, 6,\n","         3, 2, 3, 3, 6, 5, 3, 2])),\n"," torch.return_types.max(\n"," values=tensor([0.9437, 0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.8630, 1.0000, 1.0000, 1.0000, 0.9976, 0.6622, 1.0000, 1.0000,\n","         1.0000, 0.9313, 0.9770, 0.9754, 1.0000, 1.0000, 1.0000, 0.9881, 1.0000,\n","         1.0000, 1.0000, 0.9927, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9980,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9995, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.4122, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.7504, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9913, 0.9987, 1.0000, 0.9996, 1.0000, 1.0000, 1.0000,\n","         0.9984, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9998, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9998,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9341, 1.0000,\n","         0.9978, 1.0000, 1.0000, 1.0000, 0.5760, 1.0000, 0.9996, 1.0000, 1.0000,\n","         1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([5, 4, 0, 3, 2, 4, 5, 4, 6, 6, 4, 3, 3, 5, 6, 3, 4, 5, 4, 4, 0, 0, 4, 0,\n","         3, 4, 0, 2, 5, 3, 4, 4, 6, 6, 3, 4, 3, 6, 2, 0, 5, 6, 3, 3, 3, 5, 5, 2,\n","         4, 6, 5, 3, 4, 4, 6, 6, 6, 3, 3, 6, 0, 5, 3, 3, 0, 3, 6, 0, 0, 4, 0, 2,\n","         5, 6, 4, 0, 3, 2, 3, 4, 5, 6, 0, 6, 3, 6, 6, 0, 4, 6, 0, 3, 6, 6, 4, 4,\n","         3, 4, 3, 3, 0, 4, 5, 4, 5, 4, 4, 5, 6, 5, 3, 6, 3, 3, 4, 4, 2, 6, 4, 0,\n","         4, 6, 6, 0, 4, 3, 0, 5])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 1.0000, 0.9994, 1.0000, 1.0000, 1.0000, 0.5031, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9988, 0.6433, 1.0000,\n","         1.0000, 0.9974, 0.9987, 1.0000, 0.9998, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.9981, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.8846, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.3684,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9625, 0.9607, 0.9854,\n","         1.0000, 1.0000, 0.6506, 1.0000, 1.0000, 1.0000, 0.9570, 0.6359, 1.0000,\n","         0.9996, 0.7642, 1.0000, 1.0000, 1.0000, 0.6460, 1.0000, 1.0000, 0.9998,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9585, 1.0000, 0.5535, 0.9997,\n","         1.0000, 1.0000, 1.0000, 0.6194, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.7768, 1.0000, 0.9137, 0.9989, 1.0000, 0.9090, 1.0000, 0.9920,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7436, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9686, 1.0000, 1.0000, 0.9946, 0.9998,\n","         1.0000, 1.0000, 1.0000, 0.8019, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([6, 3, 6, 4, 5, 5, 2, 0, 3, 5, 0, 6, 4, 5, 6, 0, 6, 3, 5, 4, 2, 0, 6, 0,\n","         2, 6, 4, 3, 6, 3, 5, 5, 6, 5, 5, 4, 0, 3, 3, 4, 3, 4, 6, 4, 4, 5, 3, 4,\n","         5, 4, 5, 6, 2, 6, 3, 3, 6, 6, 6, 4, 3, 0, 5, 4, 6, 0, 6, 4, 0, 4, 3, 6,\n","         6, 4, 3, 3, 5, 0, 3, 6, 5, 6, 3, 3, 3, 3, 0, 0, 2, 4, 3, 0, 6, 6, 2, 6,\n","         0, 3, 2, 0, 0, 3, 6, 3, 6, 3, 0, 4, 2, 6, 5, 3, 4, 4, 4, 4, 4, 4, 3, 0,\n","         4, 3, 3, 4, 4, 2, 6, 3])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.9994, 1.0000, 0.5882, 1.0000, 1.0000,\n","         0.9770, 0.9846, 0.9906, 1.0000, 1.0000, 1.0000, 1.0000, 0.9963, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9996, 1.0000, 1.0000, 0.8464, 1.0000,\n","         0.9997, 1.0000, 0.6324, 1.0000, 1.0000, 0.9977, 1.0000, 0.9860, 1.0000,\n","         0.9881, 1.0000, 1.0000, 0.9738, 0.3662, 1.0000, 1.0000, 0.9786, 1.0000,\n","         1.0000, 1.0000, 0.9528, 1.0000, 1.0000, 0.9958, 1.0000, 1.0000, 0.9250,\n","         0.5055, 0.9995, 0.9996, 1.0000, 1.0000, 1.0000, 0.9970, 0.5655, 1.0000,\n","         0.6376, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9999, 1.0000, 1.0000, 1.0000, 0.8947, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9921, 1.0000, 1.0000, 1.0000, 0.9973, 1.0000, 1.0000, 1.0000, 0.9995,\n","         0.9876, 1.0000, 0.5304, 1.0000, 1.0000, 0.8296, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9989, 1.0000, 1.0000, 0.9969, 1.0000, 0.9998, 0.8770,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9786, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.7322, 0.9995, 1.0000, 1.0000, 0.9926, 1.0000,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([6, 3, 3, 3, 2, 3, 6, 3, 3, 2, 2, 3, 3, 3, 6, 5, 6, 3, 3, 2, 3, 4, 4, 6,\n","         0, 6, 3, 4, 2, 4, 3, 5, 4, 3, 2, 3, 2, 3, 3, 4, 4, 6, 3, 4, 3, 4, 5, 6,\n","         2, 6, 4, 5, 4, 6, 0, 3, 4, 6, 4, 2, 6, 6, 4, 4, 3, 5, 3, 0, 3, 4, 3, 4,\n","         6, 2, 3, 3, 3, 6, 4, 5, 3, 4, 0, 6, 5, 4, 4, 4, 6, 6, 6, 4, 2, 2, 5, 4,\n","         4, 2, 6, 3, 3, 3, 3, 3, 2, 5, 4, 2, 3, 2, 5, 4, 6, 5, 0, 5, 3, 5, 2, 4,\n","         3, 2, 0, 6, 0, 3, 0, 0])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 1.0000, 0.9837, 0.3751, 1.0000, 1.0000, 1.0000, 0.9973, 1.0000,\n","         1.0000, 1.0000, 0.9297, 1.0000, 0.8296, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.8846, 1.0000, 1.0000, 0.9810, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9453, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 0.9932, 1.0000,\n","         1.0000, 1.0000, 0.9268, 1.0000, 1.0000, 0.5932, 0.8861, 1.0000, 0.6937,\n","         1.0000, 0.9999, 1.0000, 1.0000, 0.9973, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9963, 1.0000, 1.0000, 1.0000, 0.9997, 0.9604, 0.9987, 1.0000, 0.8125,\n","         0.9720, 0.8886, 0.9968, 1.0000, 1.0000, 1.0000, 0.7969, 1.0000, 1.0000,\n","         1.0000, 0.9989, 1.0000, 1.0000, 1.0000, 0.9959, 1.0000, 1.0000, 1.0000,\n","         0.8597, 0.8991, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.5502, 1.0000, 1.0000, 1.0000, 0.9954, 0.6871, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.5720, 0.9744, 0.9987, 1.0000, 0.6949, 0.9914, 0.9973,\n","         1.0000, 1.0000, 0.9994, 0.9979, 0.9999, 0.7579, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.8893, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9576,\n","         1.0000, 0.5568], grad_fn=<MaxBackward0>),\n"," indices=tensor([4, 0, 0, 3, 4, 6, 6, 3, 4, 6, 4, 6, 6, 4, 2, 4, 6, 0, 2, 6, 6, 4, 3, 5,\n","         5, 6, 3, 0, 3, 2, 2, 5, 5, 6, 3, 6, 6, 3, 6, 6, 0, 2, 6, 6, 4, 4, 4, 4,\n","         6, 3, 4, 3, 4, 3, 6, 4, 3, 3, 3, 6, 3, 6, 4, 3, 4, 0, 3, 3, 3, 3, 2, 3,\n","         0, 2, 2, 6, 6, 0, 2, 0, 6, 3, 4, 3, 3, 3, 0, 4, 4, 3, 0, 3, 2, 6, 2, 3,\n","         3, 5, 5, 3, 4, 3, 3, 4, 5, 3, 4, 6, 6, 5, 4, 3, 0, 4, 6, 2, 6, 0, 5, 2,\n","         4, 4, 2, 3, 3, 4, 0, 4])),\n"," torch.return_types.max(\n"," values=tensor([0.6476, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9938, 1.0000,\n","         0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 0.9858, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6176, 1.0000, 0.9436, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9998, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.9979, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9997, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.8624, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8527,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9993, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.9996, 0.8923, 1.0000, 1.0000, 0.9885, 1.0000, 0.9917, 1.0000,\n","         1.0000, 0.8776, 1.0000, 1.0000, 1.0000, 1.0000, 0.5833, 1.0000, 0.9736,\n","         1.0000, 1.0000, 0.9527, 0.9131, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.9960, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.5965, 1.0000,\n","         0.4988, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([6, 3, 3, 4, 6, 5, 4, 3, 3, 4, 4, 2, 4, 3, 4, 3, 3, 0, 6, 0, 2, 3, 4, 6,\n","         2, 4, 3, 3, 6, 4, 6, 5, 6, 3, 3, 3, 0, 3, 5, 5, 5, 3, 6, 2, 6, 0, 0, 4,\n","         2, 4, 6, 3, 4, 3, 5, 2, 0, 3, 3, 2, 3, 3, 2, 3, 4, 5, 3, 5, 2, 3, 4, 4,\n","         4, 4, 6, 0, 6, 3, 6, 6, 3, 4, 0, 4, 4, 6, 4, 5, 3, 3, 4, 6, 4, 6, 3, 6,\n","         4, 6, 3, 4, 3, 2, 3, 3, 3, 0, 3, 3, 0, 3, 6, 6, 6, 3, 3, 6, 3, 4, 4, 5,\n","         4, 3, 6, 5, 3, 0, 3, 5])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 0.9720, 0.9995, 1.0000, 0.8746, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.8407, 0.6547, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.9426, 1.0000, 1.0000, 1.0000, 1.0000, 0.9873, 1.0000, 0.9973,\n","         1.0000, 0.9991, 0.9861, 0.9956, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9246, 1.0000, 1.0000, 0.9986, 1.0000, 1.0000, 0.9571, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9847, 0.9989, 1.0000, 0.9343, 1.0000, 1.0000, 1.0000,\n","         0.9896, 1.0000, 1.0000, 1.0000, 1.0000, 0.9997, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.8615, 0.7127, 1.0000, 1.0000, 1.0000,\n","         0.9059, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7697, 1.0000,\n","         0.8892, 1.0000, 1.0000, 0.9992, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9173, 0.9722, 1.0000, 1.0000, 1.0000,\n","         0.9961, 1.0000, 1.0000, 0.9816, 1.0000, 1.0000, 1.0000, 0.9272, 1.0000,\n","         1.0000, 1.0000, 0.9994, 0.9366, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9858, 1.0000, 0.9982, 1.0000, 0.9826, 0.5350, 0.9999,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([4, 6, 0, 0, 2, 6, 3, 3, 2, 4, 0, 6, 6, 3, 4, 6, 3, 5, 4, 6, 3, 6, 2, 3,\n","         2, 3, 6, 0, 4, 4, 2, 4, 3, 3, 4, 0, 2, 3, 0, 2, 0, 3, 6, 0, 3, 5, 0, 0,\n","         3, 6, 4, 4, 6, 0, 6, 0, 4, 3, 3, 5, 5, 0, 5, 5, 6, 0, 0, 6, 0, 3, 4, 6,\n","         6, 0, 3, 3, 0, 5, 4, 6, 0, 6, 3, 3, 4, 3, 0, 4, 3, 4, 6, 3, 2, 6, 4, 2,\n","         3, 6, 3, 6, 0, 3, 2, 0, 5, 3, 4, 3, 5, 3, 4, 0, 2, 3, 0, 3, 5, 3, 4, 4,\n","         4, 2, 5, 5, 6, 0, 3, 3])),\n"," torch.return_types.max(\n"," values=tensor([0.9999, 1.0000, 1.0000, 0.6505, 0.9960, 1.0000, 0.9551, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9452, 1.0000, 0.9786, 1.0000, 0.9972, 0.9999, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9998,\n","         1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 0.9817, 0.9998, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.6854, 0.9115, 0.9991, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9961, 1.0000, 0.9885, 0.7556, 1.0000,\n","         0.9979, 1.0000, 0.9998, 0.9981, 1.0000, 1.0000, 1.0000, 0.9980, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 0.9992, 1.0000,\n","         1.0000, 1.0000, 0.9963, 1.0000, 0.9999, 0.9997, 0.5282, 0.9919, 1.0000,\n","         0.8735, 1.0000, 0.9295, 1.0000, 0.9996, 0.9989, 0.9992, 0.9964, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.9989, 1.0000, 0.9858, 0.7113, 0.9999, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.9850, 1.0000, 0.7166, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.8985, 0.9996, 1.0000, 1.0000, 0.9997, 0.9114,\n","         0.7322, 0.8797, 1.0000, 1.0000, 0.9999, 0.9997, 1.0000, 1.0000, 0.9995,\n","         1.0000, 0.5503], grad_fn=<MaxBackward0>),\n"," indices=tensor([6, 0, 5, 0, 6, 4, 2, 5, 3, 4, 3, 0, 6, 6, 5, 0, 4, 4, 3, 0, 3, 2, 3, 6,\n","         3, 5, 2, 4, 3, 0, 3, 3, 5, 0, 4, 2, 5, 3, 6, 6, 3, 2, 0, 3, 3, 5, 2, 0,\n","         3, 6, 3, 3, 2, 3, 0, 5, 2, 4, 0, 6, 3, 6, 2, 6, 6, 3, 4, 4, 4, 6, 2, 4,\n","         2, 4, 6, 2, 6, 4, 3, 2, 3, 6, 5, 6, 0, 4, 5, 6, 2, 6, 0, 3, 6, 6, 0, 2,\n","         4, 6, 6, 6, 2, 0, 0, 3, 3, 4, 0, 5, 3, 3, 6, 6, 2, 3, 2, 6, 2, 6, 4, 6,\n","         3, 4, 0, 0, 3, 0, 2, 5])),\n"," torch.return_types.max(\n"," values=tensor([0.9723, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9746, 0.9274, 0.9119, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9992,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9912, 1.0000, 0.8460, 1.0000, 0.5152,\n","         1.0000, 0.5304, 1.0000, 1.0000, 0.9980, 1.0000, 0.9995, 1.0000, 1.0000,\n","         0.9480, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.9975, 1.0000, 0.9997, 1.0000, 1.0000, 0.9577, 1.0000, 0.9997,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9826, 1.0000, 1.0000, 0.9280, 0.9999,\n","         1.0000, 1.0000, 0.9797, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.7341, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.9631, 1.0000, 1.0000, 1.0000, 0.9894, 1.0000,\n","         1.0000, 0.9998, 1.0000, 1.0000, 0.9719, 1.0000, 0.9999, 1.0000, 1.0000,\n","         1.0000, 0.9992, 0.9981, 1.0000, 0.9996, 0.9679, 1.0000, 1.0000, 0.9960,\n","         1.0000, 1.0000, 1.0000, 0.5534, 0.9993, 1.0000, 1.0000, 0.5151, 1.0000,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([4, 4, 3, 4, 4, 4, 4, 4, 4, 3, 0, 4, 3, 5, 3, 3, 3, 6, 2, 3, 0, 4, 3, 5,\n","         5, 3, 6, 4, 2, 0, 3, 0, 0, 5, 3, 3, 2, 3, 5, 5, 3, 4, 4, 3, 6, 4, 2, 3,\n","         3, 6, 3, 2, 3, 3, 5, 4, 0, 6, 3, 4, 2, 5, 4, 6, 3, 6, 5, 4, 3, 3, 3, 6,\n","         4, 6, 4, 6, 3, 2, 3, 0, 0, 4, 3, 5, 3, 3, 4, 4, 5, 3, 6, 6, 0, 6, 3, 5,\n","         0, 6, 5, 6, 4, 4, 0, 6, 3, 6, 6, 6, 3, 0, 3, 5, 2, 6, 4, 6, 4, 4, 4, 0,\n","         3, 6, 6, 5, 6, 4, 4, 6])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 0.9998, 1.0000, 0.7500, 0.9996, 1.0000, 0.9976, 0.9925, 1.0000,\n","         0.9997, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9983, 1.0000, 1.0000, 1.0000, 1.0000, 0.5356, 0.9999, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9212, 1.0000, 1.0000, 0.9987, 1.0000, 1.0000, 1.0000,\n","         0.9956, 1.0000, 1.0000, 1.0000, 1.0000, 0.9614, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.8235, 1.0000, 1.0000, 1.0000, 0.9947, 1.0000, 1.0000, 0.9999,\n","         1.0000, 0.9999, 1.0000, 0.9145, 1.0000, 0.9954, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 0.7213, 1.0000, 1.0000, 1.0000,\n","         0.9515, 1.0000, 0.9295, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.8460, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9971, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9984, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8560, 0.9995, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.5454, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9991, 1.0000, 1.0000, 0.9485, 0.9372, 1.0000, 1.0000,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([0, 5, 5, 2, 6, 4, 6, 2, 0, 4, 4, 6, 0, 3, 5, 4, 5, 5, 2, 4, 5, 3, 0, 2,\n","         4, 4, 0, 6, 3, 2, 5, 6, 0, 0, 3, 5, 3, 3, 6, 6, 3, 6, 3, 2, 3, 0, 4, 3,\n","         6, 6, 3, 3, 3, 4, 6, 3, 0, 4, 2, 3, 0, 3, 3, 3, 2, 5, 6, 6, 4, 6, 3, 4,\n","         4, 3, 0, 0, 4, 6, 0, 3, 3, 0, 2, 3, 2, 3, 4, 4, 5, 2, 0, 3, 3, 3, 6, 4,\n","         6, 0, 5, 5, 4, 4, 0, 0, 6, 4, 4, 6, 3, 2, 3, 5, 3, 0, 4, 0, 4, 3, 3, 0,\n","         3, 2, 6, 3, 5, 3, 3, 4])),\n"," torch.return_types.max(\n"," values=tensor([0.9997, 1.0000, 1.0000, 1.0000, 1.0000, 0.9926, 1.0000, 1.0000, 0.6926,\n","         0.9811, 1.0000, 0.9390, 1.0000, 0.9999, 0.9999, 1.0000, 1.0000, 0.9220,\n","         1.0000, 1.0000, 0.9902, 1.0000, 0.9269, 1.0000, 1.0000, 0.9980, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.9951, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.9810, 0.4920, 0.8754, 1.0000, 0.8949, 1.0000, 1.0000, 1.0000,\n","         0.8759, 0.9446, 1.0000, 1.0000, 0.9985, 0.7222, 0.9986, 0.9998, 0.9999,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9138, 1.0000, 0.9999, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.9987, 0.9837, 1.0000, 1.0000, 1.0000, 0.9957,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9975, 0.9999, 0.7335, 0.9999,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9971, 1.0000, 0.9966, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9974, 0.9653, 0.9982, 1.0000,\n","         1.0000, 0.8353, 1.0000, 1.0000, 0.9883, 1.0000, 1.0000, 0.9999, 0.9349,\n","         1.0000, 1.0000, 1.0000, 0.9996, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999,\n","         1.0000, 0.9960, 0.9127, 1.0000, 0.9956, 1.0000, 1.0000, 0.8813, 1.0000,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([2, 5, 3, 4, 4, 3, 6, 3, 3, 6, 4, 0, 2, 6, 3, 6, 3, 4, 0, 2, 3, 3, 4, 3,\n","         5, 4, 3, 3, 3, 0, 2, 0, 4, 6, 2, 3, 3, 6, 3, 6, 4, 4, 3, 6, 6, 0, 0, 4,\n","         0, 0, 4, 5, 3, 4, 6, 5, 3, 0, 6, 4, 0, 0, 3, 4, 0, 3, 2, 4, 4, 3, 2, 4,\n","         0, 0, 0, 6, 5, 4, 2, 2, 4, 6, 4, 2, 4, 4, 6, 0, 3, 0, 5, 0, 2, 6, 3, 4,\n","         4, 3, 4, 5, 4, 5, 4, 6, 0, 2, 0, 4, 4, 6, 4, 4, 3, 0, 4, 3, 2, 3, 3, 0,\n","         4, 4, 4, 3, 3, 6, 3, 6])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7667,\n","         0.9399, 1.0000, 1.0000, 1.0000, 0.6812, 1.0000, 0.6505, 1.0000, 1.0000,\n","         0.9999, 1.0000, 0.9998, 0.7946, 0.9923, 1.0000, 0.9808, 0.8912, 1.0000,\n","         1.0000, 0.9344, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9786, 0.9986, 1.0000, 0.9997, 1.0000, 0.9560, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 0.7255,\n","         1.0000, 1.0000, 1.0000, 0.4605, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000,\n","         1.0000, 0.6716, 1.0000, 1.0000, 1.0000, 0.8773, 0.6793, 1.0000, 0.8778,\n","         1.0000, 1.0000, 0.9980, 1.0000, 1.0000, 1.0000, 0.9748, 0.9980, 0.9985,\n","         1.0000, 0.9768, 0.6595, 1.0000, 1.0000, 1.0000, 0.8964, 1.0000, 1.0000,\n","         0.9834, 1.0000, 0.9833, 1.0000, 1.0000, 1.0000, 0.6823, 0.9049, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.7466, 0.9990, 0.9720, 1.0000, 0.9999, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.9840, 1.0000, 0.9590, 1.0000, 1.0000, 0.6395,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9946, 1.0000, 0.9998, 1.0000, 0.9999,\n","         0.9949, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([3, 3, 5, 3, 3, 3, 6, 4, 0, 4, 5, 5, 0, 2, 3, 6, 3, 0, 5, 4, 2, 3, 6, 6,\n","         3, 6, 5, 6, 5, 3, 4, 0, 0, 3, 0, 2, 6, 4, 4, 3, 5, 0, 0, 3, 3, 6, 5, 3,\n","         2, 3, 4, 3, 4, 6, 6, 4, 4, 0, 5, 0, 2, 4, 6, 3, 0, 5, 2, 4, 3, 6, 6, 2,\n","         2, 4, 0, 3, 2, 4, 4, 6, 6, 3, 0, 3, 4, 3, 5, 4, 3, 4, 0, 5, 2, 6, 3, 6,\n","         6, 3, 3, 2, 3, 3, 3, 6, 6, 3, 4, 3, 3, 4, 6, 0, 2, 0, 5, 6, 6, 2, 3, 3,\n","         3, 6, 3, 4, 0, 5, 4, 6])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 0.8760, 1.0000, 1.0000, 0.9055, 1.0000, 1.0000, 1.0000, 0.9998,\n","         0.9432, 0.9955, 0.8708, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9136, 1.0000, 0.9875, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9821, 1.0000, 0.9992, 0.9998,\n","         1.0000, 1.0000, 0.6432, 0.7069, 1.0000, 1.0000, 0.8832, 1.0000, 0.9989,\n","         0.8868, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8180, 0.9999,\n","         1.0000, 1.0000, 0.6679, 1.0000, 1.0000, 1.0000, 0.9940, 1.0000, 0.9999,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.7175, 1.0000, 0.9997, 1.0000, 1.0000,\n","         0.9991, 1.0000, 1.0000, 1.0000, 0.6786, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9980, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9649, 0.9995, 1.0000, 1.0000, 0.9997,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.8844, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9234, 0.9725, 1.0000, 1.0000, 1.0000, 1.0000, 0.9722,\n","         1.0000, 1.0000, 0.9618, 1.0000, 0.9999, 1.0000, 1.0000, 0.9189, 1.0000,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([6, 6, 6, 3, 4, 4, 0, 3, 4, 6, 6, 5, 5, 4, 3, 4, 3, 3, 6, 3, 6, 6, 0, 4,\n","         0, 3, 3, 5, 3, 6, 6, 0, 6, 5, 3, 6, 3, 2, 5, 4, 6, 3, 2, 0, 2, 6, 3, 3,\n","         3, 3, 0, 2, 0, 0, 2, 6, 4, 4, 6, 4, 6, 3, 4, 0, 5, 0, 5, 3, 4, 3, 3, 6,\n","         4, 3, 2, 0, 0, 4, 3, 0, 4, 5, 2, 3, 3, 6, 2, 3, 5, 6, 6, 4, 4, 3, 0, 6,\n","         3, 5, 3, 3, 6, 4, 3, 2, 6, 5, 3, 3, 3, 3, 6, 2, 4, 2, 3, 3, 6, 0, 0, 6,\n","         6, 0, 4, 4, 0, 4, 6, 6])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 1.0000, 1.0000, 0.8359, 1.0000, 1.0000, 0.9989, 1.0000, 1.0000,\n","         0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9616, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9969, 1.0000, 1.0000, 1.0000, 0.8745, 0.9999, 0.9981,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9229, 1.0000, 0.9999, 1.0000,\n","         1.0000, 0.7906, 0.9987, 1.0000, 1.0000, 1.0000, 0.9984, 1.0000, 1.0000,\n","         1.0000, 0.7558, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 0.9985, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7611,\n","         0.9961, 0.9224, 0.9826, 1.0000, 0.9780, 1.0000, 1.0000, 1.0000, 0.9999,\n","         0.7830, 0.9810, 0.9890, 1.0000, 0.9992, 1.0000, 0.7796, 0.9811, 0.9865,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.5640, 0.9589,\n","         1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 0.8788, 0.9403, 1.0000, 0.9552,\n","         1.0000, 0.9784, 1.0000, 1.0000, 1.0000, 0.9986, 0.9963, 1.0000, 0.4793,\n","         1.0000, 0.9780], grad_fn=<MaxBackward0>),\n"," indices=tensor([5, 3, 3, 5, 4, 4, 5, 5, 0, 2, 0, 3, 3, 5, 4, 3, 6, 3, 4, 6, 3, 4, 3, 3,\n","         0, 3, 4, 4, 3, 3, 3, 5, 4, 5, 0, 0, 3, 3, 3, 2, 3, 6, 3, 4, 3, 6, 2, 3,\n","         4, 4, 4, 6, 6, 6, 6, 0, 4, 0, 6, 4, 6, 4, 6, 0, 3, 6, 3, 3, 3, 3, 3, 3,\n","         2, 3, 5, 3, 6, 3, 2, 3, 4, 0, 6, 4, 5, 6, 5, 6, 2, 4, 4, 5, 3, 2, 3, 3,\n","         0, 5, 4, 5, 3, 5, 2, 5, 5, 5, 0, 5, 4, 4, 4, 4, 5, 6, 0, 6, 5, 3, 4, 4,\n","         4, 6, 4, 6, 3, 3, 3, 4])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 0.8579, 0.9905, 1.0000, 0.9848, 0.9999, 1.0000, 1.0000, 0.9994,\n","         1.0000, 1.0000, 0.8389, 0.9996, 1.0000, 0.9185, 1.0000, 1.0000, 0.9998,\n","         0.9820, 1.0000, 1.0000, 0.7481, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.9998, 1.0000, 0.9622, 1.0000, 0.9638, 0.9655, 1.0000, 0.9559,\n","         0.9956, 1.0000, 1.0000, 1.0000, 0.9986, 1.0000, 1.0000, 0.9962, 0.8573,\n","         0.5765, 1.0000, 1.0000, 0.9596, 1.0000, 0.9973, 0.9996, 0.9783, 1.0000,\n","         0.9968, 1.0000, 1.0000, 1.0000, 1.0000, 0.6478, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.9983, 1.0000, 1.0000, 0.9827, 0.9941, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.9749, 1.0000, 0.9765, 0.9273, 1.0000, 1.0000,\n","         0.9997, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9988,\n","         0.6628, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 0.9999, 1.0000, 1.0000,\n","         1.0000, 0.6981, 1.0000, 0.6699, 0.9988, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.9182, 1.0000, 1.0000, 0.9895, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9999, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([5, 6, 2, 4, 0, 2, 3, 6, 2, 4, 2, 6, 3, 5, 6, 5, 0, 3, 6, 6, 3, 5, 5, 6,\n","         5, 6, 3, 3, 6, 2, 2, 4, 3, 0, 5, 3, 4, 4, 2, 3, 2, 4, 3, 6, 0, 2, 5, 4,\n","         2, 4, 4, 4, 3, 3, 4, 3, 3, 0, 4, 3, 5, 4, 0, 5, 4, 6, 5, 4, 2, 2, 6, 4,\n","         6, 5, 5, 4, 4, 2, 0, 0, 3, 4, 3, 3, 5, 4, 0, 6, 4, 4, 0, 3, 2, 3, 6, 6,\n","         4, 3, 0, 4, 5, 0, 6, 4, 6, 5, 2, 4, 3, 6, 6, 3, 2, 5, 3, 6, 3, 5, 6, 3,\n","         3, 3, 4, 3, 4, 6, 3, 2])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 1.0000, 1.0000, 0.6209, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9933, 0.9004, 1.0000, 0.6963,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9951, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.9963, 0.9763, 0.9969, 0.9995, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 0.9393,\n","         0.9980, 0.9999, 1.0000, 1.0000, 1.0000, 0.9999, 0.9730, 1.0000, 0.9998,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9997, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.9767, 0.8061, 0.9997, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.5170, 1.0000, 1.0000, 1.0000, 0.8286, 0.9999, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9997, 0.9986, 1.0000,\n","         0.9855, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.6671, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8767,\n","         0.8478, 0.9961], grad_fn=<MaxBackward0>),\n"," indices=tensor([4, 6, 5, 4, 0, 5, 6, 6, 2, 3, 3, 6, 5, 2, 3, 6, 4, 0, 3, 4, 4, 4, 0, 3,\n","         3, 4, 4, 6, 6, 2, 4, 6, 4, 4, 5, 4, 0, 5, 6, 6, 6, 4, 3, 3, 3, 4, 4, 0,\n","         4, 0, 3, 6, 2, 3, 5, 3, 3, 0, 3, 4, 3, 5, 6, 4, 6, 6, 3, 3, 2, 4, 3, 0,\n","         5, 3, 3, 5, 3, 6, 2, 3, 5, 6, 6, 3, 0, 2, 6, 0, 5, 0, 3, 6, 3, 5, 3, 3,\n","         6, 5, 0, 6, 5, 5, 3, 2, 3, 4, 4, 4, 0, 5, 0, 2, 6, 4, 4, 0, 5, 0, 2, 5,\n","         3, 3, 5, 3, 0, 4, 3, 3])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 0.9648, 1.0000, 0.5525, 1.0000, 1.0000, 1.0000, 1.0000, 0.9019,\n","         0.5899, 0.9922, 0.8465, 1.0000, 0.9989, 1.0000, 0.9011, 1.0000, 0.9979,\n","         0.6739, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9963, 1.0000, 1.0000,\n","         0.9753, 1.0000, 1.0000, 1.0000, 0.9683, 1.0000, 1.0000, 1.0000, 0.5124,\n","         0.9976, 0.9990, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.6942, 0.9798, 1.0000, 1.0000, 0.9738, 0.9780, 0.6773, 1.0000,\n","         1.0000, 0.5754, 0.8445, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9802, 0.9922, 0.9999, 0.9958, 1.0000, 0.8780, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.9978, 0.9940, 1.0000, 0.7242, 1.0000, 1.0000,\n","         0.9974, 1.0000, 1.0000, 1.0000, 0.9998, 0.9621, 0.9887, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9931, 0.9310, 0.9583, 1.0000, 1.0000, 0.9853, 0.7491,\n","         0.9997, 1.0000, 1.0000, 0.9574, 1.0000, 1.0000, 0.9338, 1.0000, 1.0000,\n","         1.0000, 0.9979, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 0.9944, 1.0000,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([6, 2, 4, 4, 6, 3, 5, 6, 6, 0, 2, 0, 2, 0, 5, 4, 6, 0, 0, 4, 2, 6, 6, 4,\n","         4, 0, 6, 6, 6, 2, 5, 6, 3, 4, 3, 5, 4, 0, 6, 3, 4, 3, 4, 5, 6, 4, 6, 0,\n","         4, 4, 3, 3, 6, 3, 2, 3, 0, 6, 4, 2, 2, 0, 4, 4, 5, 3, 3, 6, 5, 3, 3, 6,\n","         6, 3, 5, 3, 4, 2, 5, 6, 4, 6, 3, 5, 0, 2, 4, 2, 3, 3, 6, 3, 3, 3, 3, 6,\n","         4, 4, 0, 6, 5, 6, 6, 6, 3, 3, 6, 4, 3, 3, 3, 2, 6, 4, 4, 2, 0, 6, 6, 5,\n","         4, 3, 2, 6, 4, 6, 5, 3])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 0.9946, 1.0000, 1.0000, 0.9941, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.9993, 1.0000, 1.0000, 1.0000, 1.0000, 0.9998, 0.8627, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9608, 0.9984, 0.8590, 0.9481, 1.0000,\n","         1.0000, 1.0000, 0.9956, 1.0000, 1.0000, 0.9920, 1.0000, 0.9969, 1.0000,\n","         1.0000, 0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 0.9830, 0.9999, 0.7741,\n","         1.0000, 0.7837, 1.0000, 1.0000, 0.9226, 0.5337, 1.0000, 0.9951, 0.9841,\n","         0.9997, 1.0000, 0.8633, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.5811, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9973,\n","         0.9979, 1.0000, 0.8106, 0.8087, 1.0000, 1.0000, 0.9712, 1.0000, 0.9966,\n","         0.7293, 1.0000, 0.9949, 0.9491, 0.9999, 0.5705, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.5159, 1.0000, 1.0000, 1.0000, 1.0000, 0.9822, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9310, 1.0000, 1.0000, 0.9997, 0.9999,\n","         1.0000, 0.9928, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 0.9991, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9010, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([4, 0, 6, 4, 4, 4, 4, 3, 0, 3, 2, 6, 6, 6, 4, 4, 6, 4, 0, 2, 0, 5, 6, 4,\n","         2, 4, 3, 4, 3, 0, 5, 3, 2, 6, 0, 6, 0, 0, 0, 0, 3, 3, 6, 4, 6, 4, 0, 3,\n","         4, 6, 3, 4, 0, 4, 4, 4, 6, 0, 5, 6, 3, 6, 6, 3, 2, 3, 6, 4, 0, 6, 3, 0,\n","         6, 6, 6, 0, 5, 6, 0, 6, 6, 3, 5, 0, 4, 5, 4, 3, 6, 3, 0, 4, 3, 2, 6, 4,\n","         3, 6, 3, 3, 4, 0, 5, 3, 4, 2, 3, 6, 4, 3, 6, 3, 6, 3, 5, 6, 5, 3, 4, 3,\n","         4, 4, 4, 3, 6, 3, 3, 5])),\n"," torch.return_types.max(\n"," values=tensor([0.7109, 1.0000, 1.0000, 0.8352, 0.5278, 1.0000, 1.0000, 0.9747, 0.9999,\n","         0.9974, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6183, 1.0000, 1.0000,\n","         0.9996, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9906, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 0.9855, 0.9998, 0.7933, 1.0000,\n","         1.0000, 1.0000, 0.9530, 1.0000, 1.0000, 0.9401, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8357, 0.8208,\n","         0.8584, 1.0000, 1.0000, 0.9979, 0.9970, 0.9929, 1.0000, 1.0000, 1.0000,\n","         0.9998, 0.7556, 1.0000, 1.0000, 1.0000, 0.9678, 1.0000, 1.0000, 1.0000,\n","         0.5232, 0.9345, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9214, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9922,\n","         0.9230, 0.8656, 1.0000, 1.0000, 1.0000, 1.0000, 0.9373, 1.0000, 0.9985,\n","         1.0000, 0.8228, 0.9928, 1.0000, 0.9828, 0.9101, 1.0000, 0.9058, 0.9691,\n","         1.0000, 0.9984, 1.0000, 1.0000, 0.9833, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.9792, 0.9981, 0.8803, 1.0000, 0.9882, 1.0000, 1.0000, 1.0000,\n","         0.7841, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([4, 3, 0, 0, 4, 3, 3, 4, 2, 3, 4, 6, 6, 4, 2, 0, 3, 3, 0, 3, 0, 5, 5, 3,\n","         6, 5, 6, 4, 5, 0, 6, 3, 6, 0, 0, 6, 3, 0, 3, 5, 3, 3, 4, 5, 0, 4, 3, 4,\n","         3, 5, 4, 6, 6, 3, 4, 2, 4, 0, 2, 2, 5, 5, 4, 4, 4, 6, 3, 3, 4, 6, 3, 3,\n","         3, 0, 4, 3, 3, 6, 0, 5, 6, 0, 3, 3, 2, 3, 2, 3, 4, 6, 5, 3, 0, 0, 3, 3,\n","         3, 3, 3, 6, 6, 4, 3, 2, 4, 3, 6, 5, 3, 4, 5, 3, 6, 3, 4, 3, 3, 4, 2, 4,\n","         3, 6, 4, 3, 3, 3, 4, 3])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999,\n","         0.9731, 1.0000, 1.0000, 0.6189, 1.0000, 0.9880, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.9996, 1.0000, 1.0000, 0.9999, 1.0000, 0.9107,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9851, 1.0000, 1.0000, 0.8958,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9975, 1.0000, 1.0000, 0.7660, 0.9999,\n","         1.0000, 1.0000, 0.9996, 1.0000, 1.0000, 1.0000, 0.9984, 1.0000, 1.0000,\n","         0.9999, 0.9935, 1.0000, 0.9994, 0.9917, 1.0000, 1.0000, 1.0000, 0.9942,\n","         1.0000, 0.7755, 1.0000, 1.0000, 1.0000, 0.4693, 1.0000, 0.9158, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9998, 1.0000, 1.0000, 0.9657, 0.9946, 0.9983, 1.0000, 1.0000, 0.9944,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9995, 0.9960, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9950,\n","         1.0000, 0.8553, 0.9211, 0.9855, 0.9996, 1.0000, 0.9929, 1.0000, 0.9998,\n","         0.9995, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000,\n","         0.9963, 0.9998], grad_fn=<MaxBackward0>),\n"," indices=tensor([3, 6, 3, 3, 0, 3, 4, 5, 3, 6, 0, 5, 0, 5, 4, 3, 3, 3, 2, 4, 3, 6, 3, 4,\n","         5, 6, 0, 5, 0, 3, 2, 2, 0, 6, 0, 2, 4, 5, 6, 3, 4, 5, 5, 2, 3, 3, 3, 4,\n","         5, 2, 3, 3, 6, 0, 4, 3, 4, 3, 0, 4, 5, 3, 5, 6, 0, 0, 5, 0, 3, 5, 4, 5,\n","         0, 3, 6, 6, 5, 3, 3, 5, 3, 3, 0, 3, 6, 5, 2, 4, 3, 3, 2, 5, 3, 3, 3, 3,\n","         6, 4, 6, 3, 2, 5, 3, 3, 0, 6, 0, 2, 3, 6, 5, 2, 0, 3, 2, 3, 6, 5, 3, 2,\n","         3, 0, 0, 6, 6, 4, 4, 0])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9970,\n","         1.0000, 1.0000, 0.9992, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.6758, 1.0000, 0.9977, 1.0000, 0.9990, 0.9953,\n","         1.0000, 1.0000, 1.0000, 0.9898, 1.0000, 1.0000, 1.0000, 1.0000, 0.9989,\n","         1.0000, 1.0000, 1.0000, 0.9952, 1.0000, 1.0000, 1.0000, 0.7579, 0.9654,\n","         1.0000, 1.0000, 1.0000, 0.9999, 0.9954, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9840, 1.0000, 0.7468, 1.0000, 1.0000, 1.0000, 0.8727, 1.0000, 0.9717,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9997, 1.0000, 0.5414, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9990, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9938, 1.0000, 1.0000,\n","         0.7935, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9988, 1.0000, 1.0000,\n","         0.9515, 1.0000, 1.0000, 1.0000, 1.0000, 0.9997, 0.9999, 1.0000, 1.0000,\n","         0.5368, 1.0000, 1.0000, 1.0000, 0.9370, 1.0000, 1.0000, 1.0000, 0.9999,\n","         1.0000, 1.0000, 0.9062, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6833,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([3, 0, 2, 6, 6, 3, 4, 6, 4, 0, 0, 0, 4, 4, 5, 2, 4, 0, 3, 3, 5, 3, 5, 6,\n","         3, 2, 4, 6, 4, 4, 4, 5, 3, 3, 3, 4, 2, 6, 3, 3, 3, 5, 4, 3, 4, 2, 3, 5,\n","         4, 2, 3, 6, 0, 5, 4, 3, 4, 0, 0, 0, 4, 5, 0, 4, 3, 5, 3, 3, 6, 3, 6, 3,\n","         5, 6, 4, 4, 2, 0, 6, 3, 3, 0, 2, 0, 4, 0, 4, 4, 5, 2, 6, 4, 4, 4, 4, 6,\n","         0, 3, 3, 4, 6, 3, 0, 3, 6, 6, 5, 4, 3, 3, 4, 6, 6, 3, 3, 3, 3, 3, 0, 2,\n","         6, 3, 3, 3, 3, 4, 3, 3])),\n"," torch.return_types.max(\n"," values=tensor([0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 0.9693, 0.9988, 1.0000, 1.0000,\n","         0.9355, 1.0000, 1.0000, 1.0000, 0.9346, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9970, 1.0000, 1.0000, 1.0000, 0.9986, 0.9844, 0.9998,\n","         1.0000, 1.0000, 1.0000, 0.9644, 1.0000, 1.0000, 1.0000, 1.0000, 0.9830,\n","         0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9238, 1.0000,\n","         0.9625, 0.6008, 1.0000, 1.0000, 1.0000, 0.9933, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.9255, 0.9984, 1.0000, 0.6503, 1.0000, 0.8490,\n","         1.0000, 1.0000, 0.9348, 0.9159, 0.8835, 1.0000, 1.0000, 1.0000, 0.9993,\n","         1.0000, 1.0000, 0.9916, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8848,\n","         0.9866, 1.0000, 1.0000, 0.9825, 1.0000, 0.7877, 0.9940, 0.7399, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9877, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.5409, 1.0000, 1.0000, 0.9991, 1.0000, 1.0000, 0.9083,\n","         0.8333, 1.0000, 0.9951, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([5, 3, 3, 6, 0, 4, 2, 2, 5, 0, 6, 3, 0, 0, 4, 5, 0, 3, 3, 6, 3, 4, 2, 0,\n","         4, 4, 0, 3, 3, 6, 5, 5, 5, 5, 6, 6, 3, 6, 3, 0, 6, 3, 3, 3, 6, 3, 0, 6,\n","         6, 3, 4, 3, 0, 3, 3, 0, 5, 2, 3, 6, 4, 0, 4, 4, 5, 6, 3, 3, 6, 6, 6, 4,\n","         5, 5, 6, 3, 6, 4, 3, 3, 3, 3, 5, 3, 2, 3, 4, 4, 3, 3, 6, 3, 4, 5, 6, 6,\n","         0, 4, 0, 3, 0, 0, 3, 3, 0, 4, 3, 3, 0, 2, 6, 3, 4, 4, 4, 3, 3, 5, 5, 6,\n","         5, 4, 4, 6, 3, 5, 3, 3])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9164, 0.9525, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9287, 1.0000, 1.0000, 0.6971, 0.9993,\n","         1.0000, 1.0000, 0.9909, 0.8385, 1.0000, 1.0000, 0.9262, 1.0000, 1.0000,\n","         0.9641, 0.9994, 0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 0.9974, 0.9565,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9475, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9123, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9998, 1.0000, 1.0000, 0.9978, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9865, 0.9927, 1.0000, 0.9243, 1.0000, 1.0000, 0.9118, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9435, 0.6819, 0.9958, 1.0000, 1.0000, 0.9796, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 0.9993, 0.9958, 1.0000, 1.0000, 0.9530,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9988, 0.5437,\n","         1.0000, 1.0000, 1.0000, 0.9985, 0.9995, 1.0000, 1.0000, 0.9994, 1.0000,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([3, 4, 3, 3, 6, 4, 3, 3, 3, 4, 4, 2, 0, 4, 4, 6, 3, 6, 6, 2, 4, 0, 2, 3,\n","         6, 2, 0, 3, 3, 2, 3, 0, 4, 6, 4, 4, 0, 2, 2, 3, 5, 3, 5, 2, 0, 5, 0, 4,\n","         4, 4, 4, 4, 4, 5, 3, 4, 3, 3, 3, 6, 3, 2, 0, 4, 0, 2, 4, 3, 6, 6, 4, 3,\n","         4, 4, 3, 5, 6, 6, 6, 2, 3, 2, 4, 3, 2, 6, 4, 6, 0, 6, 6, 3, 0, 3, 3, 0,\n","         3, 4, 3, 4, 5, 0, 0, 6, 0, 2, 3, 0, 0, 2, 6, 2, 6, 0, 6, 4, 4, 5, 0, 3,\n","         4, 3, 5, 0, 3, 6, 3, 5])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 1.0000, 1.0000, 0.9454, 0.9460, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9746, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9894, 1.0000, 1.0000, 0.7879, 1.0000, 1.0000, 0.7997, 1.0000, 1.0000,\n","         0.9409, 1.0000, 1.0000, 1.0000, 0.9978, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.9987, 1.0000, 1.0000, 0.8235, 0.9954, 1.0000, 1.0000, 1.0000, 1.0000,\n","         0.7560, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9927, 1.0000, 1.0000,\n","         0.9996, 1.0000, 0.5932, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9993,\n","         0.9810, 1.0000, 1.0000, 0.9205, 1.0000, 1.0000, 1.0000, 0.9993, 1.0000,\n","         0.9980, 1.0000, 1.0000, 1.0000, 0.8951, 0.9952, 1.0000, 0.9983, 0.5884,\n","         1.0000, 0.6231, 1.0000, 1.0000, 0.9996, 1.0000, 0.8207, 0.9445, 1.0000,\n","         0.7621, 1.0000, 1.0000, 1.0000, 0.9891, 0.7201, 0.7184, 1.0000, 0.7823,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 0.9990, 1.0000, 0.9521, 1.0000, 0.9535, 0.8745, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.8481, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([3, 4, 3, 2, 6, 0, 6, 4, 4, 0, 6, 2, 6, 2, 5, 2, 4, 3, 0, 5, 3, 0, 3, 3,\n","         0, 2, 3, 6, 6, 6, 3, 5, 3, 5, 5, 3, 6, 0, 3, 0, 2, 0, 4, 5, 3, 5, 6, 3,\n","         3, 6, 6, 5, 3, 6, 4, 6, 3, 3, 3, 5, 3, 6, 0, 5, 0, 5, 2, 6, 3, 3, 0, 3,\n","         0, 4, 0, 6, 4, 6, 4, 2, 5, 3, 2, 0, 2, 5, 6, 0, 4, 3, 4, 5, 4, 4, 3, 2,\n","         6, 4, 0, 3, 4, 6, 6, 3, 6, 2, 3, 3, 4, 4, 6, 6, 4, 3, 5, 4, 3, 5, 3, 4,\n","         2, 3, 3, 3, 3, 5, 6, 0])),\n"," torch.return_types.max(\n"," values=tensor([1.0000, 0.9536, 1.0000, 0.9998, 0.9250, 1.0000, 1.0000, 0.9970, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9986, 1.0000,\n","         1.0000, 1.0000, 0.9996, 1.0000, 1.0000, 1.0000, 1.0000, 0.9660, 1.0000,\n","         0.9999, 1.0000, 0.9894, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7159,\n","         0.9938, 1.0000, 0.9999, 1.0000, 0.8209, 1.0000, 1.0000, 1.0000, 0.8248,\n","         0.6203, 1.0000, 1.0000, 1.0000, 1.0000, 0.5353, 1.0000, 0.9509, 1.0000,\n","         0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9853,\n","         1.0000, 1.0000, 0.9969, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9274, 0.9031,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9997, 1.0000, 1.0000, 0.9866,\n","         1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 0.9281, 0.9994, 1.0000, 1.0000, 1.0000, 1.0000,\n","         1.0000, 0.9739, 1.0000, 1.0000, 1.0000, 0.9425, 1.0000, 1.0000, 1.0000,\n","         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9674,\n","         1.0000, 1.0000], grad_fn=<MaxBackward0>),\n"," indices=tensor([4, 5, 0, 2, 4, 3, 3, 2, 5, 6, 6, 2, 2, 2, 5, 5, 2, 4, 5, 4, 3, 3, 3, 4,\n","         5, 2, 5, 2, 6, 2, 5, 3, 3, 2, 2, 6, 6, 3, 5, 5, 6, 3, 2, 3, 3, 3, 3, 3,\n","         3, 5, 2, 3, 6, 3, 3, 2, 3, 5, 3, 6, 2, 6, 6, 3, 3, 3, 3, 2, 0, 5, 4, 4,\n","         5, 6, 0, 4, 3, 6, 3, 3, 0, 5, 2, 3, 0, 5, 0, 3, 5, 3, 5, 5, 4, 2, 3, 0,\n","         3, 3, 3, 6, 3, 3, 6, 3, 3, 6, 3, 3, 0, 2, 0, 6, 4, 3, 6, 4, 0, 0, 6, 5,\n","         6, 4, 6, 3, 6, 4, 4, 6])),\n"," torch.return_types.max(\n"," values=tensor([1., 1., 1., 1., 1.], grad_fn=<MaxBackward0>),\n"," indices=tensor([3, 3, 0, 3, 5]))]"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["test_result"]},{"cell_type":"code","execution_count":null,"id":"fb24df44","metadata":{"id":"fb24df44","outputId":"b132c462-ed8c-43ee-afbc-fc3457371325"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/youssef/home/youssef/Desktop/anaconda/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.0\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","/home/youssef/home/youssef/Desktop/anaconda/lib/python3.9/site-packages/torch/nn/functional.py:4298: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n","  warnings.warn(\n","/home/youssef/home/youssef/Desktop/anaconda/lib/python3.9/site-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n","  warnings.warn(\n"]}],"source":["from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","\n","y_pred = []\n","y_true = []\n","for inputs, labels in test_loader:\n","        output = net(inputs) # Feed Network\n","\n","        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n","        y_pred.extend(output) # Save Prediction\n","\n","        labels = labels.data.cpu().numpy()\n","        y_true.extend(labels) # Save Truth"]},{"cell_type":"code","execution_count":null,"id":"f16cb918","metadata":{"id":"f16cb918"},"outputs":[],"source":["classes = ('Angry', 'Disgust', 'Fear', 'Happy', 'Sad',\n","        'Surprise', 'Neutral')"]},{"cell_type":"code","execution_count":null,"id":"ed46163e","metadata":{"id":"ed46163e","outputId":"97ac5e07-e726-4396-f02e-db67a9611e8b"},"outputs":[{"data":{"image/png":"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\n","text/plain":["<Figure size 864x504 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","cf_matrix = confusion_matrix(y_true, y_pred)\n","df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n","                     columns = [i for i in classes])\n","plt.figure(figsize = (12,7))\n","sn.heatmap(df_cm, annot=True)\n","plt.savefig('output.png')"]},{"cell_type":"code","execution_count":null,"id":"5d6b1546","metadata":{"id":"5d6b1546","outputId":"70682844-7851-484a-93a0-63b8217b604c"},"outputs":[{"data":{"text/plain":["3    902\n","6    633\n","4    586\n","2    500\n","0    490\n","5    406\n","1     72\n","Name: emotion, dtype: int64"]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["labelstest[\"emotion\"].value_counts()"]},{"cell_type":"code","execution_count":null,"id":"3078eb8b","metadata":{"id":"3078eb8b","outputId":"379ba778-48fb-4089-9217-a9f06dc351c3"},"outputs":[{"data":{"text/plain":["<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f704cd383a0>"]},"execution_count":78,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","\n","cm = confusion_matrix(y_true,y_pred)\n","ConfusionMatrixDisplay(cm).plot()"]},{"cell_type":"code","execution_count":null,"id":"76dd45bd","metadata":{"id":"76dd45bd"},"outputs":[],"source":["from sklearn.metrics import precision_score, recall_score"]},{"cell_type":"code","execution_count":null,"id":"f55f7842","metadata":{"id":"f55f7842","outputId":"82a678e7-47b7-4f4f-dd81-8cf89a41719b"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.6586792978545556\n"]}],"source":["print(precision_score(y_true, y_pred, average = \"micro\"))"]},{"cell_type":"code","execution_count":null,"id":"89a67c58","metadata":{"id":"89a67c58","outputId":"96b7b322-30e3-48dd-ca77-6ae07d2b7361"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.5567256847356071\n"]},{"name":"stderr","output_type":"stream","text":["/home/youssef/home/youssef/Desktop/anaconda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["print(precision_score(y_true, y_pred, average = \"macro\"))"]},{"cell_type":"code","execution_count":null,"id":"2988ded6","metadata":{"id":"2988ded6","outputId":"82c23972-0f8e-43ac-b2ff-c518eb388f21"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.6586792978545556\n"]}],"source":["print(recall_score(y_true, y_pred, average = \"micro\"))"]},{"cell_type":"code","execution_count":null,"id":"89325421","metadata":{"id":"89325421","outputId":"88ff8d42-6772-4055-c2f1-4267c0968b45"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.5596363423526033\n"]}],"source":["print(recall_score(y_true, y_pred, average = \"macro\"))"]},{"cell_type":"markdown","id":"10a5bdde","metadata":{"id":"10a5bdde"},"source":["Conclusions:\n","1. Data is extremely unbalanced\n","2. We decided to go for data augmentation"]},{"cell_type":"code","execution_count":null,"id":"50bdf6e3","metadata":{"id":"50bdf6e3","outputId":"0fb10db2-1bc2-4f57-bb5a-7f3e65b47fd3"},"outputs":[{"ename":"TypeError","evalue":"Invalid shape (1, 48, 128, 48) for image data","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [88]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, label \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m~/home/youssef/Desktop/anaconda/lib/python3.9/site-packages/matplotlib/_api/deprecation.py:456\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[1;32m    451\u001b[0m     warn_deprecated(\n\u001b[1;32m    452\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    455\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/home/youssef/Desktop/anaconda/lib/python3.9/site-packages/matplotlib/pyplot.py:2640\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[1;32m   2635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[1;32m   2636\u001b[0m         X, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2637\u001b[0m         alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, extent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   2638\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filternorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, filterrad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.0\u001b[39m,\n\u001b[1;32m   2639\u001b[0m         resample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2640\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2642\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2644\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2648\u001b[0m     sci(__ret)\n\u001b[1;32m   2649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n","File \u001b[0;32m~/home/youssef/Desktop/anaconda/lib/python3.9/site-packages/matplotlib/_api/deprecation.py:456\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[1;32m    451\u001b[0m     warn_deprecated(\n\u001b[1;32m    452\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    455\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/home/youssef/Desktop/anaconda/lib/python3.9/site-packages/matplotlib/__init__.py:1412\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1414\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1415\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1416\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n","File \u001b[0;32m~/home/youssef/Desktop/anaconda/lib/python3.9/site-packages/matplotlib/axes/_axes.py:5488\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5481\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5482\u001b[0m im \u001b[38;5;241m=\u001b[39m mimage\u001b[38;5;241m.\u001b[39mAxesImage(\u001b[38;5;28mself\u001b[39m, cmap, norm, interpolation,\n\u001b[1;32m   5483\u001b[0m                       origin, extent, filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[1;32m   5484\u001b[0m                       filterrad\u001b[38;5;241m=\u001b[39mfilterrad, resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[1;32m   5485\u001b[0m                       interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5486\u001b[0m                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 5488\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5489\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5491\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n","File \u001b[0;32m~/home/youssef/Desktop/anaconda/lib/python3.9/site-packages/matplotlib/image.py:715\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A[:, :, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    714\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    716\u001b[0m                     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n","\u001b[0;31mTypeError\u001b[0m: Invalid shape (1, 48, 128, 48) for image data"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["count = 0\n","for image, label in test_loader:\n","\n","    plt.imshow(image.permute(1,2,0,3))\n","    plt.show()\n","    print(f\"Label: {label}\")\n","    count += 1\n","    if(count == 10):\n","        break"]},{"cell_type":"code","execution_count":null,"id":"0f183e91","metadata":{"id":"0f183e91"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}